<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="小舟从此逝，江海寄余生。">
<meta property="og:type" content="website">
<meta property="og:title" content="缓缓行舟">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="缓缓行舟">
<meta property="og:description" content="小舟从此逝，江海寄余生。">
<meta property="og:locale">
<meta property="article:author" content="微澜">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/page/2/"/>





  <title>缓缓行舟</title>
  








<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">缓缓行舟</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">给行船途中的所感所获一个容身之处</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/22/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/22/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/" itemprop="url">四篇图像解耦工作简要介绍</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-12-22T19:20:07+08:00">
                2020-12-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-Variational-Interaction-Information-Maximization-for-Cross-domain-Disentanglement"><a href="#1-Variational-Interaction-Information-Maximization-for-Cross-domain-Disentanglement" class="headerlink" title="1 Variational Interaction Information Maximization for Cross-domain Disentanglement"></a>1 Variational Interaction Information Maximization for Cross-domain Disentanglement</h2><p>这篇文章的思想是使用信息论知识，实现跨域图像解耦表示，是基于VAE的一个改进工作。</p>
<p>对于图像对x，y，二者之间既有共享的表征信息，也有不共享的表征信息，这篇文章提了如下图所示的架构，训练一个VAE，同时学到X的特定表征，Y的特定表征以及X与Y之间的共享表征，这一目的通过最大化X与Y的联合分布之间的边际似然函数实现：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/QQ图片20201222160929.png" alt=""></p>
<p>在实际应用中直接优化上述公式有些困难，作者还做了其他的简化表达，经过改进后，其损失函数的思想在于，希望每个数据之前特异的特征被编码到各自单独的编码器$Z_x,Z_y$之中，而二者之间相互共享的表征则被同一个编码器所$Z_s$编码，为了实现这一目的（三类表征之间尽量分开），作者引入了互信息思想，希望尽可能最小化$I(Z_s, Z_x)$与$I(Z_s, Z_y)$来实现共享表征与特定表征之间的分离，具体实施时使用了如下图所示的公式：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/QQ截图20201222162104.jpg" alt=""></p>
<p>上面这个公式的第一项的作用在于鼓励$Z_s, Z_x$联合向共享域X中提供信息（与数据集X保持紧密联系），后两项的目的在于减少二者之间的信息总量。</p>
<p>同时，为了鼓励共享表征和特异表征之间的分离，还构造了如下的互信息正则化项：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.jpg" alt=""></p>
<p>上面两个等式中，最大化第一项意味着共享表征中含有来自数据集中提取到的表征，最小化第二项意味着从一个数据集中提取到的表征可以很容易地从另一个数据集中推断出来，意味着这个表征是共享的。</p>
<p>下图中的编码器r是设计来为下游任务（图像翻译和检索）使用的。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/1.0.jpg" alt="img;" style="zoom:;" /></p>
<p>解耦效果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/1.4.jpg" style="zoom:67%;" /></p>
<p>定量评估没有说明其具体的解耦指标得分，主要是对跨域图像检索进行的评估。</p>
<h2 id="2-ICAM-Interpretable-Classification-via-Disentangled-Representations-and-Feature-Attribution-Mapping"><a href="#2-ICAM-Interpretable-Classification-via-Disentangled-Representations-and-Feature-Attribution-Mapping" class="headerlink" title="2 ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping"></a>2 ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping</h2><p>一篇在医学影像领域的解耦应用，主要使用GAN-VAE架构，用来鉴别正常人与病患身体结构的差异性，进行影像诊断。网络框架如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/2.1.jpg" alt=""></p>
<p>简要介绍网络各个组成部分的作用：</p>
<p>内容编码器$E^c$：编码输入图像对中共享的与类别无关的信息，用鉴别器对编码特征进行判断，希望编码器对图像对的两张图像的共享信息输入趋同的特征；</p>
<p>属性编码器$E^a$：编码类别相关特征，用来分类；</p>
<p>生成器G：以上述两个编码器输出的特征作为联合输入，目的在于输出受内容特征与属性特征共同控制的图像；</p>
<p>特征映射$Attr Map$：定位类间差异区域。</p>
<p>整个框架基于GAN网络，同时引入VAE思想，使用编码器编码的特征作为重构输入而不是随机噪声，在生成虚假图像后再次进行一次图像生成，用二次生成的图像与真实图像之间的差异性作为最根本的损失。通过这一结构，其目的在于挖掘出决定相似图像类别差异的特征，实现类别与无关特征之间的解耦。</p>
<h2 id="3-Elastic-InfoGAN-Unsupervised-Disentangled-Representation-Learning-in-Class-Imbalanced-Data"><a href="#3-Elastic-InfoGAN-Unsupervised-Disentangled-Representation-Learning-in-Class-Imbalanced-Data" class="headerlink" title="3 Elastic-InfoGAN:  Unsupervised Disentangled Representation Learning in Class-Imbalanced Data"></a>3 Elastic-InfoGAN:  Unsupervised Disentangled Representation Learning in Class-Imbalanced Data</h2><p>本文的贡献是用InfoGAN实现对类不平衡数据的解耦。</p>
<p>InfoGAN假设数据服从均匀分布，因此在类别不平衡的数据中解耦表现较差：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.1.jpg" alt=""></p>
<p>这篇文章针对这一问题对InfoGAN做了两个改进，其一是不对数据分布进行假设，而将其视为优化过程中的可学习参数，为了实现这一点，采用Gumbel-Softmax分布作为噪声的潜在分布，该分布有可微参数，因此可以进行更新；其二是通过实验发现InfoGAN在类不平衡信息中很容易学到图像的低级特征（与之前分享的解释对比学习工作的发现有异曲同工之妙），因此这篇文章引入对比学习的思想，对数据进行增强，强迫模型学习身份表示，以抑制类不平衡的影响。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.2.jpg" alt=""></p>
<p>整篇文章的工作重点就是上述的两个方面，其一，用可学习分布代替InfoGAN假设的均匀分布，优化InfoGAN的同时更新分布（左图）。k维类别潜码的采样方法如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.3.jpg" alt=""></p>
<p>$g_i$代表从Gumbel（0，1）分布采样的样本点，温度参数代表不同类之间的相似程度，假如温度参数很小，将会趋近于onn-hot编码（均匀分布）。</p>
<p>其二，使用简单数据增强方法给数据构造一个正对，同时引入负对，添加一个对比损失项，强迫模型学习身份表示。使用的是常规的对比损失：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.4.jpg" alt=""></p>
<p>最终的损失为InfoLoss和对比损失之和：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.5.jpg" alt=""></p>
<p>定性实验结果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.6.jpg" alt=""></p>
<p>定量实验结果：</p>
<p>使用NMI和ENT（平均熵，评价同一个潜码生成的图像是否属于同一类；每一个潜码是否只与一个真实类别标签关联；越小越好）作为评价指标：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.7.jpg" alt=""></p>
<h2 id="4-WAE模型"><a href="#4-WAE模型" class="headerlink" title="4 WAE模型"></a>4 WAE模型</h2><p>ICLR2021中有两篇论文在WAE（WASSERSTEIN AUTOENCODER）的框架下进行解耦图像生成，WAE是2018年由Google在WGAN的基础上提出来的一种自编码器模型，由于目前没有了解其原理，因此只对这两篇论文在WAE基础上的改进进行简单的介绍。</p>
<h3 id="4-1-Learning-disentangled-representations-with-the-Wasserstein-Autoencoder"><a href="#4-1-Learning-disentangled-representations-with-the-Wasserstein-Autoencoder" class="headerlink" title="4.1 Learning  disentangled representations with the Wasserstein Autoencoder"></a>4.1 Learning  disentangled representations with the Wasserstein Autoencoder</h3><p>想法是把β-TCVAE的构造移植到WAE模型中，重点在于利用TCVAE的loss对WAE进行改进：</p>
<p>TCWAE loss：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.1.jpg" alt=""></p>
<p>β-TCVAE loss：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.2.jpg" alt=""></p>
<p>对比两项损失，可以发现TCWAE具有与β-TCVAE几乎相同的loss函数，区别在于没有最后一个互信息项，以及在第一项的度量上有所差异。</p>
<p>效果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.3.jpg" alt=""></p>
<h3 id="4-2-DISENTANGLED-RECURRENT-WASSERSTEIN-AUTOENCODER"><a href="#4-2-DISENTANGLED-RECURRENT-WASSERSTEIN-AUTOENCODER" class="headerlink" title="4.2 DISENTANGLED RECURRENT WASSERSTEIN AUTOENCODER"></a>4.2 DISENTANGLED RECURRENT WASSERSTEIN AUTOENCODER</h3><p>第二篇WAE相关的文章是将WAE应用于时序图像解耦的工作，用来捕捉时序图像上的相关信息，实现静态因子和动态因子的解耦。</p>
<p><img src="" alt=""><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.4.jpg" alt="4.4" style="zoom:67%;" /></p>
<p>上图是这篇文章提出了来的模型的解耦效果，每一行都代表一个时序（对应不同表情）。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/22/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0%E4%B9%8B%E2%80%94%E2%80%94%E5%95%86%E7%A9%BA%E9%97%B4(Quotient%20Spaces)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/10/22/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0%E4%B9%8B%E2%80%94%E2%80%94%E5%95%86%E7%A9%BA%E9%97%B4(Quotient%20Spaces)/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-10-22T23:26:45+08:00">
                2020-10-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="数学知识学习之——商空间-Quotient-Spaces"><a href="#数学知识学习之——商空间-Quotient-Spaces" class="headerlink" title="数学知识学习之——商空间(Quotient Spaces)"></a>数学知识学习之——商空间(Quotient Spaces)</h1><h2 id="1-定义"><a href="#1-定义" class="headerlink" title="1 定义"></a>1 定义</h2><p>商空间(Quotient Spaces)是线性代数中的知识，本质是定义了一个向量空间中满足特定条件的子空间。</p>
<p>首先，定义向量<strong>等价</strong>的概念：对于某个域$\mathbb{F}$，$V$是$\mathbb{F}$上的一个向量空间，设其存在一个子空间$W$，即$W \subseteq V$，若存在属于$V$的向量${v_1}, {v_2} \in V$，假如二者存在如下关系：</p>
<script type="math/tex; mode=display">
{v_1} - {v_2} \in W</script><p>则认为${v_1}, {v_2}$等价，记为${v_1} \sim {v_2}$。向量等价具有推广性，若${v_1} \sim {v_2} ，{v_2} \sim {v_3} $，则${v_1} \sim {v_3} $。</p>
<p>对于属于$V$的向量${v} \in V$，可以自然地定义其等价向量的集合为：</p>
<script type="math/tex; mode=display">
\overline{v} = v + W =  \lbrace v + w : w \in W \rbrace</script><p>称为$v$的<strong>等价类</strong>。注意，根据向量等价的推广性，所有等价向量的1等价类都是相同的，即当$v_1 \sim v_2$时，$\overline{v_1} = \overline{v_2}$，代表的都是同一个等价类。</p>
<p>向量空间$V$关于$W$的所有等价类的集合，称为商空间，记作：</p>
<script type="math/tex; mode=display">
V/W = \{ \overline{v} : v \in V \}</script><p><strong>Example:</strong>对于$V \in \mathbb{R}^2$，$W = \mathbb{R} * (0, 1)$，向量$v_0 = (x_0, y_0)$的等价类为$\{ v: v = (x_0, y) \}$</p>
<p>通过下面这张图直观理解商空间的定义，下图中每个子集合都代表了一个等价类，这些等价类的集合即为商空间。有一种说法即是商空间中的每个等价类最终坍缩到了一个点上，实现了对向量空间的降维。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/Quotient%20Spaces/1.0.png" style="zoom:50%;" /></p>
<p>商空间的性质：</p>
<p>等价类满足线性条件（这是一个well define（无歧义的））：</p>
<script type="math/tex; mode=display">
\overline{v_1} + \overline{v_2} = \overline{v_1 + v_2} \\
\lambda \overline{v} = \overline{\lambda v}</script><p>这两个结论可从定义入手，由$W$是线性空间证明。</p>
<p>进一步，从向量空间$V$映射到商空间$V/W$的过程记为商映射(quotient map)$\pi$：</p>
<script type="math/tex; mode=display">
\pi : V \rightarrow V/W = v \mapsto \overline{v}</script><p>此映射既是一个<strong>线性</strong>映射(linear map)，又是一个<strong>满射</strong>(surjective map，值域中任何元素都有至少有一个变量与之对应)，线性映射$(\pi (v_1 + v_2) = \pi (v_1) + \pi (v_2), \pi (\lambda v) = \lambda \pi (v))$)可由等价类的线性条件得证，现在证明商映射为满射：</p>
<p>由满射的定义，Kernel $\pi$的值为</p>
<script type="math/tex; mode=display">
\begin{align}
Ker \pi =& \{ v \in V : \pi(v) = \bar{0} \} \\
 =& \{ v \in V : \bar{v} = \bar{0} \}  \\
 =& \{ v \in V : v \in W \} \\
 =& W
 \end{align}</script><p>得证。</p>
<h2 id="2-性质"><a href="#2-性质" class="headerlink" title="2 性质"></a>2 性质</h2><p>1） 当$V,W$均为有限维时，有$\dim_{\mathbb{F}}V/W = \dim_{\mathbb{F}}V - \dim_{\mathbb{F}}W$，这说明什么？假如$V \in \mathbb{R}^r， W \in \mathbb{R}^s$，可知V的一组基$\Beta_V = (w_1, w_2, …, w_r)$，若$\Beta_W = (w_1, w_2, …, w_s)$，则有$\Beta_{V/W} = (w_{s+1}, w_{s+2}, …, w_r)$。这个结论可以直接使用，证明相对而言比较简单，由基的基本定义和商空间的定义即可得到。</p>
<p>2） 假设存在一个线性映射$\psi: V \rightarrow W’$，并且$W \subseteq Ker \psi$，则称$\psi$穿越(factors through)了$\pi$。穿越的含义是必然存在且仅存在一个线性映射$\phi: V/W  \rightarrow W’$，此处的$W’$与$W$无特殊关系，仅作表示。上述三个映射存在以下关系$\psi = \phi \circ \pi$，如下图所示：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/Quotient%20Spaces/1.1.png" alt=""></p>
<p><strong>证明2）</strong>：</p>
<p>先证明$\phi$<strong>存在</strong>。将$\phi: V/W  \rightarrow W’$定义为：</p>
<script type="math/tex; mode=display">
\phi(\bar{v}) := \psi(v)</script><p>当$\bar{v} = \bar{v_1}$时，显然有$v - v_1 \in W$，因此:</p>
<script type="math/tex; mode=display">
\psi(v) = \psi(v_1 + w) = \psi(v_1) + \psi(w)(w \in W)</script><p>由于$\psi$是线性映射，上式必然成立，又由于$W \subseteq Ker \psi$，故$\psi(w) = 0$，所以</p>
<script type="math/tex; mode=display">
\psi(v) = \psi(v_1)</script><p>这说明，从商空间$V/W$到$W’$的映射是成立且well define的，因此肯定有$\psi = \phi \circ \pi$，并且由$\psi$是线性映射直接推出$\phi$是线性映射。</p>
<p>再证明$\phi$的<strong>唯一性</strong>。假定存在映射$\sigma$也满足上述条件，即$\sigma \circ \pi = \phi \circ \pi$，对于所有$v \in V$，都有$\sigma(\bar{v}) = \phi(\bar{v})$，而$\pi$是满射的，所以$\sigma = \phi$。</p>
<p><strong>推论：</strong></p>
<p>假设存在两个向量空间$V_1, V_2 \in \mathbb{F}$，其对应的子空间为$W_1, W_2$，各自的商映射为${ { \pi } _1} : V_1 \rightarrow V_1 / W_1; { { \pi } _2} : V_2 \rightarrow V_2 / W_2$  。如果$V_1, V_2$之间存在线性映射关系$T: V_1 \rightarrow V_2, T(W_1) \subseteq W_2$，并定义映射$\tilde{T} : V_1 \rightarrow V_2 / W_2$，即$v_1 \rightarrow {\pi}_2(T(v_1))$，将会有$W_1 \subseteq Ker(\tilde{T})$，根据性质2），可以推知将会存在一个独一无二的线性映射$\bar{T}: V_1 / W_1 \rightarrow V_2 / W_2$，满足$\bar{T} \circ {pi}_1= \tilde{T}$，$\bar{T}$称为诱导映射(induced map)，如下图所示：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/Quotient%20Spaces/1.2.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/14/SimCLR%EF%BC%9A%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/14/SimCLR%EF%BC%9A%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" itemprop="url">SimCLR：用于视觉表征的对比学习框架</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-14T13:27:16+08:00">
                2020-09-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>论文链接: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.05709.pdf">https://arxiv.org/pdf/2002.05709.pdf</a></p>
<p>官方github链接: <a target="_blank" rel="noopener" href="https://github.com/google-research/simclr">https://github.com/google-research/simclr</a></p>
<p>他人复现pytorch链接: <a target="_blank" rel="noopener" href="https://github.com/sthalles/SimCLR">https://github.com/sthalles/SimCLR</a> </p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/0.gif" style="zoom:67%;" /></p>
<h2 id="1-概况"><a href="#1-概况" class="headerlink" title="1 概况"></a>1 概况</h2><p><strong>核心观点：</strong></p>
<ol>
<li>数据增强(data augmentations)的组合对预测任务的表现有重要影响，对于非监督学习而言，数据增强的提升作用更大；</li>
<li>本文定义了一个对比损失和表征之间的可学习非线性转换，大幅提高了表征的质量；</li>
<li>具有对比交叉熵损失(contrastive cross entropy loss)的表征学习得益于归一化嵌入和适当地调整温度参数；</li>
<li>与监督学习相比，对比学习可以通过更多的训练和更大的Batch Size 获得更好的表现，更深更宽的网络对对比学习表现的提升也有益。</li>
</ol>
<p><strong>框架效果：</strong></p>
<ol>
<li><p>在对Image Net 进行分类实验时，在top-1精度上取得了76.5%的结果，获比之前最先进的无监督或半监督提升7%，与有监督的Res-Net-50性能相媲美；</p>
</li>
<li><p>对Image Net 1%的Label 进行微调时，SimCLR 实现了85.8%的top-5精度，相对性能提升10%,超越了100× fewer label的AlexNet；</p>
</li>
<li><p>在其他自然图像分类数据集上进行微调时，SimCLR 在12个数据集中的10个数据集上的表现相当于或优于强监督的Baseline。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/1.png" alt="ImageNet Top-1精度表现" style="zoom: 67%;" /></p>
</li>
</ol>
<h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h2><h3 id="2-1-对比学习框架"><a href="#2-1-对比学习框架" class="headerlink" title="2.1 对比学习框架"></a>2.1 对比学习框架</h3><p>SimCLR 通过潜在空间上的对比损失，最大化相同数据示例的不同增强视图之间的协议进行表征学习，主要由四个主要组件组成：</p>
<ol>
<li><p><strong>随机数据增强模块</strong></p>
<p>将任意给定的数据示例随即转换为同一示例的两个相关视图，用$\widetilde{x}_i$和$\widetilde{x}_j$表示，将其视为一个正对。本文使用了三种方法进行数据增强：<strong>随即裁剪和调整（如随机翻转）</strong>（裁剪后调整图像尺寸为原图大小），<strong>随机色彩失真</strong>和<strong>随机高斯模糊</strong>，作者认为随即裁剪和色彩失真的结合是使网络具有良好性能的关键。</p>
</li>
<li><p><strong>神经网络基编码器(base encoder)</strong></p>
<p>基编码器定义为$f(·)$，其作用在于从增强后的数据集中提取表征向量。作者认为SimCLR可以在无任何约束的情况下选择各种网络架构的基编码器，论文中选择的是常见的ResNet。因此，对于数据$\widetilde{x}_i$，有：</p>
<script type="math/tex; mode=display">
\boldsymbol{h}_i=f(\widetilde{x}_i)=ResNet(\widetilde{x}_i)</script><p>其中，$\boldsymbol{h}_i\in\R^d$，为经过平均池化层之后的输出。</p>
</li>
<li><p><strong>小型神经网络投影头(projection head)</strong></p>
<p>投影头$g(·)$的作用是将编码后的表征$h_i$映射到应用对比损失的潜在空间中，本文使用的是一个两层MLP，具体计算方法如下：</p>
<script type="math/tex; mode=display">
z_i=g(h_i)=W^{(2)}(\sigma(W^{(1)}(h_i)))</script><p>其中，$\sigma$是ReLU 函数，用$z_i$比$\boldsymbol{h}_i$更好构造对比损失。</p>
</li>
<li><p><strong>对比损失函数</strong></p>
<p>在对比任务中，倘若给定一个包含正对$\widetilde{x}_i$和$\widetilde{x}_j$的数据集$\left\{\widetilde{x}_k\right\}$，重构损失的作用是从$\left\{\widetilde{x}_k\right\}_{k{\neq}i}$中找出给定$\widetilde{x}_i$对应的正对$\widetilde{x}_j$。</p>
</li>
</ol>
<p>因此，整个SimCLR的结构如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/2.png" alt="SimCLR对比学习框架结构图" style="zoom:67%;" /></p>
<ol>
<li><p><strong>样本构造：</strong></p>
<p>对于预测任务，随机采样一小批($N$个))数据样本，之后采用数据增强方法，将每一个样本进行扩充，一共得到$2N$个数据点，即$N$个正对。在本文中，不直接构造负对，而是对于给定的正对$\left\{\widetilde{x}_i, \widetilde{x}_j\right\}$，将除该样本对以外的其他$2(N-1)$个样本示例都视为负样本。</p>
</li>
<li><p><strong>损失函数构造：</strong></p>
<p>本文的对比损失函数基于$l_2$正则化，是从其他论文中参考得到的，称我为NT-Xent(标准化温度尺度的交叉熵损失)。该loss首先定义了一个sim函数表示正则化：$\rm{sim}(\boldsymbol{u},\boldsymbol{v})={\boldsymbol{u}^\mathsf{T}}\boldsymbol{v}/|\boldsymbol{u}||\boldsymbol{v}|$，正对$\left\{\widetilde{x}_i, \widetilde{x}_j\right\}$之间的损失函数即为：</p>
<script type="math/tex; mode=display">
\ell_{i,j}=-\log\frac{\exp(\rm{sim}(\it{z_i},z_j\rm)/\tau)}{\sum_{k=1}^{2N}{\mathbb{l}_{[k{\neq}i] }\exp(\rm{sim}(\it{z_i},z_j\rm)/\tau)} }</script><p>上式中，$\mathbb{l}_{[k{\neq}i]}$是一个指示函数，当且仅当$k=i$时取0，否则为1。$\tau$是进行优化的温度参数，后续实验显示为0.1时效果最好。</p>
</li>
<li><p><strong>算法概述</strong></p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.png" alt="SimCLR框架算法" style="zoom:80%;" /></p>
<p>根据算法可以发现，在构造数据时，对$\widetilde{x}_i$与$\widetilde{x}_j$分别采用一种增强方法，并且分别计算了一个正对中每个数据点对另一种增强方法所有数据的余弦相似性。</p>
</li>
</ol>
<h3 id="2-2-大Batch-Size训练方法"><a href="#2-2-大Batch-Size训练方法" class="headerlink" title="2.2 大Batch Size训练方法"></a>2.2 大Batch Size训练方法</h3><p>作者提到，为了保持框架的简单化，在模型训练时不采用记忆库训练模型，而是采用大Batch Size的形式，将Batch Size从256扩大为<strong>8192</strong>，在进行数据增强之后，一个正对将有$（8192-1）×2=16382$个负示例。为了克服大Batch Size使用SGD/Momentum进行优化时可能导致的训练不稳定问题，论文对每个Batch Size都采用了<strong>LARS</strong>优化器。</p>
<p>此外，由于标准ResNet架构采用Batch Normlization(BN)进行规范化，作者指出，在具有数据并行性的分布式训练中，BN的均值和方差通常在每张卡上进行局部的聚合。而论文提出的对比学习中，正对在同一张卡中计算，模型可以利用局部信息泄漏提高预测精度，而不需要改善表征，他们在训练过程中使用了一种<strong>Global BN</strong>的方法，取所有卡上BN的均值和方差作为表示。</p>
<h3 id="2-3-评价方案"><a href="#2-3-评价方案" class="headerlink" title="2.3 评价方案"></a>2.3 评价方案</h3><ol>
<li><p><strong>数据集选择</strong></p>
<p>大部分实验都在<strong>ImageNet ILSVRC-2012</strong> 数据集上进行；少部分实验在<strong>CIFAR-10</strong>上进行；还使用了一些广泛用于<strong>迁移学习</strong>的数据。</p>
<p>使用线性评估协议进行评估，在一个冻结基网络的顶层训练一个线性分类器，测试分类精度，也比较了一些迁移学习和半监督方法。</p>
</li>
<li><p><strong>默认设置</strong></p>
<p>除非另有规定：</p>
<p>在数据增强时使用<strong>随机裁剪和调整(随机翻转)</strong>，<strong>颜色失真</strong>，和<strong>高斯模糊</strong>三种方法。</p>
<p>使用<strong>ResNet-50</strong>作为基础网络编码器,并使用两层MLP将编码后的表征投影到一个128维的潜在空间之中。</p>
<p>训练时，使用<strong>NT-Xent</strong> loss，使用<strong>LARS</strong>进行优化，设置学习率为$4.8 (= 0.3×Batch Size/256)$，权值衰减率为$10^{-6}$。Batch Size大小为4096，共训练100个epoch。此外，在前10个epoch使用线性预热，并在不重新启动的情况下使用余弦衰减计划衰减学习速率。</p>
</li>
</ol>
<h2 id="3-数据增强"><a href="#3-数据增强" class="headerlink" title="3 数据增强"></a>3 数据增强</h2><h3 id="3-1-数据增强涵盖了对比预测任务的两种情况"><a href="#3-1-数据增强涵盖了对比预测任务的两种情况" class="headerlink" title="3.1 数据增强涵盖了对比预测任务的两种情况"></a>3.1 数据增强涵盖了对比预测任务的两种情况</h3><p>作者指出，目前数据增强方法尚未被视为对比预测任务的标准手段之一，目前流行的手段都是通过改变网络架构实现对比预测，作者认为他们的数据增强方法可以在数据层面即涵盖两种典型的对比预测任务，分别是<strong>整体——局部预测</strong>和<strong>邻近预测</strong>。</p>
<p>具体来说，如下图所示，在对图像应用随机裁剪时，将会出现如下图左右两张图的正对样本，这些样本包含了上述的两种对比预测任务情形（A, B: 整体——局部；C, D: 临近）。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/4.png" alt="图像增强在对比预测上的应用" style="zoom:67%;" /></p>
<h3 id="3-2-不同数据增强方式的组合对表征学习效果至关重要"><a href="#3-2-不同数据增强方式的组合对表征学习效果至关重要" class="headerlink" title="3.2 不同数据增强方式的组合对表征学习效果至关重要"></a>3.2 不同数据增强方式的组合对表征学习效果至关重要</h3><p>作者通过下图展示了多种数据增强的方法，需要注意的是，在实验中，其只使用了随即裁剪（包括裁剪、大小调整和翻转）、颜色失真和高斯模糊三种手段。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.png" alt="数据增强手段举例"></p>
<p>在实验过程中，作者通过研究单独和成对数据增强方法对SimCLR框架性能的影响。由于ImageNet数据库中的图像大小并不一致，在使用数据时通常已经进行了图像裁剪和调整操作，由于很难剔除随机裁剪的影响，因此作者通过不对称转换改善这一情况。</p>
<p>具体来说，首先对数据样本次啊用随机裁剪，并将其调整到相同的分辨率大小，之后对一个框架的两个分支中的一个应用目标转换$t(·)$，而另一个分支作为标志，即令：$t(x_i)=x_i$。</p>
<p>作者认为这种非对称数据的增加会对框架性能产生影响，但不应该在实质上改变单个数据增强或组合数据增强方法的影响。</p>
<p>作者测评了不同方法和组合方法的top-1识别结果，如下图所示。可以发现，单一数据增强方法都不足以学到良好的表征，即使模型可以识别出任务中的正对，组合增强方法会增大预测难度，但是能够显著提升表征质量，最后作者发现，随机裁剪和颜色失真的组合最有利于学习表征。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/6.png" alt="数据增强方法对表征学习的影响"></p>
<h3 id="3-3-对比学习比监督学习更需要数据增强"><a href="#3-3-对比学习比监督学习更需要数据增强" class="headerlink" title="3.3 对比学习比监督学习更需要数据增强"></a>3.3 对比学习比监督学习更需要数据增强</h3><p>作者调整了颜色增强的强度，发现颜色增强大大提升了SimCLR的线性评估结果。而监督学习使用更加复杂的自动增强方法时，表征学习的表现也没有比简单的裁剪+颜色失真效果好，不同的颜色增强强度也没有提升或削弱监督学习的表现。因此作者认为对比学习比监督学习可以从颜色增强中获得更大收益。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/7.png" alt="颜色增强对不同监督模型的增益" style="zoom:67%;" /></p>
<h2 id="4-Encoder-和Head-的架构"><a href="#4-Encoder-和Head-的架构" class="headerlink" title="4 Encoder 和Head 的架构"></a>4 Encoder 和Head 的架构</h2><h3 id="4-1-更大的模型使无监督对比模型表现更好"><a href="#4-1-更大的模型使无监督对比模型表现更好" class="headerlink" title="4.1 更大的模型使无监督对比模型表现更好"></a>4.1 更大的模型使无监督对比模型表现更好</h3><p>下图中绿色“×”是有监督ResNet 训练90个epoch的结果，红色“★”是本文模型训练100个epoch的及如果，蓝色“·”是本文模型训练100个epoch的结果，可以发现，随着网络参数的增加，有监督和无监督模型的表现都有所提升，但无监督模型的提升更大。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/8.png" alt="网络参数数量对表征识别的影响" style="zoom:67%;" /></p>
<h3 id="4-2-非线性投影head有助于提升其之前网络层的表征质量"><a href="#4-2-非线性投影head有助于提升其之前网络层的表征质量" class="headerlink" title="4.2 非线性投影head有助于提升其之前网络层的表征质量"></a>4.2 非线性投影head有助于提升其之前网络层的表征质量</h3><p>非线性投影head，即前文提到的$g(\boldsymbol{h})$，本文对比了包括非线性投影、线性投影以及无投影在内的三种映射方式对表征质量的影响，如下图所示。最终发现非线性投影的Top-1精度比无映射提升10%以上，比线性投影提升3%，并且这一结果在只使用一个head时不受输出维度的影响。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/9.png" alt="不同映射方式对表征质量的影响" style="zoom: 80%;" /></p>
<p>此外，作者提到使用非线性投影时，head前面的隐藏层也可以学到更好的表征。</p>
<p>同时，作者通过对使用非线性投影后的映射$z$ 以及非线性投影之前的映射$\boldsymbol{h}$对不同表征的预测表现进行对比，发现$z$ 比$\boldsymbol{h}$保留了更少的信息，具体如下表所示。作者认为是因为$z$删除了一些颜色之类的信息，使得在训练过程中这些信息更多在$\boldsymbol{h}$阶段形成。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/10.png" alt="不同方法保留信息的大小" style="zoom:67%;" /></p>
<h2 id="5-损失函数和Batch-Size"><a href="#5-损失函数和Batch-Size" class="headerlink" title="5 损失函数和Batch Size"></a>5 损失函数和Batch Size</h2><h3 id="5-1-带有可调温度参数的归一化交叉熵损失是更优选择"><a href="#5-1-带有可调温度参数的归一化交叉熵损失是更优选择" class="headerlink" title="5.1 带有可调温度参数的归一化交叉熵损失是更优选择"></a>5.1 带有可调温度参数的归一化交叉熵损失是更优选择</h3><p>作者对比了本文使用的NT-Xent损失和下表中另外两种对比损失应用于此框架的表现。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/11.png" alt="不同loss表达式" style="zoom:67%;" /></p>
<p>作者认为NT-Xent损失中的温度参数可以帮助模型从hard-negative中学习，而其他损失函数没有这个特点，必须引入额外的semi-hard-negative进行挖掘，在均使用l2正则化（余弦相似性）时，发现即使是用了semi-hard-negative的其他loss，表现也不如NT-Xent。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/12.png" alt="不同loss表现对比" style="zoom:67%;" /></p>
<p>在此基础上，作者又测试了使用l2正则化和只使用点积的情况以及不同温度参数对对比任务精度和通过Top-1预测的表征表现情况，发现不使用l2正则化会提升对比任务精度，但是降低了表征的线性评估结果。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/13.png" alt="l2正则化和温度参数影响探究" style="zoom:67%;" /></p>
<h3 id="5-2-大Batch-Size和更多训练对对比学习更有益"><a href="#5-2-大Batch-Size和更多训练对对比学习更有益" class="headerlink" title="5.2 大Batch Size和更多训练对对比学习更有益"></a>5.2 大Batch Size和更多训练对对比学习更有益</h3><p>作者通过实验发现，大Batch Size 和大epoch 对框架的表现具有提升，因为增大这两者会提供更多的负例。不过随着epoch的增加，不同Batch Size效果之间的差异在减小。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/14.png" alt="不同epoch与batch size的影响" style="zoom:67%;" /></p>
<h2 id="6-实验对比"><a href="#6-实验对比" class="headerlink" title="6 实验对比"></a>6 实验对比</h2><h3 id="6-1-线性评估"><a href="#6-1-线性评估" class="headerlink" title="6.1 线性评估"></a>6.1 线性评估</h3><p>这部分实验用预训练好的一些无监督模型训练了一个线性分类器，评估在ImageNet上的效果，可以发现，SimCLR的Top-1和Top-5表现都很优秀，当加深网络时，无监督模型的表现普遍提升（括号中的(4×)的意思是网络宽度是原网络的4倍）。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/15.png" alt="线性评估" style="zoom:80%;" /></p>
<h3 id="6-2-半监督学习"><a href="#6-2-半监督学习" class="headerlink" title="6.2 半监督学习"></a>6.2 半监督学习</h3><p>对ImageNet采样1%和10%有标签的训练集（分别是12.8张与128张图片一个类）进行微调，比较不同网络的效果。</p>
<p>微调过程使用名为Nesterov动量优化器进行调整，对于1%标记的数据，对60个epoch进行微调，对于10%标签的数据，对30个epoch进行微调，同时调整图像大小为256×256，之后在中央应用裁剪，裁剪到224×224的大小。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/16.png" alt="半监督学习" style="zoom: 80%;" /></p>
<h3 id="6-3-迁移学习"><a href="#6-3-迁移学习" class="headerlink" title="6.3 迁移学习"></a>6.3 迁移学习</h3><p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/17.png" alt="迁移学习" style="zoom:67%;" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/">&lt;i class&#x3D;&quot;fa fa-angle-left&quot;&gt;&lt;&#x2F;i&gt;</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">微澜</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
