<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="小舟从此逝，江海寄余生。">
<meta property="og:type" content="website">
<meta property="og:title" content="缓缓行舟">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="缓缓行舟">
<meta property="og:description" content="小舟从此逝，江海寄余生。">
<meta property="og:locale">
<meta property="article:author" content="微澜">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/"/>





  <title>缓缓行舟</title>
  








<meta name="generator" content="Hexo 5.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">缓缓行舟</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">给行船途中的所感所获一个容身之处</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/26/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/26/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url">《Contrastive Learning with Hard Negative Samples》阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-12-26T15:30:33+08:00">
                2020-12-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-动机和思路"><a href="#1-动机和思路" class="headerlink" title="1 动机和思路"></a>1 动机和思路</h2><p>对比学习在无监督表征学习领域的潜力无需多言，已经有非常多的例子证明其效果，目前比较多的针对对比学习的改进包括损失函数、抽样策略、数据增强方法等多方面，但是针对负对的研究相对而言更少一些，一般在构造正负对时，大部分模型都简单的把单张图像及其增强副本作为正对，其余样本均视为负对。这一策略可能会导致的问题是模型把相距很远的样本分得很开，而距离较近的负样本对之间可能比较难被区分。</p>
<p>基于此，本文构造了一个难负对的思想，主要目的在于，把离样本点距离很近但是又确实不属于同一类的样本作为负样本，加大了负样本的难度，从而使得类与类之间分的更开，来提升对比学习模型的表现。</p>
<h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h2><h3 id="2-1-难负样本选取原则"><a href="#2-1-难负样本选取原则" class="headerlink" title="2.1 难负样本选取原则"></a>2.1 难负样本选取原则</h3><p>好的难负样本有两点原则：1）与原始样本的标签不同；2）与原始样本尽量相似。</p>
<p>这一点就与之前的对比学习有比较明显的差异了，因为对比学习一般来说并不使用监督信息，因此除了锚点之外的其他样本，不管标签如何，都被认为是负对，所以问题的一个关键在于“<strong>用无监督的方法筛出不属于同一个标签的样本</strong>”。不仅如此，这里还有一个冲突的地方，既要与锚点尽可能相似，又得不属于同一类，这对于一个无监督模型来说是有难度的，因此本文在实际实现过程中进行了一个权衡，<strong>假如对样本的难度要求不是那么高的时候，就只满足原则1，而忽略原则2</strong>。同时，这种方法应该尽量<strong>不增加额外的训练成本</strong>。</p>
<h3 id="2-2-具体方法"><a href="#2-2-具体方法" class="headerlink" title="2.2 具体方法"></a>2.2 具体方法</h3><p>本文的重点在于如何进行难负样本采样，首先作者给出难负样本的采样分布函数：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.1-1608981534224.jpg"></p>
<p>即难负样本分布以与正类类别不同为条件的概率分布，$q_β(x^-)$是正负样本点积乘以系数$β$之后的指数项再乘以单纯的负样本采样分布，$β$控制采样的难易程度，值越大，代表样本越难。点积越大，$q_β(x^-)$就越大，表示这个样本更容易被采样，结合原则2，即尽量采样困难负样本。</p>
<p>但是这里没有解决原则1的问题，对于无监督方法，我们仍然不知道该怎么确定采样到的负样本与锚点的标签是不是一致的，这里作者用PU-learning的思想，把负样本分布$q_β(x^-)$拆成来自同标签分布$q_β^+(x^-)$与来自不同标签分布$q_β^-(x^-)$的两个部分：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.2-1608982447207.jpg"></p>
<p>这里对于来自$q_β^+(x^-)$的样本同样施以原则2，即为：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.3-1608982496092.jpg"></p>
<p>所以满足原则1和原则2的难负样本分布就可以写为：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.4-1608982538530.jpg"></p>
<p>上式的第一项就是常规的负样本分布，第二项作者提到可以使用语义保留转换去估计这一分布，传统无监督对比学习做的就是这件事儿。</p>
<p>所以原始对比损失：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.5-1608983198336.jpg"></p>
<p>就可以改写为只使用难负样本的情况：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.6-1608983226868.jpg"></p>
<p>分母第二项大括号里头的第一项就是之前的总负样本，第二项就是和锚点相似度很高的来自同一类的负样本。所以只需要求出分母中的两个均值，这个损失就算是给出来了。</p>
<p>计算这两项均值的方法，作者用的是蒙特卡洛重要性采样：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.7-1608983580203.jpg"></p>
<p>式子右边分母的$Z_β$和$Z_β^+$是两个配分函数，可以用均值近似:</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.8-1608983656405.jpg"></p>
<p>作者提出，在pytorch框架下，这个操作只需要额外两行代码九就能搞定，不需要做另外的操作。</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h2><p>作者以SimCLR为Baseline在几个数据集上进行了实验，给出了对应的实验效果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.9-1608984262715.jpg"></p>
<p>发现使用这种难负样本机制有所改善。</p>
<p>第二点是作者把训练好的，模型用到下游分类任务中，给出分类精度：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.10-1608984310866.jpg"></p>
<p>在部分数据集上比baseline有所提升。</p>
<p>但是这个β值并不是越大越好的，过大可能也存在问题：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.11-1608984359417.jpg"></p>
<p>可以发现，在β设置得较大的时候，模型的质量甚至会下降。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/24/Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/24/Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology/" itemprop="url">《Evaluating  the Disentanglement of Deep Generative Models through Manifold Topology》阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-12-24T18:06:57+08:00">
                2020-12-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-动机"><a href="#1-动机" class="headerlink" title="1 动机"></a>1 动机</h2><p>解耦工作对于模型的泛化、鲁棒性和可解释性来说至关重要，然而由于现有的解耦评测方法通常依赖于训练额外的、新的生成模型（分类器、编码器、回归器等）或在特定数据集以及使用特定方法预处理过的数据集上进行验证，因此评价结果的可靠性较差，且往往与任务较为相关，适用范围有限，也导致了用不同指标评测解耦模型时会出现多种排名结果。</p>
<h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h2><p>本文所提出的评价方法基于拓扑学原理，理论上较为抽象，在实现上，测量每个潜在维度（$z_i$）上的条件子流形的持续同调（TDA, <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31734839%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%EF%BC%8C%E6%9C%AC%E6%96%87%E7%9A%84%E8%AF%84%E4%BB%B7%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%E5%8D%B3%E5%9C%A8%E4%BA%8E%EF%BC%8C**%E4%BB%A5%E8%A7%A3%E8%80%A6%E5%9B%A0%E5%AD%90%E4%B8%BA%E6%9D%A1%E4%BB%B6%E7%9A%84%E6%BD%9C%E5%9C%A8%E5%AD%90%E6%B5%81%E5%BD%A2%E4%B9%8B%E9%97%B4%E5%85%B7%E6%9C%89%E6%AF%94%E4%BB%A5%E7%BA%A0%E7%BC%A0%E5%9B%A0%E5%AD%90%E4%B8%BA%E6%9D%A1%E4%BB%B6%E7%9A%84%E6%BD%9C%E5%9C%A8%E5%AD%90%E6%B5%81%E5%BD%A2%E4%B9%8B%E9%97%B4%E5%85%B7%E6%9C%89%E6%9B%B4%E9%AB%98%E7%9A%84%E6%8B%93%E6%89%91%E7%9B%B8%E4%BC%BC%E5%BA%A6**%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C%E9%87%87%E7%94%A8W.RLT%E8%B7%9D%E7%A6%BB%E5%8E%BB%E5%BA%A6%E9%87%8F%E8%BF%99%E4%B8%AA%E6%8B%93%E6%89%91%E7%9B%B8%E4%BC%BC%E6%80%A7%E3%80%82">https://zhuanlan.zhihu.com/p/31734839），然后，本文的评价核心思想即在于，**以解耦因子为条件的潜在子流形之间具有比以纠缠因子为条件的潜在子流形之间具有更高的拓扑相似度**。因此，采用W.RLT距离去度量这个拓扑相似性。</a></p>
<p>本文所采用的从数据样本空间估计拓扑空间同调的方法是相对存在时间（Relative Living Times，RLTs），为了获取RLTs，如下图所示，首先假定训练好的生成模型分布$p_{model}(x)$由一个带孔的流形$M_{model}$支撑，从$p_{model}(x)$中采样出样本点集合$X$，从而得到一个基础的单纯形复合物，改变每个样本点的阈值（即图(d)中每个点的半径）用欧氏距离度量点与点之间的临近测度，在逐渐增大阈值时，会产生不同数量的k维孔（拓扑学中度量同调的重要依据），最终实现对持久性同调的逼近，得到持久性条形码（连续同调评价的产物）。</p>
<p>RLTs则是在改变阈值以生成的多个持久性条形码的矢量化，反映了每个k维孔出现和小时的持续时间内的离散分布，对RLTs取均值，则可以得到这些k维孔的平均相对生成时间，为一个离散概率分布，<strong>测量两个数据样本集之间的平均相对生成时间分布的相似性，就可以作为两个样本集之间拓扑相似性的评价依据</strong>。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AEvaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.1.jpg"></p>
<p>为了说明其理论的有效性，作者可视化了解耦和非解耦的生成模型在dsprites数据集上的拓扑结构：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AEvaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.2.jpg"></p>
<p>可以发现，解耦模型在尺寸和转角两个因子下的子流形明显不同胚，转角子流形有一维孔，旋转则没有，而每个因子内部的各个子流形是同胚的，而不解耦模型则没有这个性质。</p>
<p>传统的基于RLTs的拓扑相似性度量使用欧氏距离，但是经验上发现使用Wasserstein距离能显著改善度量精度。因此，作者基于W重心提出<strong>W.RLTs度量方法</strong>：</p>
<h3 id="度量思想："><a href="#度量思想：" class="headerlink" title="度量思想："></a>度量思想：</h3><p>前面已经提到，采样后得到的RLTs代表k维空穴存在与否的离散分布，首先求出这个离散分布的重心<img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.3.jpg" style="zoom:80%;" />：分布重心定义为“使用W-2距离计算到达分布中所有点的最小总距离所在的位置”，计算公式如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AEvaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.4.jpg"></p>
<p>上面的$λ$是一个权重参数，其和为1。</p>
<p>在求出RLTs的重心之后，从两个角度评价其解耦性能的好坏：</p>
<p>1、 同一个因子内部的条件子流形应该有较高的拓扑相似性；</p>
<p>2、 不同因子的条件子流形具有较低的拓扑相似性。</p>
<p>因此，在某一因子$s_i$为条件下生成的条件子流形下，控制因子$s_i$的值不变，而其他因子的值可以变化，得到一个同一因子下的集群，测量这一集群内的拓扑相似性作为这一因子下的条件子流形的拓扑相似性结果；同时，测量不同因子作为条件的子流形之间的拓扑相似性，作为因子间拓扑相似性结果。作者在CelabA数据集上可视化了W.RLTs结果来说明这一思想，下图中的第一行是同一个因子内部的连续同调W.RLTs，下一行是不同因子之间的W.RLTs：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AEvaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.5.jpg"></p>
<h4 id="无监督评价方法"><a href="#无监督评价方法" class="headerlink" title="无监督评价方法"></a>无监督评价方法</h4><p>在评价时，由于实际的因子$s_i$与实验设置的因子$z_j$之间可能并不存咋一一对应的连结，因此，本文使用W距离计算每个因子$z_j$为条件的子流形的拓扑相似性，也就是使用W距离计算不同条件子流形之间的分布重心的距离，得到一个$j<em>j$维的拓扑相似性矩阵$M$，在得到拓扑相似性矩阵之后，使用奇异值分解进行频谱共聚类，其目的在于合并同态的以设置因子$z$为条件的子流形，使得$z$可以与实际因子$s$相对应，即合并解耦作用相似的因子，降维。由此得到最终的$c</em>c$维的共聚类相似性矩阵$M_c$。作者可视化了不同模型在不同数据集下的共聚类相似性矩阵（颜色越深，代表相似性越强，对角线即上即为同一条件子流形的同态相似性可视化结果）：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AEvaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.6.jpg"></p>
<p>之后，最小化共聚类相似性矩阵$M_c$的聚类内方差和聚类间方差，去求得最终的聚类数量c，使用共聚类相似性矩阵$M_c$，计算解耦得分$μ$。解耦得分定义为聚类内相似性和聚类间相似性之差：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AEvaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.7.jpg"></p>
<p>因此，一个解耦效果较好的模型，想要获得较高的解耦得分，则希望有尽量大的聚类内相似性，同时聚类间的相似性尽量小，类间相似性非常直观，就是$M_c$对角元上的值，因此：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AEvaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.8.jpg"></p>
<p>同样的，类间相似性就是$M_c$剔除对角元元素之外的值：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AEvaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.9.jpg"></p>
<p>这就是无监督方法的聚类解耦评估方法。</p>
<h4 id="有监督评价方法"><a href="#有监督评价方法" class="headerlink" title="有监督评价方法"></a>有监督评价方法</h4><p>与上面的无监督方法相比，有监督方法的改动在于不再比较不同给定因子$z_j$，之间的拓扑相似性，而是对于有标签的数据集，同时计算实际因子$s_i$的W.RLTs，因此，在评价时，只计算$z_j$与$s_i$之间的拓扑相似性，得到$j*i$维的矩阵$M$，同样进行奇异值分解（具体怎么做没有再说明），因此类间相似性和类内相似性的计算如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AEvaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.10.jpg"></p>
<p>与无监督指标不同的地方在于，有监督指标对计算结果进行了归一化处理：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AEvaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.11.jpg"></p>
<h4 id="方法的限制"><a href="#方法的限制" class="headerlink" title="方法的限制"></a>方法的限制</h4><p>本评价方法假定数据流形是不完全对称的，因此没有考虑对称流形的情况，同时，RLTs不能计算数据流形的完整拓扑，而是进行逼近。</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h2><p>作者将本文提出的方法与MIG，一个使用一个分类器的方法Disentanglement和一个专门对人脸数据库解耦表现做评估的PPL方法进行比较，对不同解耦模型做排名，得到与其他指标相似的排名结果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AEvaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.12.jpg"></p>
<p>大部分指标排名都比较相似，然而对于β-VAE，本文方法的指标与MIG相比差异较大，但是两种指标评价β-VAE时都有较大的方差，因此说明β-VAE的表现可能不是很稳定。同时还发现了两种训练目标比较相似的模型可能在解耦表现上有较大差异（Factor VAE和β-VAE）。</p>
<p>同时报告了定量结果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AEvaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.13.jpg"></p>
<p>作者指出，大部分情况下，无监督评估指标更为适用，尤其对于CelebA，由于脸部信息过多，不易于用有监督方法进行评价，而如果想评估特定的解耦（发色，眼镜），用有监督的方法会更合适。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/22/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/22/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/" itemprop="url">四篇图像解耦工作简要介绍</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-12-22T19:20:07+08:00">
                2020-12-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-Variational-Interaction-Information-Maximization-for-Cross-domain-Disentanglement"><a href="#1-Variational-Interaction-Information-Maximization-for-Cross-domain-Disentanglement" class="headerlink" title="1 Variational Interaction Information Maximization for Cross-domain Disentanglement"></a>1 Variational Interaction Information Maximization for Cross-domain Disentanglement</h2><p>这篇文章的思想是使用信息论知识，实现跨域图像解耦表示，是基于VAE的一个改进工作。</p>
<p>对于图像对x，y，二者之间既有共享的表征信息，也有不共享的表征信息，这篇文章提了如下图所示的架构，训练一个VAE，同时学到X的特定表征，Y的特定表征以及X与Y之间的共享表征，这一目的通过最大化X与Y的联合分布之间的边际似然函数实现：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/QQ%E5%9B%BE%E7%89%8720201222160929.png"></p>
<p>在实际应用中直接优化上述公式有些困难，作者还做了其他的简化表达，经过改进后，其损失函数的思想在于，希望每个数据之前特异的特征被编码到各自单独的编码器$Z_x,Z_y$之中，而二者之间相互共享的表征则被同一个编码器所$Z_s$编码，为了实现这一目的（三类表征之间尽量分开），作者引入了互信息思想，希望尽可能最小化$I(Z_s, Z_x)$与$I(Z_s, Z_y)$来实现共享表征与特定表征之间的分离，具体实施时使用了如下图所示的公式：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/QQ%E6%88%AA%E5%9B%BE20201222162104.jpg"></p>
<p>上面这个公式的第一项的作用在于鼓励$Z_s, Z_x$联合向共享域X中提供信息（与数据集X保持紧密联系），后两项的目的在于减少二者之间的信息总量。</p>
<p>同时，为了鼓励共享表征和特异表征之间的分离，还构造了如下的互信息正则化项：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.jpg"></p>
<p>上面两个等式中，最大化第一项意味着共享表征中含有来自数据集中提取到的表征，最小化第二项意味着从一个数据集中提取到的表征可以很容易地从另一个数据集中推断出来，意味着这个表征是共享的。</p>
<p>下图中的编码器r是设计来为下游任务（图像翻译和检索）使用的。</p>
<img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/1.0.jpg" alt="img;" style="zoom:;" />

<p>解耦效果：</p>
<img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/1.4.jpg" style="zoom:67%;" />

<p>定量评估没有说明其具体的解耦指标得分，主要是对跨域图像检索进行的评估。</p>
<h2 id="2-ICAM-Interpretable-Classification-via-Disentangled-Representations-and-Feature-Attribution-Mapping"><a href="#2-ICAM-Interpretable-Classification-via-Disentangled-Representations-and-Feature-Attribution-Mapping" class="headerlink" title="2 ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping"></a>2 ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping</h2><p>一篇在医学影像领域的解耦应用，主要使用GAN-VAE架构，用来鉴别正常人与病患身体结构的差异性，进行影像诊断。网络框架如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/2.1.jpg"></p>
<p>简要介绍网络各个组成部分的作用：</p>
<p>内容编码器$E^c$：编码输入图像对中共享的与类别无关的信息，用鉴别器对编码特征进行判断，希望编码器对图像对的两张图像的共享信息输入趋同的特征；</p>
<p>属性编码器$E^a$：编码类别相关特征，用来分类；</p>
<p>生成器G：以上述两个编码器输出的特征作为联合输入，目的在于输出受内容特征与属性特征共同控制的图像；</p>
<p>特征映射$Attr Map$：定位类间差异区域。</p>
<p>整个框架基于GAN网络，同时引入VAE思想，使用编码器编码的特征作为重构输入而不是随机噪声，在生成虚假图像后再次进行一次图像生成，用二次生成的图像与真实图像之间的差异性作为最根本的损失。通过这一结构，其目的在于挖掘出决定相似图像类别差异的特征，实现类别与无关特征之间的解耦。</p>
<h2 id="3-Elastic-InfoGAN-Unsupervised-Disentangled-Representation-Learning-in-Class-Imbalanced-Data"><a href="#3-Elastic-InfoGAN-Unsupervised-Disentangled-Representation-Learning-in-Class-Imbalanced-Data" class="headerlink" title="3 Elastic-InfoGAN:  Unsupervised Disentangled Representation Learning in Class-Imbalanced Data"></a>3 Elastic-InfoGAN:  Unsupervised Disentangled Representation Learning in Class-Imbalanced Data</h2><p>本文的贡献是用InfoGAN实现对类不平衡数据的解耦。</p>
<p>InfoGAN假设数据服从均匀分布，因此在类别不平衡的数据中解耦表现较差：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.1.jpg"></p>
<p>这篇文章针对这一问题对InfoGAN做了两个改进，其一是不对数据分布进行假设，而将其视为优化过程中的可学习参数，为了实现这一点，采用Gumbel-Softmax分布作为噪声的潜在分布，该分布有可微参数，因此可以进行更新；其二是通过实验发现InfoGAN在类不平衡信息中很容易学到图像的低级特征（与之前分享的解释对比学习工作的发现有异曲同工之妙），因此这篇文章引入对比学习的思想，对数据进行增强，强迫模型学习身份表示，以抑制类不平衡的影响。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.2.jpg"></p>
<p>整篇文章的工作重点就是上述的两个方面，其一，用可学习分布代替InfoGAN假设的均匀分布，优化InfoGAN的同时更新分布（左图）。k维类别潜码的采样方法如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.3.jpg"></p>
<p>$g_i$代表从Gumbel（0，1）分布采样的样本点，温度参数代表不同类之间的相似程度，假如温度参数很小，将会趋近于onn-hot编码（均匀分布）。</p>
<p>其二，使用简单数据增强方法给数据构造一个正对，同时引入负对，添加一个对比损失项，强迫模型学习身份表示。使用的是常规的对比损失：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.4.jpg"></p>
<p>最终的损失为InfoLoss和对比损失之和：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.5.jpg"></p>
<p>定性实验结果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.6.jpg"></p>
<p>定量实验结果：</p>
<p>使用NMI和ENT（平均熵，评价同一个潜码生成的图像是否属于同一类；每一个潜码是否只与一个真实类别标签关联；越小越好）作为评价指标：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.7.jpg"></p>
<h2 id="4-WAE模型"><a href="#4-WAE模型" class="headerlink" title="4 WAE模型"></a>4 WAE模型</h2><p>ICLR2021中有两篇论文在WAE（WASSERSTEIN AUTOENCODER）的框架下进行解耦图像生成，WAE是2018年由Google在WGAN的基础上提出来的一种自编码器模型，由于目前没有了解其原理，因此只对这两篇论文在WAE基础上的改进进行简单的介绍。</p>
<h3 id="4-1-Learning-disentangled-representations-with-the-Wasserstein-Autoencoder"><a href="#4-1-Learning-disentangled-representations-with-the-Wasserstein-Autoencoder" class="headerlink" title="4.1 Learning  disentangled representations with the Wasserstein Autoencoder"></a>4.1 Learning  disentangled representations with the Wasserstein Autoencoder</h3><p>想法是把β-TCVAE的构造移植到WAE模型中，重点在于利用TCVAE的loss对WAE进行改进：</p>
<p>TCWAE loss：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.1.jpg"></p>
<p>β-TCVAE loss：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.2.jpg"></p>
<p>对比两项损失，可以发现TCWAE具有与β-TCVAE几乎相同的loss函数，区别在于没有最后一个互信息项，以及在第一项的度量上有所差异。</p>
<p>效果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.3.jpg"></p>
<h3 id="4-2-DISENTANGLED-RECURRENT-WASSERSTEIN-AUTOENCODER"><a href="#4-2-DISENTANGLED-RECURRENT-WASSERSTEIN-AUTOENCODER" class="headerlink" title="4.2 DISENTANGLED RECURRENT WASSERSTEIN AUTOENCODER"></a>4.2 DISENTANGLED RECURRENT WASSERSTEIN AUTOENCODER</h3><p>第二篇WAE相关的文章是将WAE应用于时序图像解耦的工作，用来捕捉时序图像上的相关信息，实现静态因子和动态因子的解耦。</p>
<p><img src=""><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.4.jpg" alt="4.4" style="zoom:67%;" /></p>
<p>上图是这篇文章提出了来的模型的解耦效果，每一行都代表一个时序（对应不同表情）。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">微澜</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
