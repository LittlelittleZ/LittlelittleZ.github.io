<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="小舟从此逝，江海寄余生。">
<meta property="og:type" content="website">
<meta property="og:title" content="缓缓行舟">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="缓缓行舟">
<meta property="og:description" content="小舟从此逝，江海寄余生。">
<meta property="og:locale">
<meta property="article:author" content="微澜">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/"/>





  <title>缓缓行舟</title>
  








<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">缓缓行舟</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">给行船途中的所感所获一个容身之处</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/19/High-Frequency_Component_Helps_Explain_the_Generalization_of_Convolutional_Neural_Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/19/High-Frequency_Component_Helps_Explain_the_Generalization_of_Convolutional_Neural_Networks/" itemprop="url">《High-Frequency Component...》 阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-19T22:14:37+08:00">
                2021-01-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="High-Frequency-Component-Helps-Explain-the-Generalization-of-Convolutional-Neural-Networks"><a href="#High-Frequency-Component-Helps-Explain-the-Generalization-of-Convolutional-Neural-Networks" class="headerlink" title="High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks"></a>High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks</h1><p><strong>题目</strong>：高频成分有助于CNN泛化能力的解释</p>
<p><strong>来源</strong>：CVPR 2020  oral 卡内基梅隆大学</p>
<h2 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1. Motivation"></a>1. Motivation</h2><p>1) CNN的泛化能力不直观，不能直接对其进行解释。</p>
<p>2) 目前对CNN泛化能力进行解释的方法有针对随机梯度下降、不同的复杂性度量等方面的研究，本文是基于傅里叶变换的角度，从频域入手解释CNN进行特征提取的性质。</p>
<h2 id="2-Contribution"><a href="#2-Contribution" class="headerlink" title="2. Contribution"></a>2. Contribution</h2><p><strong>核心观点</strong>：CNN可以比人类从更高粒度的层面观察图像的特征，换句话说，就是CNN可以从高频信息中对图像进行观察，而人类只能从低频信息中进行观察，这一差异导致了CNN会做出对人类而言不够直观的泛化表现。</p>
<p>基于以上观点，本文展示了：</p>
<p>1) CNN如何利用高频成分在鲁棒性以及准确性上进行权衡；</p>
<p>2) 以图像频谱为工具，提出了一些假设解释CNN的几种泛化行为；</p>
<p>3) 从频率角度给出提升CNN对简单攻击鲁棒性的方法。</p>
<h2 id="3-CNN会在鲁棒性和准确性上进行衡量"><a href="#3-CNN会在鲁棒性和准确性上进行衡量" class="headerlink" title="3. CNN会在鲁棒性和准确性上进行衡量"></a>3. CNN会在鲁棒性和准确性上进行衡量</h2><p><strong>结论</strong>：在通过样本训练出的CNN网络中，一定存在一个样本，使得CNN在任意距离度量方法与鲁棒性阈值下都无法同时满足鲁棒性和准确性均为1的要求，从而导致CNN会在二者之间进行一定的权衡。</p>
<p>为了证明这一结论，本文提出了两个<strong>假设</strong>：</p>
<p>假设1：在对图像进行观察时，人类只能从低频信息中做出预测，而CNN网络同时会利用高频和低频信息。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E7%9B%B8%E5%85%B3/High-Frequency%20Component.../1.1.jpg" style="zoom: 67%;" /></p>
<p>（人类倾向于使用数据的语义信息（低频）对图像进行观察，而CNN不仅会使用低频的语义信息，还能使用与语义信息具有特定相关性的高频信息联合对图像进行观察）</p>
<p>假设2：对于一个CNN模型而言，必然存在一个样本$&lt;\mathrm{x}, \mathrm{y}&gt;$，使得：$f(\mathrm{x} ; \theta) \neq f\left(\mathrm{x}_{l} ; \theta\right)$</p>
<p>假设的<strong>证明</strong>：</p>
<p>作者用ResNet18在Cifar10上分别使用图像的高频和低频部分进行预测，实验结果发现CNN模型在低频成分上的预测很不准确，高频成分的预测结果与原始图像更加吻合，同时证明了假设1和假设2的成立。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E7%9B%B8%E5%85%B3/High-Frequency%20Component.../1.2.jpg" alt=""></p>
<p>(上图中，第一列为原始图像及对应的预测结果，第二列为图像的低频成分，第三列为高频成分。)</p>
<h3 id="3-1-CNN会利用高频组件"><a href="#3-1-CNN会利用高频组件" class="headerlink" title="3.1 CNN会利用高频组件"></a>3.1 CNN会利用高频组件</h3><p>图像信息$x$可以视为是高频信息$x_h$与低频信息$x_l$的组合：</p>
<script type="math/tex; mode=display">
x=[x_h, x_l]</script><p>对于高频和低频信息，首先将$x$进行傅里叶变换，得到频域表示$z$，之后使用一个阈值函数$t(\mathrm{z} ; r)$进行高频分量和低频分量的区分，将高频分量和低频分量再使用傅里叶逆变换回时域，即得到高频和低频信息：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathrm{z}=\mathcal{F}(\mathrm{x}), & \mathrm{z}_{l}, \mathrm{z}_{h}=t(\mathrm{z} ; r) \\
\mathrm{x}_{l}=\mathcal{F}^{-1}\left(\mathrm{z}_{l}\right), & \mathrm{x}_{h}=\mathcal{F}^{-1}\left(\mathrm{z}_{h}\right)
\end{aligned}</script><p>$t(\mathrm{z} ; r)$具有一个超参数$r$，具体实现时，通过计算每一个像素点$(i, j)$与其傅里叶变换之后的中心$(c_i, c_j)$之间的欧氏距离，作为区分依据：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\mathbf{z}_{l}(i, j)=\left\{\begin{array}{ll}
\mathbf{z}(i, j), & \text { if } d\left((i, j),\left(c_{i}, c_{j}\right)\right) \leq r \\
0, & \text { otherwise }
\end{array}\right. \\
\mathbf{z}_{h}(i, j)=\left\{\begin{array}{ll}
0, & \text { if } d\left((i, j),\left(c_{i}, c_{j}\right)\right) \leq r \\
\mathbf{z}(i, j), & \text { otherwise }
\end{array}\right.
\end{array}</script><p>根据假设1，人类对图像进行预测的方法为：</p>
<script type="math/tex; mode=display">
\mathbf{y}:=f(\mathbf{x} ; \mathcal{H})=f\left(\mathbf{x}_{l} ; \mathcal{H}\right)</script><p>CNN网络的预测模式为：</p>
<script type="math/tex; mode=display">
\underset{\theta}{\arg \min } l(f(\mathbf{x} ; \theta), \mathbf{y})=
\underset{\theta}{\arg \min } l(f(\{\mathbf{x}_l, \mathbf{x}_h \}; \theta), \mathbf{y})</script><p>基于上述推断，可以得到以下结论：</p>
<p>1) CNN会挖掘高频成分，并在此基础上进行泛化，由于高频信息难以被人类观察，导致了CNN的泛化能力对于人类而言很不直观；</p>
<p>2) 假如针对高频成分进行扰动生成对抗性样本，会导致CNN的预测结果不准确，而对于人类而言没有什么变化。</p>
<h3 id="3-2-CNN会在鲁棒性与准确性之间进行权衡"><a href="#3-2-CNN会在鲁棒性与准确性之间进行权衡" class="headerlink" title="3.2 CNN会在鲁棒性与准确性之间进行权衡"></a>3.2 CNN会在鲁棒性与准确性之间进行权衡</h3><p>本文定义的CNN预测准确性：</p>
<script type="math/tex; mode=display">
\mathbb{E}_{(\mathbf{x}, \mathbf{y})}\alpha\left(f\left(\mathbf{x} ; \theta\right), \mathbf{y}\right)</script><p>鲁棒性：</p>
<script type="math/tex; mode=display">
\mathbb{E}_{(\mathbf{x}, \mathbf{y})} \min _{\mathbf{x}^{\prime}: d\left(\mathbf{x}^{\prime}, \mathbf{x}\right) \leq \epsilon} \alpha\left(f\left(\mathbf{x}^{\prime} ; \theta\right), \mathbf{y}\right)</script><p>根据假设1和假设2，必然存在一个样本$&lt;\mathrm{x}, \mathrm{y}&gt;$，不能同时满足准确性和鲁棒性均为1，故而CNN会对二者进行一个权衡。</p>
<h2 id="4-从数据角度挖掘CNN的行为"><a href="#4-从数据角度挖掘CNN的行为" class="headerlink" title="4. 从数据角度挖掘CNN的行为"></a>4. 从数据角度挖掘CNN的行为</h2><p>目前的研究表明，CNN对数据具有记忆能力，这意味着理论上CNN是可以通过记忆数据的信息满足训练精度的要求，而通常CNN并不采用简单的记忆来学习数据的特征，而是在鲁棒性与准确性之间进行权衡，保证一定的泛化能力。</p>
<p>此外，已经有研究发现CNN对于标签混淆数据（标签和图像类别不一一对应）的学习能力很强，根据前面的假设，正确标注的标签与图像的低频信息相关联，而标签混淆数据的低频信息不再与图像标签相对应，但是CNN仍然可以学得很好。</p>
<p>基于以上两个发现，本文对CNN的数据学习模式做了以下推理：</p>
<p>在正确标注的样本中，CNN倾向于先学习低频信息，再逐渐提取高频信息，以提升精度；</p>
<p>在混淆标签样本中，低频信息不再与标签相关联，所以模型对于低频信息与高频信息不再区别对待，意味着CNN开始记忆数据本身的信息。</p>
<p>（可以理解为对于正确标注的样本，CNN可以像做题一样建立理论与实际题目之间的相关性，而对于标签混淆样本，CNN就像面对不会做的题，为了应付考试只能把题背下来。）</p>
<p>为了验证以上想法的正确性，作者分别利用正确标注和混淆标注的CIFAR10数据的低频信息，在ResNet18上训练分类器，观察其收敛情况，如下图所示：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E7%9B%B8%E5%85%B3/High-Frequency%20Component.../1.3.png" style="zoom: 67%;" /></p>
<p>从上图中可以发现，标签混淆数据的模型收敛速度比正常标注的模型更慢，可以认为CNN确实更倾向于从信息中学习，而不是记忆；而在包含更少的低频信息时（r=4/8），正常标注数据训练的模型精度比标签混淆数据更高，而当r较大时，二者的精度没有明显差异，这意味着CNN优先从低频信息进行学习，而标签混淆信息没有低频信息与标签之间的学习关系，只能将低频信息与高频信息同等处理，在r更小，总的信息量更少的情况下，学习效果更差。（作者在MNIST，FashionMNIST，ImageNet上都进行了类似实验，得到差不多的结果）</p>
<p><strong>这里衍生出一个新的问题，即CNN通过对混淆标签数据的学习表明其可以从高频和低频信息中提取到有效信息，但是为什么CNN仍然倾向于从低频信息中学习呢？</strong></p>
<p>作者认为是因为标签信息是人工标注的，即人为地把低频信息与标签关联起来了，因此对于给定标签信息的数据，低频信息对CNN loss的影响更大。</p>
<p>为了验证这一观点，作者在正确标注数据下，用图像的高频和低频成分分别训练了一个分类器，且使其具有较高的预测精度，之后用这两个分类器在原始测试集上进行预测，最后发现在低频成分上训练的分类器泛化能力显著强于高频成分的分类器，证明了其猜想的合理性：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E7%9B%B8%E5%85%B3/High-Frequency%20Component.../1.4.png" alt=""></p>
<h2 id="5-从频率角度研究训练手段的作用"><a href="#5-从频率角度研究训练手段的作用" class="headerlink" title="5.从频率角度研究训练手段的作用"></a>5.从频率角度研究训练手段的作用</h2><h3 id="5-1-Batch-Size"><a href="#5-1-Batch-Size" class="headerlink" title="5.1 Batch Size"></a>5.1 Batch Size</h3><p>作者设置了不同的batch size，并使用不同的频率成分数据进行训练，得到下图的测试结果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E7%9B%B8%E5%85%B3/High-Frequency%20Component.../1.5.png" alt=""></p>
<p>观察“Train”与“Test”两条曲线，可以发现小的batch size有助于提高精度，而大batch size有助于缩小模型泛化差距；观察代表高频成分的点状线，发现更大的batch size中，不同r值的预测精度差距更小 ，意味着大batch size中高频成分的变化更小，从而缩小了泛化差距；观察代表低频成分的实线，发现大batch size使得loss更陡峭，可能是因为大batch size中包含更多的低频可归纳信息（没有很理解这一点）。</p>
<h3 id="5-2-训练技巧"><a href="#5-2-训练技巧" class="headerlink" title="5.2 训练技巧"></a>5.2 训练技巧</h3><p>作者比较了Dropout，Mix-up，BatchNorm和Adversarial Training四种方法对模型预测精度的影响：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E7%9B%B8%E5%85%B3/High-Frequency%20Component.../1.6.png" alt=""></p>
<p>发现Dropout和Mix-up对精度影响不大，但可以看出Mix-up捕获了更多的高频信息，对抗性学习的引入会降低精度，但是缩小了泛化差距（诱导CNN重视鲁棒性，进而牺牲精度）。</p>
<p>BN加速了模型的收敛速度，同时有助于捕获高频成分信息。作者指出，BN的核心作用就是平衡不同频率分量的比重，一般来说，高频信息比低频信息低几个量级，因此，BN注重于对高频成分的捕捉，为了说明这一点，作者对BN进行了进一步的实验：</p>
<p>作者对比了加BN与不加BN的网络，在只使用低频成分训练时的表现，发现加了BN之后模型没有明显提升：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E7%9B%B8%E5%85%B3/High-Frequency%20Component.../1.7.png" alt=""></p>
<h3 id="5-3-Networks"><a href="#5-3-Networks" class="headerlink" title="5.3 Networks"></a>5.3 Networks</h3><p>作者比较了LeNet、AlexNet、VGG、ResNet，最终发现ResNet有更好的准确率，更小的泛化差距和更弱的捕捉高频信息的能力。</p>
<h3 id="5-4-Optimizer"><a href="#5-4-Optimizer" class="headerlink" title="5.4 Optimizer"></a>5.4 Optimizer</h3><p>作者比较了SGD、ADAM、AdaGrad、AdaDelta、RMSprop五种优化器，发现SGD更倾向于捕捉高频信息，其它的没有明显差异。</p>
<h2 id="6-从对抗攻防研究卷积核"><a href="#6-从对抗攻防研究卷积核" class="headerlink" title="6. 从对抗攻防研究卷积核"></a>6. 从对抗攻防研究卷积核</h2><p>根据前面的研究已经知道，对抗性学习有助于提升模型的鲁棒性，作者通过研究发现，与一般的CNN模型相比，经过对抗性学习的模型具有<strong>更加光滑</strong>的卷积核，可视化结果如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E7%9B%B8%E5%85%B3/High-Frequency%20Component.../1.8.png" alt=""></p>
<p>卷积核的光滑度与频率有什么关系呢？更加光滑的卷积核意味着卷积核内部的权重之间具有更小的突变，在对特征或图像进行卷积时，将会产生更少的高频信息。</p>
<p>由此很容易得知，若想增加模型的鲁棒性，可以通过一定手段改善卷积核的光滑程度，作者提出了如下的trick：</p>
<p>定义卷积核中的每个元素都有八个相邻元素，将当前位置的权值加上临近八个相邻位置的权值即可使得卷积核更加光滑：</p>
<script type="math/tex; mode=display">
\mathbf{w}_{i, j}=\mathbf{w}_{i, j}+\sum_{(h, k) \in \mathcal{N}(i, j)} \rho \mathbf{w}_{h, k}</script><p>上图中经过光滑处理之后的卷积核的可视化结果如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E7%9B%B8%E5%85%B3/High-Frequency%20Component.../1.9.png" alt=""></p>
<p>作者使用这两个方法处理了两个模型，发现进行卷积核光滑操作后，模型精度下降，但是无论是对抗性模型还是单纯模型的鲁棒性均有所提升：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E7%9B%B8%E5%85%B3/High-Frequency%20Component.../2.0.png" alt=""></p>
<h2 id="7-低频成分对目标检测任务的影响"><a href="#7-低频成分对目标检测任务的影响" class="headerlink" title="7. 低频成分对目标检测任务的影响"></a>7. 低频成分对目标检测任务的影响</h2><p>作者将频率方法应用于目标检测任务时，没能很好地对其进行解释：</p>
<p>在只使用低频成分进行目标检测时，模型在下图的上半部分获得了更低的MAP值，而在下半部分的图像中获得了更好的MAP结果，而对于人类而言没有明显区别，显示出CNN与人类思维模式的仍有差异。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E7%9B%B8%E5%85%B3/High-Frequency%20Component.../2.1.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/19/Mining%20Cross-Image%20Semantics%20for%20Weakly%20Supervised%20Semantic%20Segmentation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/19/Mining%20Cross-Image%20Semantics%20for%20Weakly%20Supervised%20Semantic%20Segmentation/" itemprop="url">《Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation》 阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-19T22:14:29+08:00">
                2021-01-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Mining-Cross-Image-Semantics-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Mining-Cross-Image-Semantics-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation"></a>Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation</h1><p>题目：挖掘用于弱监督语义分割模型的跨图像语义。</p>
<p>来源：CVPR2020</p>
<h2 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1 Motivation"></a>1 Motivation</h2><p>1) 基于深度学习的语义分割模型取得了很好的表现，但是依赖于大量精确标注数据的特性导致了高昂的训练成本。</p>
<p>2) 弱监督语义分割模型对标签数据的依赖性较低，但是容易忽略图像的总体信息而过度关注局部。</p>
<p>3) 目前针对WSSS模型的改进方法通常只使用单一的图像信息挖掘对象模式，而忽略了图像间的语义相似性。</p>
<h2 id="2-Contribution"><a href="#2-Contribution" class="headerlink" title="2 Contribution"></a>2 Contribution</h2><p>1) 构建了一个共注意力分类器将跨图像语义相关有应用于完全目标模式学习与目标位置推断。</p>
<p>2) 共注意力分类器分别通过共注意力与对比注意力从跨图像语义的相似性和差异性中挖掘出完整的监督信息。</p>
<p>3) 该方法可以通用于多种不同监督信息的WSSS模型。</p>
<h2 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3 Approach"></a>3 Approach</h2><p>本文的实验方法与传统的弱监督语义分割模型相同，先用带有图像级标签的数据训练一个分类网络，获得相应的目标局部映射，用这个映射作为伪ground truth，用来监督语义分割网络的训练。</p>
<p>对于注意力的学习，主要分成了两个模块，其一是共注意力学习，用来驱动分类器从图像的共同关注要素中学习到公共语义，其二是一个相对独立的注意力学习，用来限制分类器关注其他非共同关注要素，以学习图像对各自内部的非共享语义信息。</p>
<h3 id="3-1-共注意力分类网络"><a href="#3-1-共注意力分类网络" class="headerlink" title="3.1 共注意力分类网络"></a>3.1 共注意力分类网络</h3><p>co-attention classifier由三个模块组成，分别是传统的分类器网络，以及一个共注意力网络和一个对比共注意力网络，整个网络框架如下图所示。三个模块为串联排列，首先是利用传统分类器进行分类训练，之后用提取到的特征训练了一个共注意力模块，用提取到的共注意力特征训练分类器对共享语义信息的分类敏感性，在共注意特征的基础上，继续训练了一个对比共注意模块，以提升分类器对图像对中的互斥语义信息的敏感性。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Mining%20Cross-Image%20Semantics/2.1.png" alt=""></p>
<h4 id="3-1-1-分类器模块训练"><a href="#3-1-1-分类器模块训练" class="headerlink" title="3.1.1 分类器模块训练"></a>3.1.1 分类器模块训练</h4><p>本模型的重点在于跨图像训练，因此网络结构是一种类似孪生网络的架构，其输入以成对的形式作为输入并进行训练。</p>
<p>首先，从图像训练集$\boldsymbol{I}=\{(\boldsymbol{I}_n,\boldsymbol{l}_n)\}_n$中采样图片对$\boldsymbol{I}_m$与$\boldsymbol{I}_n$，以及其各自的标签$\boldsymbol{l}_m$与$\boldsymbol{l}_n$，$\boldsymbol{l}_n\in{\{0,1\}^K}$是一个$K$维数据，代表图像中所具有的类别属性。之后将图像数据输入卷积神经网络中提取出各自的特征图$\boldsymbol{F}_m$与$\boldsymbol{F}_n$，$\boldsymbol{F}_{n} \in \mathbb{R}^{C \times H \times W}$。</p>
<p>在提取到feature map以后，将其送入一个全卷积层$\varphi(·)$得到类感知激活映射$\boldsymbol{S}_m$与$\boldsymbol{S}_n$，$\boldsymbol{S}_{n} \in \mathbb{R}^{K \times H \times W}$。之后再经全局池化平均(GAP)处理，得到分类器计算出的$\boldsymbol{I}_m$与$\boldsymbol{I}_n$的类别得分$s_n，s_{m} \in \mathbb{R}^{K}$，计算图像对各自的类别得分与其真实标签$\boldsymbol{l}_m$与$\boldsymbol{l}_n$之间的交叉熵，作为分类器的损失函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L}_{\text {basic }}^{\operatorname{mn}}\left(\left(\boldsymbol{I}_{m}, \boldsymbol{I}_{n}\right),\left(\boldsymbol{l}_{m}, l_{n}\right)\right) &=\mathcal{L}_{\mathrm{CE} }\left(\boldsymbol{s}_{m}, l_{m}\right)+\mathcal{L}_{\mathrm{CE}}\left(\boldsymbol{s}_{n}, l_{n}\right) \\
&=\mathcal{L}_{\mathrm{CE}}\left(\operatorname{GAP}\left(\varphi\left(\boldsymbol{F}_{m}\right)\right), \boldsymbol{l}_{m}\right)+\mathcal{L}_{\mathrm{CE} }\left(\operatorname{GAP}\left(\varphi\left(\boldsymbol{F}_{n}\right)\right), \boldsymbol{l}_{n}\right)
\end{aligned}</script><h4 id="3-1-2-跨图像语义挖掘的co-attention"><a href="#3-1-2-跨图像语义挖掘的co-attention" class="headerlink" title="3.1.2 跨图像语义挖掘的co-attention"></a>3.1.2 跨图像语义挖掘的co-attention</h4><p>这部分阐述了共注意力机制的理论知识，首先，在分类器的基础上，根据学到的图像feature map，$\boldsymbol{F}_m$与$\boldsymbol{F}_n$，计算二者之间的亲和力矩阵以衡量相关性：</p>
<script type="math/tex; mode=display">
\boldsymbol{P}=\boldsymbol{F}_{m}^{\top} \boldsymbol{W}_{\boldsymbol{P} } \boldsymbol{F}_{n} \in \mathbb{R}^{H W \times H W}</script><p>这里的$\boldsymbol{F}_m$与$\boldsymbol{F}_n$均被平铺成$C×HW$的尺寸，上式中的$\boldsymbol{W}_\boldsymbol{P}$是一个$C×C$的可学习矩阵，根据这个公式，亲和力矩阵$\boldsymbol{P}$中的每一个元素$p_{ij}$均代表了$\boldsymbol{F}_{m}$中第$i$个元素与$\boldsymbol{F}_{n}$中第$j$个元素之间的相似性。</p>
<p>之后，分别按列和行对$\boldsymbol{P}$进行softmax操作，即可得到中$\boldsymbol{F}_{m}^{m \cap n}$与$\boldsymbol{F}_{n}^{m \cap n}$每个元素的归一化注意力图$\boldsymbol{A}_{m}$与$\boldsymbol{F}_{n}$对$\boldsymbol{F}_{m}$中每个元素的归一化注意力图$\boldsymbol{A}_{n}$。</p>
<script type="math/tex; mode=display">
\boldsymbol{A}_{m}=\operatorname{softmax}(\boldsymbol{P}) \in[0,1]^{H W \times H W}, \quad \boldsymbol{A}_{n}=\operatorname{softmax}\left(\boldsymbol{P}^{\top}\right) \in[0,1]^{H W \times H W}</script><p>接着便可以计算$\boldsymbol{F}_{n}(\boldsymbol{F}_{m})$对$\boldsymbol{F}_{m}(\boldsymbol{F}_{n})$的注意力总结：</p>
<script type="math/tex; mode=display">
\boldsymbol{F}_{m}^{m \cap n}=\boldsymbol{F}_{n} \boldsymbol{A}_{n} \in \mathbb{R}^{C \times H \times W}, \quad \boldsymbol{F}_{n}^{m \cap n}=\boldsymbol{F}_{m} \boldsymbol{A}_{m} \in \mathbb{R}^{C \times H \times W}</script><p>将上式计算结果调整为$C×W×H$的尺寸，便得到了共注意力特征$\boldsymbol{F}_{m}^{m \cap n}$与$\boldsymbol{F}_{n}^{m \cap n}$。二者保留了来自对方的共同语义，并在自身的原始特征图中定位了共享语义对象。由此可以认为，在共注意图中，只对具有相同语义特征的区域有较为明显的激活反应，而对非共同特征则有较低的权值，以此帮助分类器准确定位出共享语义。</p>
<p>将提取到的共注意力特征$\boldsymbol{F}_{m}^{m \cap n}$与$\boldsymbol{F}_{n}^{m \cap n}$代替原始feature map $\boldsymbol{F}_m$与$\boldsymbol{F}_n$，带入基础分类损失中，得到共注意力分类损失的计算公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L}_{\text {co-att } }^{m n}\left(\left(\boldsymbol{I}_{m}, \boldsymbol{I}_{n}\right),\left(\boldsymbol{l}_{m}, l_{n}\right)\right)=& \mathcal{L}_{\mathrm{CE} }\left(\boldsymbol{s}_{m}^{m \cap n}, \boldsymbol{l}_{m} \cap l_{n}\right)+\mathcal{L}_{\mathrm{CE} }\left(\boldsymbol{s}_{n}^{m \cap n}, l_{m} \cap l_{n}\right) \\
=& \mathcal{L}_{\mathrm{CE} }\left(\operatorname{GAP}\left(\varphi\left(\boldsymbol{F}_{m}^{m \cap n}\right)\right), \boldsymbol{l}_{m} \cap \boldsymbol{l}_{n}\right)+\\
& \mathcal{L}_{\mathrm{CE} }\left(\operatorname{GAP}\left(\varphi\left(\boldsymbol{F}_{n}^{m \cap n}\right)\right), \boldsymbol{l}_{m} \cap \boldsymbol{l}_{n}\right)
\end{aligned}</script><p>上式旨在提升分类器对以共注意特征为输入的共享语义标签的分类结果的准确性，以使分类器更加集中于图片对的共享语义信息。</p>
<h4 id="3-1-3-用于跨图像互斥语义挖掘的contrastive-co-attention"><a href="#3-1-3-用于跨图像互斥语义挖掘的contrastive-co-attention" class="headerlink" title="3.1.3 用于跨图像互斥语义挖掘的contrastive co-attention"></a>3.1.3 用于跨图像互斥语义挖掘的contrastive co-attention</h4><p>共注意的作用在于使分类器对图像对之间的共享语义信息更敏感，而对比共注意旨在与共注意形成互补，目的是加强分类器对图像对中独属于单一图像自身的独特语义信息的敏感性。</p>
<p>根据论文的网络架构，可以发现对比共注意模型以提取到的共注意特征作为$\boldsymbol{F}_{m}^{m \cap n}$与$\boldsymbol{F}_{n}^{m \cap n}$输入，首先，对比共注意模块使用如下方法强化共注意特征中的共享语义信息（理论上，共注意特征中共享语义区域会有更大的值）：</p>
<script type="math/tex; mode=display">
\boldsymbol{B}_{m}^{m \cap n}=\sigma\left(\boldsymbol{W}_{B} \boldsymbol{F}_{m}^{m \cap n}\right) \in[0,1]^{H \times W}, \quad \boldsymbol{B}_{n}^{m \cap n}=\sigma\left(\boldsymbol{W}_{B} \boldsymbol{F}_{n}^{m \cap n}\right) \in[0,1]^{H \times W}</script><p>上式中的$\boldsymbol{W}_{B}$是一个$1×1$的卷积层，$\sigma$代表sigmoid激活函数，因此，在$\boldsymbol{B}_{m}^{m \cap n}$与$\boldsymbol{B}_{n}^{m \cap n}$中，共享语义区域比非共享语义区域具有明显更大的值。用1减去以上的值，即得到对比共注意矩阵：</p>
<script type="math/tex; mode=display">
\boldsymbol{A}_{m}^{m \backslash n}=\mathbf{1}-\boldsymbol{B}_{m}^{m \cap n} \in[0,1]^{H \times W}, \quad \boldsymbol{A}_{n}^{n \backslash m}=1-\boldsymbol{B}_{n}^{m \cap n} \in[0,1]^{H \times W}</script><p>结合整个过程，对比共注意特征相当于在强化了共注意特征之后将其围绕1取反，反向突出了除了共享语义区域之外的阴性区域，以此作为非共享语义要素的注意图。</p>
<p>回顾共注意特征的计算过程，$\boldsymbol{F}_{m}^{m \cap n}$由$\boldsymbol{F}_{n}$与$\boldsymbol{A}_{n}$计算得到，相对来说，其内部对与来自$\boldsymbol{F}_{n}$中的语义信息注意力更高，因此，经过上述处理以后，$\boldsymbol{A}_{m}^{m \backslash n}$中的注意力信息更加集中于来自$\boldsymbol{F}_{m}$的互斥语义。</p>
<p>在此基础上，计算对比共注意特征：</p>
<script type="math/tex; mode=display">
\boldsymbol{F}_{m}^{m \backslash n}=\boldsymbol{F}_{m} \otimes \boldsymbol{A}_{m}^{m \backslash n} \in \mathbb{R}^{C \times H \times W}, \quad \boldsymbol{F}_{n}^{n \backslash m}=\boldsymbol{F}_{n} \otimes \boldsymbol{A}_{n}^{n \backslash m} \in \mathbb{R}^{C \times H \times W}</script><p>“$\otimes$”表示元素间的点积操作，同样将其带入分类器损失，即可得到对比共注意力损失函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L}_{\overline{\mathrm{co}-\mathrm{att} } }^{m n}\left(\left(\boldsymbol{I}_{m}, \boldsymbol{I}_{n}\right),\left(\boldsymbol{l}_{m}, l_{n}\right)\right)=& \mathcal{L}_{\mathrm{CE} }\left(\boldsymbol{s}_{m}^{m \backslash n}, \boldsymbol{l}_{m} \backslash \boldsymbol{l}_{n}\right)+\mathcal{L}_{\mathrm{CE} }\left(\boldsymbol{s}_{n}^{n \backslash m}, \boldsymbol{l}_{n} \backslash \boldsymbol{l}_{m}\right) \\
=& \mathcal{L}_{\mathrm{CE} }\left(\operatorname{GAP}\left(\varphi\left(\boldsymbol{F}_{m}^{m \backslash n}\right)\right), \boldsymbol{l}_{m} \backslash \boldsymbol{l}_{n}\right)+\\
& \mathcal{L}_{\mathrm{CE} }\left(\operatorname{GAP}\left(\varphi\left(\boldsymbol{F}_{n}^{n \backslash m}\right), \boldsymbol{l}_{n} \backslash \boldsymbol{l}_{m}\right)\right.
\end{aligned}</script><p>该损失计算的是每个对比共注意特征的分类得分与其自身包含的互斥语义类别标签之间的交叉熵，目的在于使分类器对与对比共注意特征，将其归类为对应的互斥语义类别之中。</p>
<p>为什么要额外引入一个对比共注意呢？本质上，对比共注意是对共注意信息的互补，以便分类器可以很好地学习到图像中剩余的互斥语义信息，并且帮助分类器区分出共享语义与非共享语义。如下图，假如在共注意特征提取时，非共享语义“奶牛”被错误识别为共享语义“人类”，在对比共注意特征中，该部分将会被剔除出非共享语义注意的范围，而剩余的注意力信息不足以帮助分类器寻找到“奶牛”，从而能促使分类器对共享语义与非共享语义的区分度。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Mining%20Cross-Image%20Semantics/2.2.png" alt=""></p>
<h3 id="3-2-网络架构细节"><a href="#3-2-网络架构细节" class="headerlink" title="3.2 网络架构细节"></a>3.2 网络架构细节</h3><p>实际进行实验时，损失函数为上述三者的和，每个损失函数系数均为1，并且保证每个图像对至少有一个共同的语义标签。</p>
<script type="math/tex; mode=display">
\mathcal{L}=\sum_{m,n}{\mathcal{L}_{\text {basic } }^{\operatorname{mn} }+\mathcal{L}_{\text {co-att } }^{m n}+\mathcal{L}_{\overline{\mathrm{co}-\mathrm{att}}  }^{m n} }</script><p>本文采用了单轮前馈预测和额外参考信息的多轮共注意预测，如下图所示，第一种是一般的训练方法。对于多轮预测，具体来说就是在计算共注意分类损失时，使用了数据集中的额外相关图像（后续有做消融实验对比）。具体来说，对于含有标签$k$的某一图像$\boldsymbol{l}_{n}$，计算其与其他若干张含有标签$k$的图像关于$k$的共注意特征，并计算出每一对之间的共注意分类得分，用这个分类得分的均值作为$\boldsymbol{l}_{n}$对于标签$k$的的共注意分类结果。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Mining%20Cross-Image%20Semantics/2.3.png" alt=""></p>
<p>在获取到高质量的定位图之后，以此作为伪ground truth进行语义分割网络的训练，本文选择的分类网络是<strong>基于ImageNet预训练的VGG-16</strong>，语义分割网络是<strong>基于ResNet101的DeepLab-LargeFOV架构</strong>。</p>
<h2 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4 Experiment"></a>4 Experiment</h2><h3 id="4-1-只在PASCAL-VOC-Data上学习WSSS"><a href="#4-1-只在PASCAL-VOC-Data上学习WSSS" class="headerlink" title="4.1 只在PASCAL VOC Data上学习WSSS"></a>4.1 只在PASCAL VOC Data上学习WSSS</h3><p>本实验只使用仅含图像级标签监督信息的PASCAL VOC 2012中的图像进行训练，共包含20个类，一共10582张图片用于训练，val集包含1449张图像，test集包含1456张图像。对于val与test，使用语义分割常用的标准交并比（IoU）作为评价指标，即（目标区域与预测区域的交集）/（目标区域与预测区域的并集），具体结果如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Mining%20Cross-Image%20Semantics/2.4.png" style="zoom:80%;" /></p>
<p>作者还可视化了其语义分割结果，与PSA，OAA+进行对比，图像如下（从左到右依次是PSA,OAA+和本文方法）：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Mining%20Cross-Image%20Semantics/2.9.png" alt=""></p>
<h3 id="4-2-在PASCAL-VOC-Data和额外简单单标签数据上学习WSSS"><a href="#4-2-在PASCAL-VOC-Data和额外简单单标签数据上学习WSSS" class="headerlink" title="4.2 在PASCAL VOC Data和额外简单单标签数据上学习WSSS"></a>4.2 在PASCAL VOC Data和额外简单单标签数据上学习WSSS</h3><p>本实验在PASCAL VOC 2012与额外的简单单标签数据上训练模型，其中额外的单标签数据来自ImageNet CLS-LOC和Caltech-256数据库的子集，共包含20个类别20057张额外简单单标签图像数据。IoU结果如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Mining%20Cross-Image%20Semantics/2.5.png" style="zoom:80%;" /></p>
<h3 id="4-3-在PASCAL-VOC-Data和从网络获取的数据上学习WSSS"><a href="#4-3-在PASCAL-VOC-Data和从网络获取的数据上学习WSSS" class="headerlink" title="4.3 在PASCAL VOC Data和从网络获取的数据上学习WSSS"></a>4.3 在PASCAL VOC Data和从网络获取的数据上学习WSSS</h3><p>本实验在PASCAL VOC 2012与额外的根据标签名称从必应搜索获得的图像数据上进行训练，共包含20个类别以及总共76683张图像。IoU结果如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Mining%20Cross-Image%20Semantics/2.6.png" style="zoom:80%;" /></p>
<h3 id="4-4-训练出的WSSS在-mathrm-LID-20-挑战上的表现"><a href="#4-4-训练出的WSSS在-mathrm-LID-20-挑战上的表现" class="headerlink" title="4.4 训练出的WSSS在$\mathrm{LID}_{20}$挑战上的表现"></a>4.4 训练出的WSSS在$\mathrm{LID}_{20}$挑战上的表现</h3><p>数据集基于ImageNet，包含共有200个图像级标签的349319张图像，本挑战采用的分类器网络基于ResNet-38，并对网络实验参数进行了一些调整。使用均交并比（mIoU）,即计算所有类别的交并比取均值作为评价方案，结果如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Mining%20Cross-Image%20Semantics/2.7.png" style="zoom:80%;" /></p>
<p>注意，在$LID_{19}$中，还可以使用额外的显著性属性标记信息，本文在不使用该信息的情况下表现也优于19年的冠军队伍。</p>
<h3 id="4-5-消融实验"><a href="#4-5-消融实验" class="headerlink" title="4.5 消融实验"></a>4.5 消融实验</h3><h4 id="4-5-1-推理策略"><a href="#4-5-1-推理策略" class="headerlink" title="4.5.1 推理策略"></a>4.5.1 推理策略</h4><p>本实验比较了单轮推理策略，多轮推理策略使用不同方法的IoU得分，结果如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Mining%20Cross-Image%20Semantics/2.8.png" style="zoom:80%;" /></p>
<p>结果显示多轮策略比单论策略的表现有所提升，而当包含了其他相关图像时，对比共注意的加入无法提升定位图推断的性能，作者认为是因为对比共注意特征来自图像自身，而定位图推断更多依赖于上下文信息。</p>
<h4 id="4-5-2-损失函数"><a href="#4-5-2-损失函数" class="headerlink" title="4.5.2 损失函数"></a>4.5.2 损失函数</h4><p>本实验评估了多种损失函数组合方式对mIoU得分的影响，如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Mining%20Cross-Image%20Semantics/2.10.png" style="zoom:80%;" /></p>
<p>可以看出，加入共注意损失时，性能提升了3.8%，再加入对比共注意损失时性能又提升了0.7%。</p>
<h4 id="4-5-3-额外图像"><a href="#4-5-3-额外图像" class="headerlink" title="4.5.3 额外图像"></a>4.5.3 额外图像</h4><p>作者比较了输入0-5张额外相关图像进行训练时模型的IoU表现，发现添加三张时模型效果最好，随着数量继续增大，噪声的影响盖过了辅助信息的作用，导致性能下降。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Mining%20Cross-Image%20Semantics/2.11.png" style="zoom:80%;" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/19/Say%20As%20You%20Wish%20Fine-grained%20Control%20of%20Image%20Caption%20Generation%20with%20Abstract%20Scene%20Graphs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/19/Say%20As%20You%20Wish%20Fine-grained%20Control%20of%20Image%20Caption%20Generation%20with%20Abstract%20Scene%20Graphs/" itemprop="url">《Say As You Wish》阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-19T21:31:13+08:00">
                2021-01-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Say-As-You-Wish-Fine-grained-Control-of-Image-Caption-Generation-with-Abstract-Scene-Graphs"><a href="#Say-As-You-Wish-Fine-grained-Control-of-Image-Caption-Generation-with-Abstract-Scene-Graphs" class="headerlink" title="Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs"></a>Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs</h1><p>题目：如你所愿:用抽象场景图精细控制图像标题生成。</p>
<p>来源：CVPR2020</p>
<p>本文提出了一种基于抽象场景图的图像字幕模型。</p>
<h2 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1 Motivation"></a>1 Motivation</h2><p>1) 图像字幕模型结合了多种计算机视觉技术，具有广泛的应用前景，但存在图像细节描述不足等问题。</p>
<p>2) 按图像内容进行拼接式描述的图像字幕模型极大地阻碍了字幕的多样性，无法表达对图像的完整理解。</p>
<h2 id="2-Contirbution"><a href="#2-Contirbution" class="headerlink" title="2 Contirbution"></a>2 Contirbution</h2><p>1) 第一个提出用抽象场景图对图像标题生成进行细粒度控制</p>
<p>2) 提出的ASG2Caption模型用于自动识别抽象的图形节点，生成具有预期内容和顺序的字幕。</p>
<p>3) 实现了最先进的可控性给定指定ASGs在两个数据集。</p>
<h2 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3 Approach"></a>3 Approach</h2><h3 id="3-1-抽象场景图"><a href="#3-1-抽象场景图" class="headerlink" title="3.1 抽象场景图"></a>3.1 抽象场景图</h3><p>本文首先提出一种抽象场景图（Abstract Scene Graph），用来表达图像内部对象属性及对象之间的关联性，如下图所示，对于图像$\mathcal{I}$，其ASG定义为$\mathcal{G}=(\mathcal{V}, \mathcal{E})$，其中的$\mathcal{V}$和$\mathcal{E}$分别表示ASG的节点和边。如下图所示，节点根据意图角色分为三种类型，即对象节点$o$，属性节点$a$和关系节点$r$。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E6%8A%BD%E8%B1%A1%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%9B%B8%E5%85%B3/Say%20As%20You%20Wish/3.1.png" alt=""></p>
<p>ASG的构造规则如下：</p>
<p>首先用户添加其感兴趣的对象节点$o_i$，$o_i$以图像$\mathcal{I}$为基础，并且带有在图像中的边界框，以表示目标在图像中的位置；</p>
<p>如果用户想了解关于$o_i$的更多信息，可以在图中给$o_i$添加多个属性节点$a_i$，并分配有向边，用一个额外变量$|l_i|$表示对象$o_i$的属性节点个数；</p>
<p>如果用户想描述两个对象节点$o_i$和$o_j$之间的关系，添加关系节点$r_{i,j}$，并分配从$o_i→r_{i,j}$以及$r_{i,j}→o_j$的有向边。</p>
<p>由此，用户即可方便地构造以细粒度方法描述用户对图像$\mathcal{I}$进行描述的ASG模型。</p>
<p>构造抽象场景图的过程也可以通过简单的分类神经网络与对象建议网络自动生成。</p>
<h3 id="3-2-ASG2Caption模型"><a href="#3-2-ASG2Caption模型" class="headerlink" title="3.2 ASG2Caption模型"></a>3.2 ASG2Caption模型</h3><p>这一部分阐述了作者通过使用给定图像$\mathcal{I}$和其ASG训练图像字幕模型的过程。本质上来说ASG2Caption模型是一个编码-解码网络，整个网络框架如下所示：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E6%8A%BD%E8%B1%A1%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%9B%B8%E5%85%B3/Say%20As%20You%20Wish/3.2.png" alt=""></p>
<h4 id="3-2-1-角色感知图编码器"><a href="#3-2-1-角色感知图编码器" class="headerlink" title="3.2.1 角色感知图编码器"></a>3.2.1 角色感知图编码器</h4><p>该编码器将ASG中基于给定图像$\mathcal{I}$的节点编码成一系列的节点嵌入$\mathcal{X}=\left\{x_{1}, \cdots, x_{|\mathcal{V}|}\right\}$，每一个$x_{i}$不仅需要反映节点对应的视觉信息，还应该表达出节点的意图，因为对于对象节点和其对应的属性节点，其对应于图像中的区域可能是相同的。因此本文提了一个角色感知图编码器，包括一个角色感知节点嵌入模块（Role-aware Node Embedding.），以区分节点意图，还包括一个多关系图卷积神经网络（Multi-relational Graph Convolutional Network.），用来进行上下文编码。</p>
<h5 id="3-2-1-1-角色感知节点嵌入模块"><a href="#3-2-1-1-角色感知节点嵌入模块" class="headerlink" title="3.2.1.1 角色感知节点嵌入模块"></a>3.2.1.1 角色感知节点嵌入模块</h5><p>对于来自图$\mathcal{G}$中的第$i$个节点，首先将其初始化为对应的图像特征$v_i$，具体来说，目标节点$o_i$的特征提取自其在图像中对应的边界框；属性标节点$a_i$的特征提取自与其对应目标节相同的边界框区域；关系节点$r_i$的特征提取自与其关联的两个目标节点的联合边界框。</p>
<p>由于只有视觉特征还无法对节点意向进行描述，在前面提取出图像特征$v_i$的基础上，还进一步对每个节点进行角色嵌入增强，最终得到角色感知节点嵌入的表达形式：</p>
<script type="math/tex; mode=display">
x_{i}^{(0)}=\left\{\begin{array}{cl}
v_{i} \odot W_{r}[0], & \text { if } i \in o； \\
v_{i} \odot\left(W_{r}[1]+\operatorname{pos}[i]\right), & \text { if } i \in a； \\
v_{i} \odot W_{r}[2], & \text { if } i \in r.
\end{array}\right.</script><p>上式中，$W_{r} \in \mathbb{R}^{3 \times d}$表示角色嵌入的可学习矩阵，$d$代表特征维度，其每一行分别对应不同的节点。在属性节点嵌入过程中，还额外增加了一个$\operatorname{pos}[i]$区分连接同一对象的不同属性。</p>
<h5 id="3-2-1-1-多关系图卷积网络"><a href="#3-2-1-1-多关系图卷积网络" class="headerlink" title="3.2.1.1 多关系图卷积网络"></a>3.2.1.1 多关系图卷积网络</h5><p>虽然在本文的ASG中，节点之间的关系是单向的，但实际上节点之间会互相影响，而且由于节点类型不同，从一种类型节点传递信息到另一类型节点的方式与其逆过程不同。因此，作者对之前提出的ASG进行了拓展，得到用于上下文编码的多关系图$\mathcal{G}_{m}=(\mathcal{V}, \mathcal{E},\mathcal{R})$：</p>
<p>具体地说，$\mathcal{R}$包含了六种边来捕捉相邻节点之间的相互关系，分别是从对象到属性、对象到关系、对象到对象以及这三种关系的逆方向。</p>
<p>在明确了节点之间的关系之后，使用MR-GCN网络对进行角色感知嵌入之后的特征继续进行编码，采用如下方式：</p>
<script type="math/tex; mode=display">
x_{i}^{(l+1)}=\sigma\left(W_{0}^{(l)} x_{i}^{(l)}+\sum_{\tilde{r} \in \mathcal{R} } \sum_{j \in \mathcal{N}_{i}^{r} } \frac{1}{\left|\mathcal{N}_{i}^{\tilde{r}}\right|} W_{\tilde{r} }^{(l)} x_{j}^{(l)}\right)</script><p>上式中，$x_{i}^{(l+1)}$表示节点$i$的特征经过第$l+1$层之后的上下文编码嵌入结果，直观地解释就是在上一层的基础上，额外添加了与该节点临近所有节点在不同关系下的平均值之和，即引入了临近节点的信息，可以体现该节点与其他节点的关系。$\sigma$代表的是ReLU激活函数。</p>
<p>取所有节点嵌入的平均值作为全局图嵌入：</p>
<script type="math/tex; mode=display">
\bar{g}=\frac{1}{|\mathcal{V}|} \sum_{i} x_{i}</script><p>将其与全局图像特征进行融合，得到全局编码特征$\bar{v}$。</p>
<h4 id="3-2-2-图语言解码器"><a href="#3-2-2-图语言解码器" class="headerlink" title="3.2.2 图语言解码器"></a>3.2.2 图语言解码器</h4><p>图解码器的目的是将编码之后的ASG图解码为图像标题，本文采用的解码器包含两层LSTM结构，分别是注意力LSTM以及语言LSTM。</p>
<p>注意力LSTM以全局编码特征$\bar{v}$、前一个词嵌入$w_{t-1}$、和上一层语言LSTM的输出$h_{t-1}^{l}$作为输入，输出注意力查询$h_{t}^{a}$：中括号中的参数拼接在一起作为输入。</p>
<script type="math/tex; mode=display">
h_{t}^{a}=\operatorname{LSTM}\left(\left[\bar{v} ; w_{t-1} ; h_{t-1}^{l}\right], h_{t-1}^{a} ; \theta^{a}\right)</script><p>对于语言LSTM，注意力查询$h_{t}^{a}$被用来提取第$t$步的节点嵌入$\mathcal{X}_t$的上下文向量$z_t$，之后将$z_t$和$h_{t}^{a}$作为输入，以此生成单词：</p>
<script type="math/tex; mode=display">
\begin{aligned}
h_{t}^{l} &=\operatorname{LSTM}\left(\left[z_{t} ; h_{t}^{a}\right], h_{t-1}^{l} ; \theta^{l}\right) \\
p\left(y_{t} \mid y_{<t}\right) &=\operatorname{softmax}\left(W_{p} h_{t}^{l}+b_{p}\right)
\end{aligned}</script><p>在生成单词$y_t$之后，本文还提出了一个图更新机制用来更新节点嵌入$\mathcal{X}_t \to \mathcal{X}_{t+1}$。</p>
<h5 id="3-2-2-1-基于图的注意力机制"><a href="#3-2-2-1-基于图的注意力机制" class="headerlink" title="3.2.2.1 基于图的注意力机制"></a>3.2.2.1 基于图的注意力机制</h5><p>本文将图的注意力分成语义内容和图结构两方面，分别称为图内容注意（graph content attention）和图流注意（graph flow attention）。图注意的作用是用来计算上下文向量$z_t$。</p>
<p><strong>1）图内容注意</strong></p>
<p>图内容注意重点考虑节点嵌入$\mathcal{X}_t$和注意力查询$h_{t}^{a}$之间的语义相关性，通过如下方法计算语义得分：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde{\alpha}_{t, i}^{c} &=w_{c}^{T} \tanh \left(W_{x c} x_{t, i}+W_{h c} h_{t}^{a}\right) \\
\boldsymbol{\alpha_{t}^{c} }&=\operatorname{softmax}\left(\boldsymbol{\tilde{\alpha}_{t}^{c} }\right)
\end{aligned}</script><p>这是一个比较基础的注意力网络，其中的$W$与$w$都是内容注意力网络的可学参数。</p>
<p><strong>2）图流注意力</strong></p>
<p>图流注意力的作用是捕获原始ASG中隐含的用户希望生成标题时的预期顺序（比如说如果当前参与的节点是关系节点，根据图的关系，下一个节点很可能是对象节点）。</p>
<p>对于图流，与ASG相比分配了一个额外的开始符号，并且对象节点与属性节点之间是双向连接关系，实际的连接方向由文本流畅性决定，此外，当一个节点没有输出边时，将为该节点构建一个自环路边（走不通时返回），确保图上的注意力不消失。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E6%8A%BD%E8%B1%A1%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%9B%B8%E5%85%B3/Say%20As%20You%20Wish/3.3.png" alt=""></p>
<p>图流的转移有三种情况：</p>
<p>1）原地不动：当使用多个词描述一个节点时。</p>
<script type="math/tex; mode=display">
\alpha_{t, 0}^{f}=\alpha_{t-1}</script><p>2）前进一步：从一个关系节点转移到其对象节点时。</p>
<script type="math/tex; mode=display">
\alpha_{t, 1}^{f}=\left(M_{f}\right) \alpha_{t-1}</script><p>3）前进两步：从关系节点转移到属性节点时。</p>
<script type="math/tex; mode=display">
\alpha_{t, 2}^{f}=\left(M_{f}\right)^{2} \alpha_{t-1}</script><p>$M_{f}$表示的是邻接矩阵，每一行表示对某一个节点的归一化转移情况。</p>
<p>最终的流注意是一个由动态阀门控制的流量分数软插值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
s_{t} &=\operatorname{softmax}\left(W_{s} \sigma\left(W_{s h} h_{t}^{a}+W_{s z} z_{t-1}\right)\right) \\
\boldsymbol{\alpha_{t}^{f}} &=\sum_{k=0}^{2} s_{t, k} \boldsymbol{\alpha_{t, k}^{f} }
\end{aligned}</script><p>完整图流注意形式流程图：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E6%8A%BD%E8%B1%A1%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%9B%B8%E5%85%B3/Say%20As%20You%20Wish/3.4.png" style="zoom:80%;" /></p>
<p>个人理解，图流注意的思想是，首先对图进行改造，构造一个新的图，包含了不同的流动关系，然后以注意力LSTM模型计算出的注意力查询$h_{t}^{a}$以及上一步的上下文向量$z_t$作为不同三种流的流量阀门，控制采纳的权重得分，最终确定最终的图流。即确定下一个进行文本解析的节点。</p>
<p><strong>3）注意力融合</strong></p>
<p>在计算出图内容注意力$\boldsymbol{\tilde{\alpha}_{t}^{c}}$与图流注意力$\boldsymbol{\alpha_{t}^{f}}$之后，使用一个可学网络学习注意力融合权重，进行注意力动态融合，具体表达式如下：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\beta_{t}=\operatorname{sigmoid}\left(w_{g} \sigma\left(W_{g h} h_{t}^{a}+W_{g z} z_{t-1}\right)\right) \\
\boldsymbol{\alpha_{t}}=\beta_{t} \boldsymbol{\tilde{\alpha}_{t}^{c} }+\left(1-\beta_{t}\right) \boldsymbol{\alpha_{t}^{f} }
\end{array}</script><p><strong>4）上下文向量计算</strong></p>
<p>在学习到注意力向量之后，求每一个节点与其对应注意力的加权和，即得到$t$步时最终的上下文向量</p>
<script type="math/tex; mode=display">
z_{t}=\sum_{i=1}^{|\mathcal{V}|} \alpha_{t, i} x_{t, i}</script><h5 id="3-2-2-2-图更新机制"><a href="#3-2-2-2-图更新机制" class="headerlink" title="3.2.2.2 图更新机制"></a>3.2.2.2 图更新机制</h5><p>在进行图像字幕工作时，每一个节点的访问强度被注意力矩阵保存下来，因此，参与更多的节点会被更新得更多，同时，有一些介词和助词虽然访问了图节点，但是并不代表节点的含义，这种情况下不应该对节点进行更新。本文提出了一种视觉哨兵门对注意力强度进行修正：</p>
<script type="math/tex; mode=display">
\boldsymbol{u}_{\boldsymbol{t}}=\operatorname{sigmoid}\left(f_{v s}\left(h_{t}^{l} ; \theta_{v s}\right)\right) \boldsymbol{\alpha}_{\boldsymbol{t} }</script><p>上式代表了一个使用sigmoid激活的全连接层网络，用来输出一个标量，指示参与被访问的节点是否由被生成的文本所表示。</p>
<p>更新机制来源于NTM方法，每个节点特征的更新分为两个部分：1）擦除；2）添加。</p>
<p><strong>1）擦除</strong></p>
<p>首先根据每个节点在$t$步时的更新强度$u_{t,i}$进行擦除操作，具体来说，还是使用了一个全连接层网络计算擦除强度：</p>
<script type="math/tex; mode=display">
\begin{aligned}
e_{t, i} &=\operatorname{sigmoid}\left(f_{\text {ers}}\left(\left[h_{t}^{l} ; x_{t, i}\right] ; \theta_{e r s}\right)\right) \\
\hat{x}_{t+1, i} &=x_{t, i}\left(1-u_{t, i} e_{t, i}\right)
\end{aligned}</script><p>之后，根据擦除强度对节点的特征进行擦除操作。</p>
<p><strong>2）添加</strong></p>
<p>对于重要节点，需要把被擦除的部分进行返还，这部分同样训练了一个全连接层网络计算动态添加强度：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a_{t, i} &=\sigma\left(f_{a d d}\left(\left[h_{t}^{l} ; x_{t, i}\right] ; \theta_{a d d}\right)\right) \\
x_{t+1, i} &=\hat{x}_{t+1, i}+u_{t, i} a_{t, i}
\end{aligned}</script><p>再根据添加强度进行添加操作。</p>
<h4 id="3-2-3-损失函数"><a href="#3-2-3-损失函数" class="headerlink" title="3.2.3 损失函数"></a>3.2.3 损失函数</h4><p>网络的损失函数是图像字幕翻译时每一步的翻译准确概率的对数和，是一个比较经典的LSTM模型的训练损失。</p>
<script type="math/tex; mode=display">
L=-\log \sum_{t=1}^{T} p\left(y_{t} \mid y_{<t}, \mathcal{G}, \mathcal{I}\right)</script><h2 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4 Experiment"></a>4 Experiment</h2><p>本文实验数据库使用的是图像字幕中常用的<strong>VisuakGenome</strong> 和<strong>MSCOCO</strong>，自动构建出三元数据（image $\mathcal{I}$，ASG $\mathcal{G}$，caption $\mathcal{y}$）。三元数据的构建使用的是其他论文的方法。</p>
<p>本文对模型质量的评价从<strong>可控性</strong>和<strong>多样性</strong>两方面进行评估。</p>
<p>对于可控性的评估，使用与ground truth图像标题对其的ASG作为控制信号，具体的指标包括BLEU，METEOR，ROUGE，CIDEr，SPICE。评价思想是如果语义识别正确，句子结构应该与ASG比较符合，得分较高。</p>
<p>对于多样性的评估，首先采样相同数量的标题，之后通过两个指标评估采样出来的标题的相似性。</p>
<p>其一是DIV-n：评估长度为n个字节的词段在整个标题中出现的频率；其二是SelfCIDEr，派生自CIDEr的一种评测方法。</p>
<p>具体实验时，使用在VisualGennome上预训练的Faster-RCNN提取ASG的节点特征，使用在ImageNet上预训练的ResNet152提取全局图像表征。</p>
<h3 id="4-1-可控性评估"><a href="#4-1-可控性评估" class="headerlink" title="4.1 可控性评估"></a>4.1 可控性评估</h3><p>评估了他们的模型与一些其他方法在可控性指标上的表现：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E6%8A%BD%E8%B1%A1%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%9B%B8%E5%85%B3/Say%20As%20You%20Wish/3.5.png" alt=""></p>
<p>具体图像字幕的可视化：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E6%8A%BD%E8%B1%A1%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%9B%B8%E5%85%B3/Say%20As%20You%20Wish/3.6.png" style="zoom:80%;" /></p>
<h3 id="4-2-不同组件效果的消融研究"><a href="#4-2-不同组件效果的消融研究" class="headerlink" title="4.2 不同组件效果的消融研究"></a>4.2 不同组件效果的消融研究</h3><p>对不同模块的添加对模型表现进行了评估：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E6%8A%BD%E8%B1%A1%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%9B%B8%E5%85%B3/Say%20As%20You%20Wish/3.7.png" alt=""></p>
<p>第一二行是两个baseline，对三四行的评估发现添加图上下文编码能够提升模型表现，五六行比较了图流结果与图更新机制之间的表现差异，七八行评估了在本文提出的模型基础上，添加集束搜索（beam search，一种对贪心算法的改进）之后，模型表现达到了最优。</p>
<h3 id="4-3-不同关心角度对字幕生成的影响"><a href="#4-3-不同关心角度对字幕生成的影响" class="headerlink" title="4.3 不同关心角度对字幕生成的影响"></a>4.3 不同关心角度对字幕生成的影响</h3><p><img src="http://little_z_c.gitee.io/imagebed/%E6%8A%BD%E8%B1%A1%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%9B%B8%E5%85%B3/Say%20As%20You%20Wish/3.8.png" alt=""></p>
<p>根据不同关心角度构建的不同ASG，可能会输出完全不同的图像字幕结果，同时，具有大致相同结构的ASG会生成相似的标题，但是存在不同描述。证明模型细粒度级别上的敏感性。</p>
<h3 id="4-4-多样性评估"><a href="#4-4-多样性评估" class="headerlink" title="4.4 多样性评估"></a>4.4 多样性评估</h3><p>在两个数据集上与不同模型进行对比，评价多样性水平：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E6%8A%BD%E8%B1%A1%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%9B%B8%E5%85%B3/Say%20As%20You%20Wish/3.9.png" style="zoom:80%;" /></p>
<p>使用不同ASG，生成了很不同的文本描述：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E6%8A%BD%E8%B1%A1%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%9B%B8%E5%85%B3/Say%20As%20You%20Wish/3.10.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/19/Learning%20Selective%20Self-Mutual%20Attention%20for%20RGB-D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/19/Learning%20Selective%20Self-Mutual%20Attention%20for%20RGB-D/" itemprop="url">《Learning Selective Self-Mutual Attention for RGB-D Saliency Detection--RGB-D>阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-19T21:31:13+08:00">
                2021-01-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="显著性检测的学习选择性自相互注意-CVPR2020"><a href="#显著性检测的学习选择性自相互注意-CVPR2020" class="headerlink" title="显著性检测的学习选择性自相互注意 CVPR2020"></a>显著性检测的学习选择性自相互注意 CVPR2020</h1><h2 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1 Motivation"></a>1 Motivation</h2><p>1) 相比于传统的RGB显著性检测方法，包含深度信息的RGB-D检测可以更好地识别出图像的阳性区域。</p>
<p>2) 以往的RGB-D检测使用的融合策略（如早期融合、结果融合）作用有限。</p>
<h2 id="2-Contribution"><a href="#2-Contribution" class="headerlink" title="2 Contribution"></a>2 Contribution</h2><p>1) 基于Non-Local，提出一种新的中间融合策略，通过融合深度注意，准确定位出对象的主体。</p>
<p>2) 将注意力机制应用于双流CNN模型，并引入新的残差融合模块，提高了显著性检测的性能，优于所有现存方法。</p>
<h2 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3 Approach"></a>3 Approach</h2><p>本文使用的注意力模型基于Non Local，是在此基础上进行的改进。整个模型框架如下图右侧图像所示：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Learning%20Selective%20Self-Mutual%20Attention/1.png" alt="1">如上图所示，不包含深度信息的RGB方法的检测结果含有很严重的假阳性高亮区域。</p>
<h3 id="3-1-Non-Local模块"><a href="#3-1-Non-Local模块" class="headerlink" title="3.1 Non Local模块"></a>3.1 Non Local模块</h3><p>​        首先简要介绍一种non local模块，如上图左侧部分所示，Non Local模型首先将输入的feature map$\boldsymbol{X}$用三个不同权值的1×1卷积层嵌入到三个通道数均为$C_1$的特征空间中。</p>
<p>​        之后，计算经过$W_{\theta}$与$W_{\phi}$嵌入之后的$\boldsymbol{X}$两个不同视图每个像素点之间的相关性。此处的计算方法是简单的矩阵乘法：</p>
<script type="math/tex; mode=display">
f(\boldsymbol{X})=\theta(\boldsymbol{X}) \phi(\boldsymbol{X})^{\top}</script><p>​        然后，使用softmax对$f(\boldsymbol{X})$进行行归一化处理得到$\boldsymbol{X}$的注意力矩阵，第$i$一行即表示像素点$i$与其他点之间的注意力权重情况。</p>
<script type="math/tex; mode=display">
A(\boldsymbol{X})=softmax(f(\boldsymbol{X}))</script><p>​        再将得到的注意力矩阵与$\boldsymbol{X}$的另一个嵌入视图$g(\boldsymbol{X})$相乘，即得到最终的包含注意力信息的特征$\boldsymbol{Y}$</p>
<script type="math/tex; mode=display">
\boldsymbol{Y}=A(\boldsymbol{X})g(\boldsymbol{X})</script><p>​        最后，引入一个残差模块，得到最终的输出特征：</p>
<script type="math/tex; mode=display">
\boldsymbol{Z}=\boldsymbol{Y}W_\boldsymbol{Z}+\boldsymbol{X}</script><h3 id="3-2-自-互注意力"><a href="#3-2-自-互注意力" class="headerlink" title="3.2 自-互注意力"></a>3.2 自-互注意力</h3><p>Non Local模块本质上是对自身特征的双线性投影，属于self-attention的范畴，在此基础上，作者提出引入相互注意力，以提升RGB-D任务定义多模态特征的显著性检测水平，因此提出了如上图所示右侧的网络架构，称为SMA（Self-Mutual Attention）。SMA的思路如下：</p>
<p>对于给定的图像在RGB模态与D模态下的feature map$\boldsymbol{X}^r$与$\boldsymbol{X}^d$，基于NL模型，计算二者各自的相关性矩阵$f(\boldsymbol{X}^r)$与$f(\boldsymbol{X}^d)$，将二者通过简单相加的方法进行融合，得到一个新的注意力矩阵：</p>
<script type="math/tex; mode=display">
A^f(\boldsymbol{\boldsymbol{X}^r,\boldsymbol{X}^d})=softmax(f^r(\boldsymbol{X}^r)+f^d(\boldsymbol{X}^d))</script><p>使用融合注意力矩阵代替各自的注意力矩阵计算$\boldsymbol{X}^r$与$\boldsymbol{X}^d$各自的注意力特征，得到最终的输出特征$\boldsymbol{Z}^r$与$\boldsymbol{Z}^d$。</p>
<p>作者认为融合注意力矩阵相比于各自独立的注意力矩阵包含了RGD与D模态下的注意力信息，使得学习到的注意力更加准确，并且在实验中切实提高了模型表现。</p>
<h3 id="3-3-选择性自-互注意力"><a href="#3-3-选择性自-互注意力" class="headerlink" title="3.3 选择性自-互注意力"></a>3.3 选择性自-互注意力</h3><p>SMA模型对于RGB模态与Deep模态之间的自注意力与相互注意力的选择是平等的，然而作者认为相互注意力并不总是在所有位置都可靠，可能会产生负面干扰。因此作者引入了一个选择机制，对相互注意力的引入程度进行了一定控制。</p>
<p>具体来说，首先将RGB模态与D模态下的feature map$\boldsymbol{X}^r$与$\boldsymbol{X}^d$拼接成一个张量，维度是[H, W, 2C]，之后经过一个1×1的矩阵进行embedding处理之后，经过softmax处理得到选择注意力矩阵，维度是[H, W, 2]：</p>
<script type="math/tex; mode=display">
\alpha=softmax\left(\operatorname{Conv}\left(\left[\boldsymbol{X}^{r}, \boldsymbol{X}^{d}\right]\right)\right)</script><p>将$\alpha$进行拆分，得到模态RGD与D各自的$\alpha^r$与$\alpha^d$，维度是[H, W, 1]，代表每个模态在所有位置上对应的可靠性，即在进行注意力融合时选择的权重，将其带入SMA的注意力矩阵计算公式，得到最终的注意力计算法则：</p>
<script type="math/tex; mode=display">
A^r(\boldsymbol{\boldsymbol{X}^r,\boldsymbol{X}^d})=softmax(f^r(\boldsymbol{X}^r)+\alpha^d{\odot}f^d(\boldsymbol{X}^d))
\\
A^d(\boldsymbol{\boldsymbol{X}^r,\boldsymbol{X}^d})=softmax(f^d(\boldsymbol{X}^d)+\alpha^r{\odot}f^r(\boldsymbol{X}^r))</script><p>${\odot}$代表通道间的点乘操作，这个模型称为$S^2MA$</p>
<h2 id="4-RGD-D显著性检测网络"><a href="#4-RGD-D显著性检测网络" class="headerlink" title="4 RGD-D显著性检测网络"></a>4 RGD-D显著性检测网络</h2><p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Learning%20Selective%20Self-Mutual%20Attention/2.png" alt=""></p>
<p>本文的网络架构采用的是Unet的架构，首先提取RGB图与Deep图各自的特征，再经过一个Dense ASPP模块，分别提取到512个通道的张量，送入本文所提出的$S^2MA$模块获取注意力特征，之后经过一个Unet网络，得到最终的显著性结果。</p>
<h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5 实验"></a>5 实验</h2><p>本文使用了七个RGB-D的数据集进行模型能力的评估，分别是：</p>
<ol>
<li><strong>NJUD</strong>：拥有1985张从互联网上搜集到的、从3D电影图片和立体照片中获取的图片；</li>
<li><strong>NLPR和RGBD135</strong>：分别包含1000和135张由微软Kinect采集到的图片；</li>
<li><strong>LFSD</strong>：包含100张由Lytro光场相机捕获的图片；</li>
<li><strong>STERE</strong>：包含1000对从互联网上下载的双目图像；</li>
<li><strong>SSD</strong>：包含八十张立体电影帧；</li>
<li><strong>DUT-RGBD</strong>：包含1200张Lytro2相机捕获的图片。</li>
</ol>
<p>本文使用了四个评价指标评估模型的性能，分别是：</p>
<ol>
<li><strong>maxF</strong>：最大F测量，评价二值化显著性图的精度和召回率，是精度和召回率的加权平均；</li>
<li><strong>$S_m$</strong>：结构度量，评价显著性图与ground truth之间区域感知和目标感知的结构相似性；</li>
<li><strong>$E_{\xi}$</strong>：测量显著性图的全局统计信息与局部像素匹配信息；</li>
<li><strong>MAE</strong>：平均绝对误差，测量显著性图与ground truth之间每个像素差值绝对值得平均水平。</li>
</ol>
<h3 id="5-1-消融实验"><a href="#5-1-消融实验" class="headerlink" title="5.1 消融实验"></a>5.1 消融实验</h3><p>设置了本文不同网络组成结构的组合下模型的表现结果，如下图所示：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Learning%20Selective%20Self-Mutual%20Attention/3.png" alt="3"></p>
<p>蓝色表示最佳水平，可以发现，本文方法比NL和SMA都要更好一些。</p>
<p>作者还可视化了他们在红色查询点下，RGB自注意、Depth自注意与他们的方法获得的自-互注意图的注意力图。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Learning%20Selective%20Self-Mutual%20Attention/4.png" alt="4"></p>
<h3 id="5-2-显著性模型有效性评估"><a href="#5-2-显著性模型有效性评估" class="headerlink" title="5.2 显著性模型有效性评估"></a>5.2 显著性模型有效性评估</h3><p>作者比较了他们的方法与其他先进方法（前三种是传统模型方法，后八种是深度学习模型方法）之间的显著性检测水平，红色是最好的表现，蓝色是第二好的表现。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Learning%20Selective%20Self-Mutual%20Attention/5.png" alt="5"></p>
<p>此外，作者同时进行了定性评价，可视化了不同方法的显著性检测结果。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Attention%E7%9B%B8%E5%85%B3/Learning%20Selective%20Self-Mutual%20Attention/image-20200924235304545.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/19/%E3%80%8AMoco%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/19/%E3%80%8AMoco%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url">Moco：无监督视觉表征学习的动量对比</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-19T16:08:44+08:00">
                2021-01-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>论文链接: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.05722">https://arxiv.org/pdf/1911.05722</a> </p>
<p>官方pytorch链接: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/moco">https://github.com/facebookresearch/moco</a></p>
<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h2><p>在NLP领域，无监督表征学习有GPT、BERT等效果非常好的模型，但是CV领域还是有监督模型作为主流。作者认为主要是因为CV和NLP领域处理的数据对应的信号空间不同：语言任务有离散的信号空间，词语词之间可以视为是独立的词组，能够很方便地构成字典（Dictionary），这种词典是无监督学习便于学习依赖的特征；而视觉领域的原始信号是在一个连续且高维空间中，无法成为结构化的信号，使得无监督学习难以展现在NLP领域发挥出的效果。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.1.png" alt="moco原理" style="zoom:80%;" /></p>
<p>作者提出了一种动态字典，即MOCO，新的minibatch对应的字典在进入队列时，将会替换最早进入的字典，使得字典始终是所有数据的子集，又始终代表最新的表征，经过试验发现，Moco的表现非常之好。</p>
<h2 id="2-思想"><a href="#2-思想" class="headerlink" title="2 思想"></a>2 思想</h2><p>对比学习的思想在于，通过一些已编码的query（q），使其与其对应的key（k）相对应，k是被编码的样本的在字典中的key$\lbrace k_0,k_1,k_2…\rbrace$，假设$q$与$k_+$相匹配，对比损失的目的是尽量拉近$q$与$k_+$之间的距离而增大$q$与其他$k_i$之间的距离（拉近正对，缩小负对，其他的$k_i$都是负对），Moco用的是点积的方法衡量相似度，称为InfoNCE loss：</p>
<script type="math/tex; mode=display">
\ell_{i,j}=-\log\frac{\exp(q·k_+/\tau)} {\sum_{i=0}^{K} {\exp(q·k_i/\tau) } }</script><p>其中，$\tau$是温度参数，一般来说，query$q=f_q(x^q)$，相应的，$k=f_k(x^k)。$$f_q，f_k$都是一个编码器网络，二者可以参数完全相同，也可以部分共享参数，甚至完全不同而$x_q，x_k$既可以是图片，也可以只是patch，或者是一系列的patch。</p>
<p>作者认为对比学习是在高维连续输入上构建离散字典的方法，由于key是随机采样产生的，并且$f_k$会在训练过程中进化，因此这个字典是动态的。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20200822184022179.png" alt="image-20200822184022179" style="zoom:80%;" /></p>
<p>作者还提到，使用队列的方法构建字典会使字典变大，但是也使得通过反向传播更新编码器参数变得困难，很简单的解决方法可以通过将$f_q$的梯度复制到$f_k$中，以忽略$f_k$的梯度，但是作者通过试验发现这样的<strong>效果很差</strong>，作者猜测可能的原因是因为<strong>编码器的快速更新降低了编码出来的关键表示的一致性</strong>。因此作者放弃了参数共享的想法，转而采用速度较慢的动量更新来解决这个问题，从而在一定程度上保证了队列中各个key之间的一致性。最终采用如下公式更新$f_k$的参数：</p>
<script type="math/tex; mode=display">
θ_k←mθ_k+(1-m)θ_q</script><p>m就是动量置信度，属于[0,1)，在这个过程中，只有$f_q$的参数通过loss使用反向传播进行更新，并且在实际应用过程中，一般将m设置为比较大的值，以使得$f_k$的更新尽量缓慢，从而使每个k的相似度更好，本文设置的m的默认值是0.999（作者实验表明，更大的动量值比更小的动量值表现得更好，也就是说$enc_k$的更新越慢越好）.</p>
<h2 id="3-与其他方法的对比"><a href="#3-与其他方法的对比" class="headerlink" title="3 与其他方法的对比"></a>3 与其他方法的对比</h2><p>作者对比了另外两种方法，分析了这两种方法的缺点：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.2.png" alt="三种方法对比" style="zoom:80%;" /></p>
<ol>
<li><p><strong>end-to-end</strong>端到端</p>
<p>端到端的编码方法是最基础的方法之一，用当前mini batch中的示例构建字典，每一个key都是通过相同参数的$f_k$编码得到的，相当于是被一致编码。但是字典大小与mini batch大小叠加，受到GPU内存的限制，同时也有大mini batch优化上的问题。最近的一些方法基于局部位置驱动的借口任务，字典的大小可以通过多个局部增大，但是这种架构需要特殊的网络设计，会使得网络向下游任务的传输更加复杂。</p>
</li>
<li><p><strong>memory bank</strong>记忆库</p>
<p>另一种方法是记忆库方法，一个记忆库由数据集中所有样本的表征组成，每一个mini batch的字典通过从记忆库中随机采样获得，而不需要进行反向传播，因此可以支持很大的字典，但是记忆库中方你样本的表征在最后一次看到样本时会被更新，导致采样的key在过去的时间里基本上是在编码器的不同步骤上生成的，不太一致。</p>
</li>
</ol>
<h2 id="4-技术细节"><a href="#4-技术细节" class="headerlink" title="4 技术细节"></a>4 技术细节</h2><p>在构造实验时，将与query q来自于同一图像的key k构造成一组正对，来自其他图像的key则视为负示例，在实现时也是对一张图像进行数据增强，生成两个随机子样本作为一对正对，之后将增强后的子样本分别投入$f_q，f_k$，生成q与k。编码器可以是任意的卷积神经网络。</p>
<p>在具体实现时，用如下算法进行构造：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.3.png" alt="算法伪代码" style="zoom:80%;" /></p>
<p>此外，对于编码器，本文使用了ResNet50，之后进行全局平均池化，再经过一个全连接层，输出一个128维的向量，按照l2范数进行归一化处理，得到最终的q与k的表征。温度参数设置为0.07。</p>
<p>进行数据增强时，从随即调整大小后的图像中裁剪出224*224大小的图像，之后再进行随机的颜色抖动、随机水平翻转与随机灰度转换处理得到子样本。</p>
<p>Shuffling BN</p>
<p>多GPU训练时会造成信息泄露，一个解决方法是，在对key进行BN时，先对当前的mini batch进行顺序洗牌，在编码后再次进行顺序洗牌，而对q不进行处理。</p>
<h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5 实验"></a>5 实验</h2><p><strong>实验数据集包括</strong>：ImageNet的训练集（128万张）、Instagram上1500个话题中的9400万张图像，话题内容与ImageNet上的类别对应，与ImageNet相比，这个数据集上的图像有一个长尾的，不平衡的数据分布，更接近真实世界的数据。</p>
<p><strong>实验用的优化算法</strong>：用的是SGD算法，动量设为0.9，在ImageNet上的Batch Size是256，用了8块GPU，初始学习率0.03，训练200个epoch，在120和160个epoch时将学习率乘以0.1；在Instagram数据集上的Batch Size为1024，用了64块GPU，初始学习率为0.12，在每62.5k的iterations时将学习率用指数衰减0.9倍，共训练125万个iterations。二者用的都是ResNet50。</p>
<h3 id="5-1-线性分类评估"><a href="#5-1-线性分类评估" class="headerlink" title="5.1 线性分类评估"></a>5.1 线性分类评估</h3><p>本实验在Image Net数据集上进行。首先训练好Moco和其他对比模型，之后冻结feature，把模型输出的feature作为监督训练一个线性分类器100个epoch，之后测量1-crop和top-1精度。</p>
<ol>
<li><strong>与其他对比架构的性能比较</strong></li>
</ol>
<p>这个实验比较了他之前提到的end-to-end和memory bank的机制，都使用InfoNCE loss。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.4.png" alt="与其他架构的消融实验对比" style="zoom:80%;" /></p>
<p>可以看到，Moco的性能表现确实最好，而且三种方式都能受益于更大的K，也就是更大的字典数量更有助于提升性能。此外，Moco和memory bank可以实现更大的K，而end-to-end的架构使得其算力限制了对大字典的支持。</p>
<p>作者还探究了不同momentum的影响，发现取0.999时表现最好。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.5.png" alt="不同momentum的精度" style="zoom:80%;" /></p>
<ol>
<li><strong>与其他非监督学习方法的比较</strong></li>
</ol>
<p>在这一部分，作者比较了K设为65536，m设为0.999时，ResNet50下不同非监督方法的性能，还比较了2×和4×宽度的网络的表现效果，具体来说，综合对比了<strong>参数数量</strong>和<strong>分类精度</strong>。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.6.png" alt="点状图" style="zoom:80%;" /></p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.7.png" style="zoom:80%;" /></p>
<p>根据实验结果，Moco在ResNet50下的精度达到了60.6%，是同等参数数量的网络中最好的，并且在4×网络下达到了<strong>68.6%</strong>，而且没有对网络采用什么特殊的架构。</p>
<h3 id="5-2-迁移特征"><a href="#5-2-迁移特征" class="headerlink" title="5.2 迁移特征"></a>5.2 迁移特征</h3><p>这一部分主要做的是用训练好的模型与其他在ImageNet训练好的模型做分割与检测实验，用的数据集是PASCAL VOC和COCO。</p>
<p>在进行实验之前，首先进行了特征正则化，用的方法是对经过训练的BN进行微调（并且在GPU之间同步），之后在新初始化的层中也使用BN，用来校准 magnitudes。</p>
<p>作者还控制了Schedule，这一部分没有太搞清楚，后期继续学习，在具体控制时，COCO是1×（大约12个epoch）或2×的schedule；</p>
<ol>
<li><strong>在PASCAL VOC上进行目标检测</strong></li>
</ol>
<p>目标检测模型是Faster R-CNN，主干是ResNet50-dilated-C5或ResNet50-C4，应用已经调好的BN。对所有层都进行端到端的微调。</p>
<p>训练时的图像是[480,800]的像素，推理时用的是800，评估默认的VOC度量为$AP_{50}$（IoU阈值为50%：交并比，即模型画出的框与人工标注框的重叠率在50%以上，则认为通过测试）和COO-style $AP$和$AP_{75}$，在VOC test2007上进行实验。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.8.png" style="zoom:80%;" /></p>
<p>可以看到Moco在Instagram下训练的效果会比有监督的ResNet50好。</p>
<p>和其他对比学习框架比起来表现也更优异：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.12.png" style="zoom:80%;" /></p>
<p>一般来说，无监督方法的预测结果都要差一些，但是Moco的性能在有些测试中比有监督方法还要好：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.9.png" alt="不同方法对比"></p>
<ol>
<li><strong>在COCO上进行目标检测</strong></li>
</ol>
<p>目标检测模型是Mask R-CNN，主干是FPN或C4，应用已经调好的BN。对所有层都进行端到端的微调。</p>
<p>训练时的图像是[640,800]的像素，推理时用的是800，评估默认的VOC度量为$AP_{50}$（IoU阈值为50%：交并比，即模型画出的框与人工标注框的重叠率在50%以上，则认为通过测试）和COCO-style $AP$和$AP_{75}$，在train2007数据集上进行微调，在val2017上进行评估，schedule是默认的1×或2×。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.10.png" alt="COCO上的实验结果"></p>
<p>可以看到，MOCO的表现基本上好于有监督模型，此外，在执行特定任务的，Moco的表现也不错：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.11.png" alt="Moco表现情况" style="zoom:80%;" /></p>
<h2 id="6-基于SimCLR的改进版Moco-v2"><a href="#6-基于SimCLR的改进版Moco-v2" class="headerlink" title="6 基于SimCLR的改进版Moco v2"></a>6 基于SimCLR的改进版Moco v2</h2><p>SimCLR是一个end-to-end的对比学习框架，取得了比较好的效果，受SimCLR的启发，作者对Moco进行了改进，主要是对数据增强的方法进行了改进，并添加了SimCLR中的MLP投影头，提升了网络的性能(在Image Net上线性分类的表现提升了6.9%)。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/19/Supervised%20Contrastive%20Learning%EF%BC%9A%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/19/Supervised%20Contrastive%20Learning%EF%BC%9A%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" itemprop="url">Supervised Contrastive Learning：有监督对比学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-19T14:56:15+08:00">
                2021-01-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-概要"><a href="#1-概要" class="headerlink" title="1 概要"></a>1 概要</h2><p>交叉熵损失是监督学习中应用最广泛的损失函数，度量两个分布（标签分布和经验回归分布）之间的KL散度，但是也存在对于有噪声的标签缺乏鲁棒性、可能存在差裕度（允许有余地的余度）导致泛化性能下降的问题。而大多数替代方案还不能很好地用于像ImageNet这样的大规模数据集。</p>
<p>许多对正则交叉熵的改进实际上是通过对loss定义的放宽进行的，特别是参考分布是轴对称的。这写改进通常具有不同的动机：比如标签平滑（Label smoothing）通过偏离轴来模糊区分正确和不正确的标签，从而在许多应用中提供了很小但是很重要的提升；在自蒸馏中，利用前几轮的“软”标签作为参考类分布进行多轮交叉熵训练；混合和相关数据增强策略通常通过线性插值创建明确的、新的训练示例，然后将相同的线性插值应用于目标标签分布，类似于软化原始交叉熵loss。用这些修改方法训练的模型显示了改进的泛化、鲁棒性和校准。</p>
<p>本文提出了一个新的loss，受对比loss与度量学习启发，完全去除参考分布，而只是将来自相同类的规范化嵌入强行加在一起，使得其比来自不同类的嵌入更加紧密。</p>
<p>具体来说，在对比学习中，核心思想是拉近某一个锚点与其正样本之间的距离，拉远锚点与该锚点其他负样本之间的距离，通常来说，一个锚点只有一个正样本，其他全视为负样本。而本文的方法认为每个锚点有许多的正样本，而不是许多负样本，并且通过标签显示样本之间的正负关联。比如下面的图，右侧是典型的对比学习方法，通常将一张原图通过数据增强得到两个子样本，这一对子样本之间构成一对正对，而与其他数据的子样本构成负对；而本文的有监督对比学习中，每个子样本可能都有很多的正对和负对。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.1.png" alt="正负对示例" style="zoom: 67%;" /></p>
<p>本文构造的loss在ResNet50和ResNet200上都取得了不错的Top-1效果，在自动增强的ResNet50上取得78。8%的Top-1精度，比同样数据增强下的交叉熵loss提升了1.6%，不仅如此，还更鲁棒。</p>
<p>具体的Contribution如下：</p>
<ol>
<li>我们提出了一个新的扩展对比损失函数，允许每个锚点有多个正对。因此，我们将对比学习适应于完全监督的设置。</li>
<li>我们表明，与交叉熵相比，这种损失使我们能够了解最先进的表示方式，从而显著提高了Top-1的准确性和鲁棒性。</li>
<li>我们的损失对超参数范围的敏感性不如交叉熵。这是一个重要的实际考虑。我们相信，这是由于我们的损失使用更自然的公式，使从同一类样本的代表被拉得更近，而不是像交叉熵一样强迫他们被拉向一个特定的目标。</li>
<li>我们分析地表明，我们的损失函数的梯度鼓励从hard positive和hard negative中学习。我们还表明，三联体损失是我们损失只有一个正极和负极被使用的一个特例。</li>
</ol>
<p>具体来说，有监督对比学习的框架是交叉熵loss和传统对比学习的结合:</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.2.png" alt="三种框架" style="zoom:80%;" /></p>
<h2 id="2-具体结构"><a href="#2-具体结构" class="headerlink" title="2 具体结构"></a>2 具体结构</h2><h3 id="2-1-表征学习框架"><a href="#2-1-表征学习框架" class="headerlink" title="2.1 表征学习框架"></a>2.1 表征学习框架</h3><p>总的来说，有监督对比学习框架的结构类似于表征学习框架，由如下几个部分组成：</p>
<ol>
<li><p><strong>数据增强模块</strong></p>
<p>数据增强模块$A(·)$的作用是将输入图像转换为随机增强的图像$\widetilde{x}$，对每张图像都生成两张增强的子图像，代表原始数据的不同视图。数据增强分为两个阶段：第一阶段是对数据进行<strong>随机裁剪</strong>，然后将其调整为原分辨率大小；第二阶段使用了三种不同的增强方法，具体包括：（1）<strong>自动增强</strong>，（2）<strong>随机增强</strong>，（3）<strong>Sim增强</strong>（按照顺序进行随机颜色失真和高斯模糊，并可能在序列最后进行额外的稀疏图像扭曲操作）。</p>
</li>
<li><p><strong>编码器网络</strong></p>
<p>编码器网络$E(·)$的作用是将增强后的图像$\widetilde{x}$映射到表征空间，每对子图像输入到同一个编码器中得到一对表征向量，本文用的是ResNet50和ResNet200，最后使用池化层得到一个2048维的表征向量。表征层使用单位超球面进行正则化。</p>
</li>
<li><p><strong>投影网络</strong></p>
<p>投影网络$P(·)$的作用是将表征向量映射成一个最终向量$z$进行loss的计算，本文用的是只有一个隐藏层的多层感知器，输出维度为128。同样使用单位超球面进行正则化。在训练完成后，这个网络会被一个单一线性层取代。</p>
</li>
</ol>
<h3 id="2-2-对比损失"><a href="#2-2-对比损失" class="headerlink" title="2.2 对比损失"></a>2.2 对比损失</h3><p>本文的数据是带有标签的，采用mini batch的方法获取数据，首先从数据中随机采样$N$个样本对，记为$\left\{ {x}_k,{y}_k\right\}_{k=1,2,…,N}$，${y}_k$是${x}_k$的标签，之后进行数据增强获得$2N$个数据样本$\left\{\widetilde{x}_k,\widetilde{y}_k\right\}_{k=1,2,…,2N}$，其中，$\widetilde{x}_{2k}$和$\widetilde{x}_{2k-1}$是分别用两种随机增强方法得到的数据对，在数据增强过程中，标签信息始终不会改变。</p>
<h4 id="2-2-1-自监督对比损失"><a href="#2-2-1-自监督对比损失" class="headerlink" title="2.2.1 自监督对比损失"></a>2.2.1 自监督对比损失</h4><p>本文的自监督对比损失与SimCLR的loss相类似，不过使用的是点积刻画样本之间的相似性，具体表达式如下：</p>
<script type="math/tex; mode=display">
\mathcal{L}^{self}=\sum_{i=1}^{2N}{\mathcal{L}_{i}^{self} }\\
\mathcal{L}_{i}^{self}=-\log\frac{\exp(z_i·z_{j(i) }/\tau) } {\sum_{k=1}^{2N}  {\mathbb{l}_{ [k{\neq}i] }·\exp(z_i·z_{j(i) }/\tau) } }</script><p>上式中，$\mathbb{l}_{ [k{\neq}i] }$是一个指示函数，当且仅当$k=i$时取0，否则为1。$\tau$是进行优化的温度参数。该loss的意义在于拉近$\widetilde{x}_i$于其正对$\widetilde{x}_{j(i)}$之间的距离而拉远$\widetilde{x}_i$与其他负对之间的距离。</p>
<h4 id="2-2-2-有监督的对比损失"><a href="#2-2-2-有监督的对比损失" class="headerlink" title="2.2.2 有监督的对比损失"></a>2.2.2 有监督的对比损失</h4><p>有监督对比损失是对自监督对比损失的推广，从公式中很容易可以看出，有监督对比损失拓展了$\widetilde{x}_i$正对的数量，将所有标签信息相同的子数据都视为正对，计算了$\widetilde{x}_i$与其所有正对之间的相似性，之后进行加权平均。</p>
<script type="math/tex; mode=display">
\mathcal{L}^{sup}=\sum_{i=1}^{2N} {\mathcal{L}_{i}^{sup} }\\
\mathcal{L}_{i}^{sup}=\frac{-1} {2N_{\widetilde{y}_i}-1}\sum_{j=1}^{2N} {\mathbb{l}_{ [i{\neq}j] }·\mathbb{l}_{ [ {\widetilde{y}_i}={\widetilde{y}_j} ] } }·\log\frac{\exp(z_i·z_{j(i)}/\tau)}{\sum_{k=1}^{2N}{\mathbb{l}_{ [k{\neq}i] }·\exp(z_i·z_{j(i)}/\tau)} }</script><p>作者指出对比损失的核心是足够多的负对，以便与正对形成鲜明的对比，他们的改进监督对比损失保留了这一特性。此外，由于增加了正对的数量，这一架构还可以更好地刻画类内相似性。</p>
<h4 id="2-2-3-有监督对比损失的梯度特性"><a href="#2-2-3-有监督对比损失的梯度特性" class="headerlink" title="2.2.3 有监督对比损失的梯度特性"></a>2.2.3 有监督对比损失的梯度特性</h4><p>这一部分论证了hard positive和hard negative更有助于提升网络的性能，主要是通过对有监督对比损失的梯度进行分析，在此略去。</p>
<p>此外，作者在论文中还论述了三联loss是他们的有监督对比损失的特例，此处省略不讲。</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h2><p>作者在评估其框架性能时，使用了<strong>Top-1精度</strong>和<strong>对损坏图像的鲁棒性</strong>两个方面进行衡量，还评价了其模型<strong>对超参数的稳定性</strong>以及<strong>正对数量</strong>对模型表现的影响。在实现上，使用的是训练好的网络，之后将网络的非线性投影头替换成一个简单的线性全连接层，使用标准交叉熵损失训练这个线性层。网络的训练在ImageNet上进行。</p>
<h3 id="3-1-ImageNet分类精度"><a href="#3-1-ImageNet分类精度" class="headerlink" title="3.1 ImageNet分类精度"></a>3.1 ImageNet分类精度</h3><p>这部分实验比较了他们的方法与其他使用交叉熵的有监督方法的Top-1与Top-5精度，同时对比了他们的架构使用交叉熵损失的表现，可以看到，综合来说他们的方法实现了最好的效果，同时，他们的架构在使用交叉熵损失时的表现就不是非常好，相对来说，他们的架构在改进loss的情况下，Top-1精度提升了3.8/2.8个点，Top-5精度提升了1/2.3个点。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.3.png" alt="线性分类评估" style="zoom:80%;" /></p>
<h3 id="3-2-对图像损坏和校准的鲁棒性"><a href="#3-2-对图像损坏和校准的鲁棒性" class="headerlink" title="3.2 对图像损坏和校准的鲁棒性"></a>3.2 对图像损坏和校准的鲁棒性</h3><p>这部分实验评价了他们的方法对图像扰动的稳定性，具体来说，他们选择使用对ImageNet数据库中的图像应用常见的自然扰动，比如加噪声、模糊和对比度变化，构造得到的ImageNet-c数据集进行测试。使用平均损坏误差与平均相对损坏误差作为评价指标，可以看到，他们的方法的误差最小，且使用改进的对比损失替换交叉熵损失也有助于提升网络的性能。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.4.png" alt="平均损坏误差" style="zoom:80%;" /></p>
<p>此外，和交叉熵损失相比，本文的对比损失在不同程度的图像损坏下都能保持一个相对稳定的平均损失误差，相比于交叉熵损失也有更高的Top-1精度：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.8.png" alt="与交叉熵损失的对比" style="zoom: 67%;" /></p>
<h3 id="3-3-对超参数的鲁棒性"><a href="#3-3-对超参数的鲁棒性" class="headerlink" title="3.3 对超参数的鲁棒性"></a>3.3 对超参数的鲁棒性</h3><p>通常深度网络对超参数都很敏感，本文还比较了他们的改进对比损失对不同优化器、不同数据增强和学习率的分类精度稳定性。三种增强方式是本文提出的三种；优化器则选用了LARS、带动量的SGD和RMSProp；选择了最佳学习率以及增大或减小十倍的三个学习率进行评估，可以发现，本文提出的loss确实和交叉熵损失相比，对这三种超参数的变化更鲁棒。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.5.png" alt="对超参数的鲁棒性" style="zoom:67%;" /></p>
<h3 id="3-4-不同正对数量对模型表现的影响"><a href="#3-4-不同正对数量对模型表现的影响" class="headerlink" title="3.4 不同正对数量对模型表现的影响"></a>3.4 不同正对数量对模型表现的影响</h3><p>作者对比了每个子数据有1、2、3、5个正对时的Top-1精度，发现正对越多越有助于提升模型表现，当然同时计算成本也更大。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.7.png" alt="不同正对数量的影响" style="zoom:80%;" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/26/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/26/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url">《Contrastive Learning with Hard Negative Samples》阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-12-26T15:30:33+08:00">
                2020-12-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-动机和思路"><a href="#1-动机和思路" class="headerlink" title="1 动机和思路"></a>1 动机和思路</h2><p>对比学习在无监督表征学习领域的潜力无需多言，已经有非常多的例子证明其效果，目前比较多的针对对比学习的改进包括损失函数、抽样策略、数据增强方法等多方面，但是针对负对的研究相对而言更少一些，一般在构造正负对时，大部分模型都简单的把单张图像及其增强副本作为正对，其余样本均视为负对。这一策略可能会导致的问题是模型把相距很远的样本分得很开，而距离较近的负样本对之间可能比较难被区分。</p>
<p>基于此，本文构造了一个难负对的思想，主要目的在于，把离样本点距离很近但是又确实不属于同一类的样本作为负样本，加大了负样本的难度，从而使得类与类之间分的更开，来提升对比学习模型的表现。</p>
<h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h2><h3 id="2-1-难负样本选取原则"><a href="#2-1-难负样本选取原则" class="headerlink" title="2.1 难负样本选取原则"></a>2.1 难负样本选取原则</h3><p>好的难负样本有两点原则：1）与原始样本的标签不同；2）与原始样本尽量相似。</p>
<p>这一点就与之前的对比学习有比较明显的差异了，因为对比学习一般来说并不使用监督信息，因此除了锚点之外的其他样本，不管标签如何，都被认为是负对，所以问题的一个关键在于“<strong>用无监督的方法筛出不属于同一个标签的样本</strong>”。不仅如此，这里还有一个冲突的地方，既要与锚点尽可能相似，又得不属于同一类，这对于一个无监督模型来说是有难度的，因此本文在实际实现过程中进行了一个权衡，<strong>假如对样本的难度要求不是那么高的时候，就只满足原则1，而忽略原则2</strong>。同时，这种方法应该尽量<strong>不增加额外的训练成本</strong>。</p>
<h3 id="2-2-具体方法"><a href="#2-2-具体方法" class="headerlink" title="2.2 具体方法"></a>2.2 具体方法</h3><p>本文的重点在于如何进行难负样本采样，首先作者给出难负样本的采样分布函数：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.1-1608981534224.jpg" alt=""></p>
<p>即难负样本分布以与正类类别不同为条件的概率分布，$q_β(x^-)$是正负样本点积乘以系数$β$之后的指数项再乘以单纯的负样本采样分布，$β$控制采样的难易程度，值越大，代表样本越难。点积越大，$q_β(x^-)$就越大，表示这个样本更容易被采样，结合原则2，即尽量采样困难负样本。</p>
<p>但是这里没有解决原则1的问题，对于无监督方法，我们仍然不知道该怎么确定采样到的负样本与锚点的标签是不是一致的，这里作者用PU-learning的思想，把负样本分布$q_β(x^-)$拆成来自同标签分布$q_β^+(x^-)$与来自不同标签分布$q_β^-(x^-)$的两个部分：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.2-1608982447207.jpg" alt=""></p>
<p>这里对于来自$q_β^+(x^-)$的样本同样施以原则2，即为：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.3-1608982496092.jpg" alt=""></p>
<p>所以满足原则1和原则2的难负样本分布就可以写为：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.4-1608982538530.jpg" alt=""></p>
<p>上式的第一项就是常规的负样本分布，第二项作者提到可以使用语义保留转换去估计这一分布，传统无监督对比学习做的就是这件事儿。</p>
<p>所以原始对比损失：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.5-1608983198336.jpg" alt=""></p>
<p>就可以改写为只使用难负样本的情况：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.6-1608983226868.jpg" alt=""></p>
<p>分母第二项大括号里头的第一项就是之前的总负样本，第二项就是和锚点相似度很高的来自同一类的负样本。所以只需要求出分母中的两个均值，这个损失就算是给出来了。</p>
<p>计算这两项均值的方法，作者用的是蒙特卡洛重要性采样：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.7-1608983580203.jpg" alt=""></p>
<p>式子右边分母的$Z_β$和$Z_β^+$是两个配分函数，可以用均值近似:</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.8-1608983656405.jpg" alt=""></p>
<p>作者提出，在pytorch框架下，这个操作只需要额外两行代码九就能搞定，不需要做另外的操作。</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h2><p>作者以SimCLR为Baseline在几个数据集上进行了实验，给出了对应的实验效果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.9-1608984262715.jpg" alt=""></p>
<p>发现使用这种难负样本机制有所改善。</p>
<p>第二点是作者把训练好的，模型用到下游分类任务中，给出分类精度：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.10-1608984310866.jpg" alt=""></p>
<p>在部分数据集上比baseline有所提升。</p>
<p>但是这个β值并不是越大越好的，过大可能也存在问题：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.11-1608984359417.jpg" alt=""></p>
<p>可以发现，在β设置得较大的时候，模型的质量甚至会下降。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/24/Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/24/Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology/" itemprop="url">《Evaluating  the Disentanglement of Deep Generative Models through Manifold Topology》阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-12-24T18:06:57+08:00">
                2020-12-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-动机"><a href="#1-动机" class="headerlink" title="1 动机"></a>1 动机</h2><p>解耦工作对于模型的泛化、鲁棒性和可解释性来说至关重要，然而由于现有的解耦评测方法通常依赖于训练额外的、新的生成模型（分类器、编码器、回归器等）或在特定数据集以及使用特定方法预处理过的数据集上进行验证，因此评价结果的可靠性较差，且往往与任务较为相关，适用范围有限，也导致了用不同指标评测解耦模型时会出现多种排名结果。</p>
<h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h2><p>本文所提出的评价方法基于拓扑学原理，理论上较为抽象，在实现上，测量每个潜在维度（$z_i$）上的条件子流形的持续同调（TDA, <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31734839），然后，本文的评价核心思想即在于，**以解耦因子为条件的潜在子流形之间具有比以纠缠因子为条件的潜在子流形之间具有更高的拓扑相似度**。因此，采用W.RLT距离去度量这个拓扑相似性。">https://zhuanlan.zhihu.com/p/31734839），然后，本文的评价核心思想即在于，**以解耦因子为条件的潜在子流形之间具有比以纠缠因子为条件的潜在子流形之间具有更高的拓扑相似度**。因此，采用W.RLT距离去度量这个拓扑相似性。</a></p>
<p>本文所采用的从数据样本空间估计拓扑空间同调的方法是相对存在时间（Relative Living Times，RLTs），为了获取RLTs，如下图所示，首先假定训练好的生成模型分布$p_{model}(x)$由一个带孔的流形$M_{model}$支撑，从$p_{model}(x)$中采样出样本点集合$X$，从而得到一个基础的单纯形复合物，改变每个样本点的阈值（即图(d)中每个点的半径）用欧氏距离度量点与点之间的临近测度，在逐渐增大阈值时，会产生不同数量的k维孔（拓扑学中度量同调的重要依据），最终实现对持久性同调的逼近，得到持久性条形码（连续同调评价的产物）。</p>
<p>RLTs则是在改变阈值以生成的多个持久性条形码的矢量化，反映了每个k维孔出现和小时的持续时间内的离散分布，对RLTs取均值，则可以得到这些k维孔的平均相对生成时间，为一个离散概率分布，<strong>测量两个数据样本集之间的平均相对生成时间分布的相似性，就可以作为两个样本集之间拓扑相似性的评价依据</strong>。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.1.jpg" alt=""></p>
<p>为了说明其理论的有效性，作者可视化了解耦和非解耦的生成模型在dsprites数据集上的拓扑结构：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.2.jpg" alt=""></p>
<p>可以发现，解耦模型在尺寸和转角两个因子下的子流形明显不同胚，转角子流形有一维孔，旋转则没有，而每个因子内部的各个子流形是同胚的，而不解耦模型则没有这个性质。</p>
<p>传统的基于RLTs的拓扑相似性度量使用欧氏距离，但是经验上发现使用Wasserstein距离能显著改善度量精度。因此，作者基于W重心提出<strong>W.RLTs度量方法</strong>：</p>
<h3 id="度量思想："><a href="#度量思想：" class="headerlink" title="度量思想："></a>度量思想：</h3><p>前面已经提到，采样后得到的RLTs代表k维空穴存在与否的离散分布，首先求出这个离散分布的重心<img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.3.jpg" style="zoom:80%;" />：分布重心定义为“使用W-2距离计算到达分布中所有点的最小总距离所在的位置”，计算公式如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.4.jpg" alt=""></p>
<p>上面的$λ$是一个权重参数，其和为1。</p>
<p>在求出RLTs的重心之后，从两个角度评价其解耦性能的好坏：</p>
<p>1、 同一个因子内部的条件子流形应该有较高的拓扑相似性；</p>
<p>2、 不同因子的条件子流形具有较低的拓扑相似性。</p>
<p>因此，在某一因子$s_i$为条件下生成的条件子流形下，控制因子$s_i$的值不变，而其他因子的值可以变化，得到一个同一因子下的集群，测量这一集群内的拓扑相似性作为这一因子下的条件子流形的拓扑相似性结果；同时，测量不同因子作为条件的子流形之间的拓扑相似性，作为因子间拓扑相似性结果。作者在CelabA数据集上可视化了W.RLTs结果来说明这一思想，下图中的第一行是同一个因子内部的连续同调W.RLTs，下一行是不同因子之间的W.RLTs：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.5.jpg" alt=""></p>
<h4 id="无监督评价方法"><a href="#无监督评价方法" class="headerlink" title="无监督评价方法"></a>无监督评价方法</h4><p>在评价时，由于实际的因子$s_i$与实验设置的因子$z_j$之间可能并不存咋一一对应的连结，因此，本文使用W距离计算每个因子$z_j$为条件的子流形的拓扑相似性，也就是使用W距离计算不同条件子流形之间的分布重心的距离，得到一个$j<em>j$维的拓扑相似性矩阵$M$，在得到拓扑相似性矩阵之后，使用奇异值分解进行频谱共聚类，其目的在于合并同态的以设置因子$z$为条件的子流形，使得$z$可以与实际因子$s$相对应，即合并解耦作用相似的因子，降维。由此得到最终的$c</em>c$维的共聚类相似性矩阵$M_c$。作者可视化了不同模型在不同数据集下的共聚类相似性矩阵（颜色越深，代表相似性越强，对角线即上即为同一条件子流形的同态相似性可视化结果）：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.6.jpg" alt=""></p>
<p>之后，最小化共聚类相似性矩阵$M_c$的聚类内方差和聚类间方差，去求得最终的聚类数量c，使用共聚类相似性矩阵$M_c$，计算解耦得分$μ$。解耦得分定义为聚类内相似性和聚类间相似性之差：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.7.jpg" alt=""></p>
<p>因此，一个解耦效果较好的模型，想要获得较高的解耦得分，则希望有尽量大的聚类内相似性，同时聚类间的相似性尽量小，类间相似性非常直观，就是$M_c$对角元上的值，因此：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.8.jpg" alt=""></p>
<p>同样的，类间相似性就是$M_c$剔除对角元元素之外的值：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.9.jpg" alt=""></p>
<p>这就是无监督方法的聚类解耦评估方法。</p>
<h4 id="有监督评价方法"><a href="#有监督评价方法" class="headerlink" title="有监督评价方法"></a>有监督评价方法</h4><p>与上面的无监督方法相比，有监督方法的改动在于不再比较不同给定因子$z_j$，之间的拓扑相似性，而是对于有标签的数据集，同时计算实际因子$s_i$的W.RLTs，因此，在评价时，只计算$z_j$与$s_i$之间的拓扑相似性，得到$j*i$维的矩阵$M$，同样进行奇异值分解（具体怎么做没有再说明），因此类间相似性和类内相似性的计算如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.10.jpg" alt=""></p>
<p>与无监督指标不同的地方在于，有监督指标对计算结果进行了归一化处理：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.11.jpg" alt=""></p>
<h4 id="方法的限制"><a href="#方法的限制" class="headerlink" title="方法的限制"></a>方法的限制</h4><p>本评价方法假定数据流形是不完全对称的，因此没有考虑对称流形的情况，同时，RLTs不能计算数据流形的完整拓扑，而是进行逼近。</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h2><p>作者将本文提出的方法与MIG，一个使用一个分类器的方法Disentanglement和一个专门对人脸数据库解耦表现做评估的PPL方法进行比较，对不同解耦模型做排名，得到与其他指标相似的排名结果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.12.jpg" alt=""></p>
<p>大部分指标排名都比较相似，然而对于β-VAE，本文方法的指标与MIG相比差异较大，但是两种指标评价β-VAE时都有较大的方差，因此说明β-VAE的表现可能不是很稳定。同时还发现了两种训练目标比较相似的模型可能在解耦表现上有较大差异（Factor VAE和β-VAE）。</p>
<p>同时报告了定量结果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.13.jpg" alt=""></p>
<p>作者指出，大部分情况下，无监督评估指标更为适用，尤其对于CelebA，由于脸部信息过多，不易于用有监督方法进行评价，而如果想评估特定的解耦（发色，眼镜），用有监督的方法会更合适。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/22/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/22/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/" itemprop="url">四篇图像解耦工作简要介绍</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-12-22T19:20:07+08:00">
                2020-12-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-Variational-Interaction-Information-Maximization-for-Cross-domain-Disentanglement"><a href="#1-Variational-Interaction-Information-Maximization-for-Cross-domain-Disentanglement" class="headerlink" title="1 Variational Interaction Information Maximization for Cross-domain Disentanglement"></a>1 Variational Interaction Information Maximization for Cross-domain Disentanglement</h2><p>这篇文章的思想是使用信息论知识，实现跨域图像解耦表示，是基于VAE的一个改进工作。</p>
<p>对于图像对x，y，二者之间既有共享的表征信息，也有不共享的表征信息，这篇文章提了如下图所示的架构，训练一个VAE，同时学到X的特定表征，Y的特定表征以及X与Y之间的共享表征，这一目的通过最大化X与Y的联合分布之间的边际似然函数实现：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/QQ图片20201222160929.png" alt=""></p>
<p>在实际应用中直接优化上述公式有些困难，作者还做了其他的简化表达，经过改进后，其损失函数的思想在于，希望每个数据之前特异的特征被编码到各自单独的编码器$Z_x,Z_y$之中，而二者之间相互共享的表征则被同一个编码器所$Z_s$编码，为了实现这一目的（三类表征之间尽量分开），作者引入了互信息思想，希望尽可能最小化$I(Z_s, Z_x)$与$I(Z_s, Z_y)$来实现共享表征与特定表征之间的分离，具体实施时使用了如下图所示的公式：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/QQ截图20201222162104.jpg" alt=""></p>
<p>上面这个公式的第一项的作用在于鼓励$Z_s, Z_x$联合向共享域X中提供信息（与数据集X保持紧密联系），后两项的目的在于减少二者之间的信息总量。</p>
<p>同时，为了鼓励共享表征和特异表征之间的分离，还构造了如下的互信息正则化项：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.jpg" alt=""></p>
<p>上面两个等式中，最大化第一项意味着共享表征中含有来自数据集中提取到的表征，最小化第二项意味着从一个数据集中提取到的表征可以很容易地从另一个数据集中推断出来，意味着这个表征是共享的。</p>
<p>下图中的编码器r是设计来为下游任务（图像翻译和检索）使用的。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/1.0.jpg" alt="img;" style="zoom:;" /></p>
<p>解耦效果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/1.4.jpg" style="zoom:67%;" /></p>
<p>定量评估没有说明其具体的解耦指标得分，主要是对跨域图像检索进行的评估。</p>
<h2 id="2-ICAM-Interpretable-Classification-via-Disentangled-Representations-and-Feature-Attribution-Mapping"><a href="#2-ICAM-Interpretable-Classification-via-Disentangled-Representations-and-Feature-Attribution-Mapping" class="headerlink" title="2 ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping"></a>2 ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping</h2><p>一篇在医学影像领域的解耦应用，主要使用GAN-VAE架构，用来鉴别正常人与病患身体结构的差异性，进行影像诊断。网络框架如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/2.1.jpg" alt=""></p>
<p>简要介绍网络各个组成部分的作用：</p>
<p>内容编码器$E^c$：编码输入图像对中共享的与类别无关的信息，用鉴别器对编码特征进行判断，希望编码器对图像对的两张图像的共享信息输入趋同的特征；</p>
<p>属性编码器$E^a$：编码类别相关特征，用来分类；</p>
<p>生成器G：以上述两个编码器输出的特征作为联合输入，目的在于输出受内容特征与属性特征共同控制的图像；</p>
<p>特征映射$Attr Map$：定位类间差异区域。</p>
<p>整个框架基于GAN网络，同时引入VAE思想，使用编码器编码的特征作为重构输入而不是随机噪声，在生成虚假图像后再次进行一次图像生成，用二次生成的图像与真实图像之间的差异性作为最根本的损失。通过这一结构，其目的在于挖掘出决定相似图像类别差异的特征，实现类别与无关特征之间的解耦。</p>
<h2 id="3-Elastic-InfoGAN-Unsupervised-Disentangled-Representation-Learning-in-Class-Imbalanced-Data"><a href="#3-Elastic-InfoGAN-Unsupervised-Disentangled-Representation-Learning-in-Class-Imbalanced-Data" class="headerlink" title="3 Elastic-InfoGAN:  Unsupervised Disentangled Representation Learning in Class-Imbalanced Data"></a>3 Elastic-InfoGAN:  Unsupervised Disentangled Representation Learning in Class-Imbalanced Data</h2><p>本文的贡献是用InfoGAN实现对类不平衡数据的解耦。</p>
<p>InfoGAN假设数据服从均匀分布，因此在类别不平衡的数据中解耦表现较差：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.1.jpg" alt=""></p>
<p>这篇文章针对这一问题对InfoGAN做了两个改进，其一是不对数据分布进行假设，而将其视为优化过程中的可学习参数，为了实现这一点，采用Gumbel-Softmax分布作为噪声的潜在分布，该分布有可微参数，因此可以进行更新；其二是通过实验发现InfoGAN在类不平衡信息中很容易学到图像的低级特征（与之前分享的解释对比学习工作的发现有异曲同工之妙），因此这篇文章引入对比学习的思想，对数据进行增强，强迫模型学习身份表示，以抑制类不平衡的影响。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.2.jpg" alt=""></p>
<p>整篇文章的工作重点就是上述的两个方面，其一，用可学习分布代替InfoGAN假设的均匀分布，优化InfoGAN的同时更新分布（左图）。k维类别潜码的采样方法如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.3.jpg" alt=""></p>
<p>$g_i$代表从Gumbel（0，1）分布采样的样本点，温度参数代表不同类之间的相似程度，假如温度参数很小，将会趋近于onn-hot编码（均匀分布）。</p>
<p>其二，使用简单数据增强方法给数据构造一个正对，同时引入负对，添加一个对比损失项，强迫模型学习身份表示。使用的是常规的对比损失：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.4.jpg" alt=""></p>
<p>最终的损失为InfoLoss和对比损失之和：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.5.jpg" alt=""></p>
<p>定性实验结果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.6.jpg" alt=""></p>
<p>定量实验结果：</p>
<p>使用NMI和ENT（平均熵，评价同一个潜码生成的图像是否属于同一类；每一个潜码是否只与一个真实类别标签关联；越小越好）作为评价指标：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.7.jpg" alt=""></p>
<h2 id="4-WAE模型"><a href="#4-WAE模型" class="headerlink" title="4 WAE模型"></a>4 WAE模型</h2><p>ICLR2021中有两篇论文在WAE（WASSERSTEIN AUTOENCODER）的框架下进行解耦图像生成，WAE是2018年由Google在WGAN的基础上提出来的一种自编码器模型，由于目前没有了解其原理，因此只对这两篇论文在WAE基础上的改进进行简单的介绍。</p>
<h3 id="4-1-Learning-disentangled-representations-with-the-Wasserstein-Autoencoder"><a href="#4-1-Learning-disentangled-representations-with-the-Wasserstein-Autoencoder" class="headerlink" title="4.1 Learning  disentangled representations with the Wasserstein Autoencoder"></a>4.1 Learning  disentangled representations with the Wasserstein Autoencoder</h3><p>想法是把β-TCVAE的构造移植到WAE模型中，重点在于利用TCVAE的loss对WAE进行改进：</p>
<p>TCWAE loss：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.1.jpg" alt=""></p>
<p>β-TCVAE loss：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.2.jpg" alt=""></p>
<p>对比两项损失，可以发现TCWAE具有与β-TCVAE几乎相同的loss函数，区别在于没有最后一个互信息项，以及在第一项的度量上有所差异。</p>
<p>效果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.3.jpg" alt=""></p>
<h3 id="4-2-DISENTANGLED-RECURRENT-WASSERSTEIN-AUTOENCODER"><a href="#4-2-DISENTANGLED-RECURRENT-WASSERSTEIN-AUTOENCODER" class="headerlink" title="4.2 DISENTANGLED RECURRENT WASSERSTEIN AUTOENCODER"></a>4.2 DISENTANGLED RECURRENT WASSERSTEIN AUTOENCODER</h3><p>第二篇WAE相关的文章是将WAE应用于时序图像解耦的工作，用来捕捉时序图像上的相关信息，实现静态因子和动态因子的解耦。</p>
<p><img src="" alt=""><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.4.jpg" alt="4.4" style="zoom:67%;" /></p>
<p>上图是这篇文章提出了来的模型的解耦效果，每一行都代表一个时序（对应不同表情）。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/14/SimCLR%EF%BC%9A%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/14/SimCLR%EF%BC%9A%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" itemprop="url">SimCLR：用于视觉表征的对比学习框架</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-14T13:27:16+08:00">
                2020-09-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>论文链接: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.05709.pdf">https://arxiv.org/pdf/2002.05709.pdf</a></p>
<p>官方github链接: <a target="_blank" rel="noopener" href="https://github.com/google-research/simclr">https://github.com/google-research/simclr</a></p>
<p>他人复现pytorch链接: <a target="_blank" rel="noopener" href="https://github.com/sthalles/SimCLR">https://github.com/sthalles/SimCLR</a> </p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/0.gif" style="zoom:67%;" /></p>
<h2 id="1-概况"><a href="#1-概况" class="headerlink" title="1 概况"></a>1 概况</h2><p><strong>核心观点：</strong></p>
<ol>
<li>数据增强(data augmentations)的组合对预测任务的表现有重要影响，对于非监督学习而言，数据增强的提升作用更大；</li>
<li>本文定义了一个对比损失和表征之间的可学习非线性转换，大幅提高了表征的质量；</li>
<li>具有对比交叉熵损失(contrastive cross entropy loss)的表征学习得益于归一化嵌入和适当地调整温度参数；</li>
<li>与监督学习相比，对比学习可以通过更多的训练和更大的Batch Size 获得更好的表现，更深更宽的网络对对比学习表现的提升也有益。</li>
</ol>
<p><strong>框架效果：</strong></p>
<ol>
<li><p>在对Image Net 进行分类实验时，在top-1精度上取得了76.5%的结果，获比之前最先进的无监督或半监督提升7%，与有监督的Res-Net-50性能相媲美；</p>
</li>
<li><p>对Image Net 1%的Label 进行微调时，SimCLR 实现了85.8%的top-5精度，相对性能提升10%,超越了100× fewer label的AlexNet；</p>
</li>
<li><p>在其他自然图像分类数据集上进行微调时，SimCLR 在12个数据集中的10个数据集上的表现相当于或优于强监督的Baseline。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/1.png" alt="ImageNet Top-1精度表现" style="zoom: 67%;" /></p>
</li>
</ol>
<h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h2><h3 id="2-1-对比学习框架"><a href="#2-1-对比学习框架" class="headerlink" title="2.1 对比学习框架"></a>2.1 对比学习框架</h3><p>SimCLR 通过潜在空间上的对比损失，最大化相同数据示例的不同增强视图之间的协议进行表征学习，主要由四个主要组件组成：</p>
<ol>
<li><p><strong>随机数据增强模块</strong></p>
<p>将任意给定的数据示例随即转换为同一示例的两个相关视图，用$\widetilde{x}_i$和$\widetilde{x}_j$表示，将其视为一个正对。本文使用了三种方法进行数据增强：<strong>随即裁剪和调整（如随机翻转）</strong>（裁剪后调整图像尺寸为原图大小），<strong>随机色彩失真</strong>和<strong>随机高斯模糊</strong>，作者认为随即裁剪和色彩失真的结合是使网络具有良好性能的关键。</p>
</li>
<li><p><strong>神经网络基编码器(base encoder)</strong></p>
<p>基编码器定义为$f(·)$，其作用在于从增强后的数据集中提取表征向量。作者认为SimCLR可以在无任何约束的情况下选择各种网络架构的基编码器，论文中选择的是常见的ResNet。因此，对于数据$\widetilde{x}_i$，有：</p>
<script type="math/tex; mode=display">
\boldsymbol{h}_i=f(\widetilde{x}_i)=ResNet(\widetilde{x}_i)</script><p>其中，$\boldsymbol{h}_i\in\R^d$，为经过平均池化层之后的输出。</p>
</li>
<li><p><strong>小型神经网络投影头(projection head)</strong></p>
<p>投影头$g(·)$的作用是将编码后的表征$h_i$映射到应用对比损失的潜在空间中，本文使用的是一个两层MLP，具体计算方法如下：</p>
<script type="math/tex; mode=display">
z_i=g(h_i)=W^{(2)}(\sigma(W^{(1)}(h_i)))</script><p>其中，$\sigma$是ReLU 函数，用$z_i$比$\boldsymbol{h}_i$更好构造对比损失。</p>
</li>
<li><p><strong>对比损失函数</strong></p>
<p>在对比任务中，倘若给定一个包含正对$\widetilde{x}_i$和$\widetilde{x}_j$的数据集$\left\{\widetilde{x}_k\right\}$，重构损失的作用是从$\left\{\widetilde{x}_k\right\}_{k{\neq}i}$中找出给定$\widetilde{x}_i$对应的正对$\widetilde{x}_j$。</p>
</li>
</ol>
<p>因此，整个SimCLR的结构如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/2.png" alt="SimCLR对比学习框架结构图" style="zoom:67%;" /></p>
<ol>
<li><p><strong>样本构造：</strong></p>
<p>对于预测任务，随机采样一小批($N$个))数据样本，之后采用数据增强方法，将每一个样本进行扩充，一共得到$2N$个数据点，即$N$个正对。在本文中，不直接构造负对，而是对于给定的正对$\left\{\widetilde{x}_i, \widetilde{x}_j\right\}$，将除该样本对以外的其他$2(N-1)$个样本示例都视为负样本。</p>
</li>
<li><p><strong>损失函数构造：</strong></p>
<p>本文的对比损失函数基于$l_2$正则化，是从其他论文中参考得到的，称我为NT-Xent(标准化温度尺度的交叉熵损失)。该loss首先定义了一个sim函数表示正则化：$\rm{sim}(\boldsymbol{u},\boldsymbol{v})={\boldsymbol{u}^\mathsf{T}}\boldsymbol{v}/|\boldsymbol{u}||\boldsymbol{v}|$，正对$\left\{\widetilde{x}_i, \widetilde{x}_j\right\}$之间的损失函数即为：</p>
<script type="math/tex; mode=display">
\ell_{i,j}=-\log\frac{\exp(\rm{sim}(\it{z_i},z_j\rm)/\tau)}{\sum_{k=1}^{2N}{\mathbb{l}_{[k{\neq}i] }\exp(\rm{sim}(\it{z_i},z_j\rm)/\tau)} }</script><p>上式中，$\mathbb{l}_{[k{\neq}i]}$是一个指示函数，当且仅当$k=i$时取0，否则为1。$\tau$是进行优化的温度参数，后续实验显示为0.1时效果最好。</p>
</li>
<li><p><strong>算法概述</strong></p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.png" alt="SimCLR框架算法" style="zoom:80%;" /></p>
<p>根据算法可以发现，在构造数据时，对$\widetilde{x}_i$与$\widetilde{x}_j$分别采用一种增强方法，并且分别计算了一个正对中每个数据点对另一种增强方法所有数据的余弦相似性。</p>
</li>
</ol>
<h3 id="2-2-大Batch-Size训练方法"><a href="#2-2-大Batch-Size训练方法" class="headerlink" title="2.2 大Batch Size训练方法"></a>2.2 大Batch Size训练方法</h3><p>作者提到，为了保持框架的简单化，在模型训练时不采用记忆库训练模型，而是采用大Batch Size的形式，将Batch Size从256扩大为<strong>8192</strong>，在进行数据增强之后，一个正对将有$（8192-1）×2=16382$个负示例。为了克服大Batch Size使用SGD/Momentum进行优化时可能导致的训练不稳定问题，论文对每个Batch Size都采用了<strong>LARS</strong>优化器。</p>
<p>此外，由于标准ResNet架构采用Batch Normlization(BN)进行规范化，作者指出，在具有数据并行性的分布式训练中，BN的均值和方差通常在每张卡上进行局部的聚合。而论文提出的对比学习中，正对在同一张卡中计算，模型可以利用局部信息泄漏提高预测精度，而不需要改善表征，他们在训练过程中使用了一种<strong>Global BN</strong>的方法，取所有卡上BN的均值和方差作为表示。</p>
<h3 id="2-3-评价方案"><a href="#2-3-评价方案" class="headerlink" title="2.3 评价方案"></a>2.3 评价方案</h3><ol>
<li><p><strong>数据集选择</strong></p>
<p>大部分实验都在<strong>ImageNet ILSVRC-2012</strong> 数据集上进行；少部分实验在<strong>CIFAR-10</strong>上进行；还使用了一些广泛用于<strong>迁移学习</strong>的数据。</p>
<p>使用线性评估协议进行评估，在一个冻结基网络的顶层训练一个线性分类器，测试分类精度，也比较了一些迁移学习和半监督方法。</p>
</li>
<li><p><strong>默认设置</strong></p>
<p>除非另有规定：</p>
<p>在数据增强时使用<strong>随机裁剪和调整(随机翻转)</strong>，<strong>颜色失真</strong>，和<strong>高斯模糊</strong>三种方法。</p>
<p>使用<strong>ResNet-50</strong>作为基础网络编码器,并使用两层MLP将编码后的表征投影到一个128维的潜在空间之中。</p>
<p>训练时，使用<strong>NT-Xent</strong> loss，使用<strong>LARS</strong>进行优化，设置学习率为$4.8 (= 0.3×Batch Size/256)$，权值衰减率为$10^{-6}$。Batch Size大小为4096，共训练100个epoch。此外，在前10个epoch使用线性预热，并在不重新启动的情况下使用余弦衰减计划衰减学习速率。</p>
</li>
</ol>
<h2 id="3-数据增强"><a href="#3-数据增强" class="headerlink" title="3 数据增强"></a>3 数据增强</h2><h3 id="3-1-数据增强涵盖了对比预测任务的两种情况"><a href="#3-1-数据增强涵盖了对比预测任务的两种情况" class="headerlink" title="3.1 数据增强涵盖了对比预测任务的两种情况"></a>3.1 数据增强涵盖了对比预测任务的两种情况</h3><p>作者指出，目前数据增强方法尚未被视为对比预测任务的标准手段之一，目前流行的手段都是通过改变网络架构实现对比预测，作者认为他们的数据增强方法可以在数据层面即涵盖两种典型的对比预测任务，分别是<strong>整体——局部预测</strong>和<strong>邻近预测</strong>。</p>
<p>具体来说，如下图所示，在对图像应用随机裁剪时，将会出现如下图左右两张图的正对样本，这些样本包含了上述的两种对比预测任务情形（A, B: 整体——局部；C, D: 临近）。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/4.png" alt="图像增强在对比预测上的应用" style="zoom:67%;" /></p>
<h3 id="3-2-不同数据增强方式的组合对表征学习效果至关重要"><a href="#3-2-不同数据增强方式的组合对表征学习效果至关重要" class="headerlink" title="3.2 不同数据增强方式的组合对表征学习效果至关重要"></a>3.2 不同数据增强方式的组合对表征学习效果至关重要</h3><p>作者通过下图展示了多种数据增强的方法，需要注意的是，在实验中，其只使用了随即裁剪（包括裁剪、大小调整和翻转）、颜色失真和高斯模糊三种手段。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.png" alt="数据增强手段举例"></p>
<p>在实验过程中，作者通过研究单独和成对数据增强方法对SimCLR框架性能的影响。由于ImageNet数据库中的图像大小并不一致，在使用数据时通常已经进行了图像裁剪和调整操作，由于很难剔除随机裁剪的影响，因此作者通过不对称转换改善这一情况。</p>
<p>具体来说，首先对数据样本次啊用随机裁剪，并将其调整到相同的分辨率大小，之后对一个框架的两个分支中的一个应用目标转换$t(·)$，而另一个分支作为标志，即令：$t(x_i)=x_i$。</p>
<p>作者认为这种非对称数据的增加会对框架性能产生影响，但不应该在实质上改变单个数据增强或组合数据增强方法的影响。</p>
<p>作者测评了不同方法和组合方法的top-1识别结果，如下图所示。可以发现，单一数据增强方法都不足以学到良好的表征，即使模型可以识别出任务中的正对，组合增强方法会增大预测难度，但是能够显著提升表征质量，最后作者发现，随机裁剪和颜色失真的组合最有利于学习表征。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/6.png" alt="数据增强方法对表征学习的影响"></p>
<h3 id="3-3-对比学习比监督学习更需要数据增强"><a href="#3-3-对比学习比监督学习更需要数据增强" class="headerlink" title="3.3 对比学习比监督学习更需要数据增强"></a>3.3 对比学习比监督学习更需要数据增强</h3><p>作者调整了颜色增强的强度，发现颜色增强大大提升了SimCLR的线性评估结果。而监督学习使用更加复杂的自动增强方法时，表征学习的表现也没有比简单的裁剪+颜色失真效果好，不同的颜色增强强度也没有提升或削弱监督学习的表现。因此作者认为对比学习比监督学习可以从颜色增强中获得更大收益。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/7.png" alt="颜色增强对不同监督模型的增益" style="zoom:67%;" /></p>
<h2 id="4-Encoder-和Head-的架构"><a href="#4-Encoder-和Head-的架构" class="headerlink" title="4 Encoder 和Head 的架构"></a>4 Encoder 和Head 的架构</h2><h3 id="4-1-更大的模型使无监督对比模型表现更好"><a href="#4-1-更大的模型使无监督对比模型表现更好" class="headerlink" title="4.1 更大的模型使无监督对比模型表现更好"></a>4.1 更大的模型使无监督对比模型表现更好</h3><p>下图中绿色“×”是有监督ResNet 训练90个epoch的结果，红色“★”是本文模型训练100个epoch的及如果，蓝色“·”是本文模型训练100个epoch的结果，可以发现，随着网络参数的增加，有监督和无监督模型的表现都有所提升，但无监督模型的提升更大。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/8.png" alt="网络参数数量对表征识别的影响" style="zoom:67%;" /></p>
<h3 id="4-2-非线性投影head有助于提升其之前网络层的表征质量"><a href="#4-2-非线性投影head有助于提升其之前网络层的表征质量" class="headerlink" title="4.2 非线性投影head有助于提升其之前网络层的表征质量"></a>4.2 非线性投影head有助于提升其之前网络层的表征质量</h3><p>非线性投影head，即前文提到的$g(\boldsymbol{h})$，本文对比了包括非线性投影、线性投影以及无投影在内的三种映射方式对表征质量的影响，如下图所示。最终发现非线性投影的Top-1精度比无映射提升10%以上，比线性投影提升3%，并且这一结果在只使用一个head时不受输出维度的影响。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/9.png" alt="不同映射方式对表征质量的影响" style="zoom: 80%;" /></p>
<p>此外，作者提到使用非线性投影时，head前面的隐藏层也可以学到更好的表征。</p>
<p>同时，作者通过对使用非线性投影后的映射$z$ 以及非线性投影之前的映射$\boldsymbol{h}$对不同表征的预测表现进行对比，发现$z$ 比$\boldsymbol{h}$保留了更少的信息，具体如下表所示。作者认为是因为$z$删除了一些颜色之类的信息，使得在训练过程中这些信息更多在$\boldsymbol{h}$阶段形成。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/10.png" alt="不同方法保留信息的大小" style="zoom:67%;" /></p>
<h2 id="5-损失函数和Batch-Size"><a href="#5-损失函数和Batch-Size" class="headerlink" title="5 损失函数和Batch Size"></a>5 损失函数和Batch Size</h2><h3 id="5-1-带有可调温度参数的归一化交叉熵损失是更优选择"><a href="#5-1-带有可调温度参数的归一化交叉熵损失是更优选择" class="headerlink" title="5.1 带有可调温度参数的归一化交叉熵损失是更优选择"></a>5.1 带有可调温度参数的归一化交叉熵损失是更优选择</h3><p>作者对比了本文使用的NT-Xent损失和下表中另外两种对比损失应用于此框架的表现。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/11.png" alt="不同loss表达式" style="zoom:67%;" /></p>
<p>作者认为NT-Xent损失中的温度参数可以帮助模型从hard-negative中学习，而其他损失函数没有这个特点，必须引入额外的semi-hard-negative进行挖掘，在均使用l2正则化（余弦相似性）时，发现即使是用了semi-hard-negative的其他loss，表现也不如NT-Xent。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/12.png" alt="不同loss表现对比" style="zoom:67%;" /></p>
<p>在此基础上，作者又测试了使用l2正则化和只使用点积的情况以及不同温度参数对对比任务精度和通过Top-1预测的表征表现情况，发现不使用l2正则化会提升对比任务精度，但是降低了表征的线性评估结果。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/13.png" alt="l2正则化和温度参数影响探究" style="zoom:67%;" /></p>
<h3 id="5-2-大Batch-Size和更多训练对对比学习更有益"><a href="#5-2-大Batch-Size和更多训练对对比学习更有益" class="headerlink" title="5.2 大Batch Size和更多训练对对比学习更有益"></a>5.2 大Batch Size和更多训练对对比学习更有益</h3><p>作者通过实验发现，大Batch Size 和大epoch 对框架的表现具有提升，因为增大这两者会提供更多的负例。不过随着epoch的增加，不同Batch Size效果之间的差异在减小。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/14.png" alt="不同epoch与batch size的影响" style="zoom:67%;" /></p>
<h2 id="6-实验对比"><a href="#6-实验对比" class="headerlink" title="6 实验对比"></a>6 实验对比</h2><h3 id="6-1-线性评估"><a href="#6-1-线性评估" class="headerlink" title="6.1 线性评估"></a>6.1 线性评估</h3><p>这部分实验用预训练好的一些无监督模型训练了一个线性分类器，评估在ImageNet上的效果，可以发现，SimCLR的Top-1和Top-5表现都很优秀，当加深网络时，无监督模型的表现普遍提升（括号中的(4×)的意思是网络宽度是原网络的4倍）。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/15.png" alt="线性评估" style="zoom:80%;" /></p>
<h3 id="6-2-半监督学习"><a href="#6-2-半监督学习" class="headerlink" title="6.2 半监督学习"></a>6.2 半监督学习</h3><p>对ImageNet采样1%和10%有标签的训练集（分别是12.8张与128张图片一个类）进行微调，比较不同网络的效果。</p>
<p>微调过程使用名为Nesterov动量优化器进行调整，对于1%标记的数据，对60个epoch进行微调，对于10%标签的数据，对30个epoch进行微调，同时调整图像大小为256×256，之后在中央应用裁剪，裁剪到224×224的大小。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/16.png" alt="半监督学习" style="zoom: 80%;" /></p>
<h3 id="6-3-迁移学习"><a href="#6-3-迁移学习" class="headerlink" title="6.3 迁移学习"></a>6.3 迁移学习</h3><p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASimCLR%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/17.png" alt="迁移学习" style="zoom:67%;" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">微澜</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
