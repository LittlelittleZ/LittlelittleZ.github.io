<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="小舟从此逝，江海寄余生。">
<meta property="og:type" content="website">
<meta property="og:title" content="缓缓行舟">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="缓缓行舟">
<meta property="og:description" content="小舟从此逝，江海寄余生。">
<meta property="og:locale">
<meta property="article:author" content="微澜">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/"/>





  <title>缓缓行舟</title>
  








<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">缓缓行舟</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">给行船途中的所感所获一个容身之处</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/19/Moco%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/19/Moco%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94/" itemprop="url">Moco：无监督视觉表征学习的动量对比</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-19T16:08:44+08:00">
                2021-01-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h2><p>在NLP领域，无监督表征学习有GPT、BERT等效果非常好的模型，但是CV领域还是有监督模型作为主流。作者认为主要是因为CV和NLP领域处理的数据对应的信号空间不同：语言任务有离散的信号空间，词语词之间可以视为是独立的词组，能够很方便地构成字典（Dictionary），这种词典是无监督学习便于学习依赖的特征；而视觉领域的原始信号是在一个连续且高维空间中，无法成为结构化的信号，使得无监督学习难以展现在NLP领域发挥出的效果。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.1.png" alt="moco原理" style="zoom:80%;" /></p>
<p>作者提出了一种动态字典，即MOCO，新的minibatch对应的字典在进入队列时，将会替换最早进入的字典，使得字典始终是所有数据的子集，又始终代表最新的表征，经过试验发现，Moco的表现非常之好。</p>
<h2 id="2-思想"><a href="#2-思想" class="headerlink" title="2 思想"></a>2 思想</h2><p>对比学习的思想在于，通过一些已编码的query（q），使其与其对应的key（k）相对应，k是被编码的样本的在字典中的key$\lbrace k_0,k_1,k_2…\rbrace$，假设$q$与$k_+$相匹配，对比损失的目的是尽量拉近$q$与$k_+$之间的距离而增大$q$与其他$k_i$之间的距离（拉近正对，缩小负对，其他的$k_i$都是负对），Moco用的是点积的方法衡量相似度，称为InfoNCE loss：</p>
<script type="math/tex; mode=display">
\ell_{i,j}=-\log\frac{\exp(q·k_+/\tau)} {\sum_{i=0}^{K} {\exp(q·k_i/\tau) } }</script><p>其中，$\tau$是温度参数，一般来说，query$q=f_q(x^q)$，相应的，$k=f_k(x^k)。$$f_q，f_k$都是一个编码器网络，二者可以参数完全相同，也可以部分共享参数，甚至完全不同而$x_q，x_k$既可以是图片，也可以只是patch，或者是一系列的patch。</p>
<p>作者认为对比学习是在高维连续输入上构建离散字典的方法，由于key是随机采样产生的，并且$f_k$会在训练过程中进化，因此这个字典是动态的。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20200822184022179.png" alt="image-20200822184022179" style="zoom:80%;" /></p>
<p>作者还提到，使用队列的方法构建字典会使字典变大，但是也使得通过反向传播更新编码器参数变得困难，很简单的解决方法可以通过将$f_q$的梯度复制到$f_k$中，以忽略$f_k$的梯度，但是作者通过试验发现这样的<strong>效果很差</strong>，作者猜测可能的原因是因为<strong>编码器的快速更新降低了编码出来的关键表示的一致性</strong>。因此作者放弃了参数共享的想法，转而采用速度较慢的动量更新来解决这个问题，从而在一定程度上保证了队列中各个key之间的一致性。最终采用如下公式更新$f_k$的参数：</p>
<script type="math/tex; mode=display">
θ_k←mθ_k+(1-m)θ_q</script><p>m就是动量置信度，属于[0,1)，在这个过程中，只有$f_q$的参数通过loss使用反向传播进行更新，并且在实际应用过程中，一般将m设置为比较大的值，以使得$f_k$的更新尽量缓慢，从而使每个k的相似度更好，本文设置的m的默认值是0.999（作者实验表明，更大的动量值比更小的动量值表现得更好，也就是说$enc_k$的更新越慢越好）.</p>
<h2 id="3-与其他方法的对比"><a href="#3-与其他方法的对比" class="headerlink" title="3 与其他方法的对比"></a>3 与其他方法的对比</h2><p>作者对比了另外两种方法，分析了这两种方法的缺点：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.2.png" alt="三种方法对比" style="zoom:80%;" /></p>
<ol>
<li><p><strong>end-to-end</strong>端到端</p>
<p>端到端的编码方法是最基础的方法之一，用当前mini batch中的示例构建字典，每一个key都是通过相同参数的$f_k$编码得到的，相当于是被一致编码。但是字典大小与mini batch大小叠加，受到GPU内存的限制，同时也有大mini batch优化上的问题。最近的一些方法基于局部位置驱动的借口任务，字典的大小可以通过多个局部增大，但是这种架构需要特殊的网络设计，会使得网络向下游任务的传输更加复杂。</p>
</li>
<li><p><strong>memory bank</strong>记忆库</p>
<p>另一种方法是记忆库方法，一个记忆库由数据集中所有样本的表征组成，每一个mini batch的字典通过从记忆库中随机采样获得，而不需要进行反向传播，因此可以支持很大的字典，但是记忆库中方你样本的表征在最后一次看到样本时会被更新，导致采样的key在过去的时间里基本上是在编码器的不同步骤上生成的，不太一致。</p>
</li>
</ol>
<h2 id="4-技术细节"><a href="#4-技术细节" class="headerlink" title="4 技术细节"></a>4 技术细节</h2><p>在构造实验时，将与query q来自于同一图像的key k构造成一组正对，来自其他图像的key则视为负示例，在实现时也是对一张图像进行数据增强，生成两个随机子样本作为一对正对，之后将增强后的子样本分别投入$f_q，f_k$，生成q与k。编码器可以是任意的卷积神经网络。</p>
<p>在具体实现时，用如下算法进行构造：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.3.png" alt="算法伪代码" style="zoom:80%;" /></p>
<p>此外，对于编码器，本文使用了ResNet50，之后进行全局平均池化，再经过一个全连接层，输出一个128维的向量，按照l2范数进行归一化处理，得到最终的q与k的表征。温度参数设置为0.07。</p>
<p>进行数据增强时，从随即调整大小后的图像中裁剪出224*224大小的图像，之后再进行随机的颜色抖动、随机水平翻转与随机灰度转换处理得到子样本。</p>
<p>Shuffling BN</p>
<p>多GPU训练时会造成信息泄露，一个解决方法是，在对key进行BN时，先对当前的mini batch进行顺序洗牌，在编码后再次进行顺序洗牌，而对q不进行处理。</p>
<h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5 实验"></a>5 实验</h2><p><strong>实验数据集包括</strong>：ImageNet的训练集（128万张）、Instagram上1500个话题中的9400万张图像，话题内容与ImageNet上的类别对应，与ImageNet相比，这个数据集上的图像有一个长尾的，不平衡的数据分布，更接近真实世界的数据。</p>
<p><strong>实验用的优化算法</strong>：用的是SGD算法，动量设为0.9，在ImageNet上的Batch Size是256，用了8块GPU，初始学习率0.03，训练200个epoch，在120和160个epoch时将学习率乘以0.1；在Instagram数据集上的Batch Size为1024，用了64块GPU，初始学习率为0.12，在每62.5k的iterations时将学习率用指数衰减0.9倍，共训练125万个iterations。二者用的都是ResNet50。</p>
<h3 id="5-1-线性分类评估"><a href="#5-1-线性分类评估" class="headerlink" title="5.1 线性分类评估"></a>5.1 线性分类评估</h3><p>本实验在Image Net数据集上进行。首先训练好Moco和其他对比模型，之后冻结feature，把模型输出的feature作为监督训练一个线性分类器100个epoch，之后测量1-crop和top-1精度。</p>
<ol>
<li><strong>与其他对比架构的性能比较</strong></li>
</ol>
<p>这个实验比较了他之前提到的end-to-end和memory bank的机制，都使用InfoNCE loss。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.4.png" alt="与其他架构的消融实验对比" style="zoom:80%;" /></p>
<p>可以看到，Moco的性能表现确实最好，而且三种方式都能受益于更大的K，也就是更大的字典数量更有助于提升性能。此外，Moco和memory bank可以实现更大的K，而end-to-end的架构使得其算力限制了对大字典的支持。</p>
<p>作者还探究了不同momentum的影响，发现取0.999时表现最好。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.5.png" alt="不同momentum的精度" style="zoom:80%;" /></p>
<ol>
<li><strong>与其他非监督学习方法的比较</strong></li>
</ol>
<p>在这一部分，作者比较了K设为65536，m设为0.999时，ResNet50下不同非监督方法的性能，还比较了2×和4×宽度的网络的表现效果，具体来说，综合对比了<strong>参数数量</strong>和<strong>分类精度</strong>。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.6.png" alt="点状图" style="zoom:80%;" /></p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.7.png" style="zoom:80%;" /></p>
<p>根据实验结果，Moco在ResNet50下的精度达到了60.6%，是同等参数数量的网络中最好的，并且在4×网络下达到了<strong>68.6%</strong>，而且没有对网络采用什么特殊的架构。</p>
<h3 id="5-2-迁移特征"><a href="#5-2-迁移特征" class="headerlink" title="5.2 迁移特征"></a>5.2 迁移特征</h3><p>这一部分主要做的是用训练好的模型与其他在ImageNet训练好的模型做分割与检测实验，用的数据集是PASCAL VOC和COCO。</p>
<p>在进行实验之前，首先进行了特征正则化，用的方法是对经过训练的BN进行微调（并且在GPU之间同步），之后在新初始化的层中也使用BN，用来校准 magnitudes。</p>
<p>作者还控制了Schedule，这一部分没有太搞清楚，后期继续学习，在具体控制时，COCO是1×（大约12个epoch）或2×的schedule；</p>
<ol>
<li><strong>在PASCAL VOC上进行目标检测</strong></li>
</ol>
<p>目标检测模型是Faster R-CNN，主干是ResNet50-dilated-C5或ResNet50-C4，应用已经调好的BN。对所有层都进行端到端的微调。</p>
<p>训练时的图像是[480,800]的像素，推理时用的是800，评估默认的VOC度量为$AP_{50}$（IoU阈值为50%：交并比，即模型画出的框与人工标注框的重叠率在50%以上，则认为通过测试）和COO-style $AP$和$AP_{75}$，在VOC test2007上进行实验。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.8.png" style="zoom:80%;" /></p>
<p>可以看到Moco在Instagram下训练的效果会比有监督的ResNet50好。</p>
<p>和其他对比学习框架比起来表现也更优异：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.12.png" style="zoom:80%;" /></p>
<p>一般来说，无监督方法的预测结果都要差一些，但是Moco的性能在有些测试中比有监督方法还要好：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.9.png" alt="不同方法对比"></p>
<ol>
<li><strong>在COCO上进行目标检测</strong></li>
</ol>
<p>目标检测模型是Mask R-CNN，主干是FPN或C4，应用已经调好的BN。对所有层都进行端到端的微调。</p>
<p>训练时的图像是[640,800]的像素，推理时用的是800，评估默认的VOC度量为$AP_{50}$（IoU阈值为50%：交并比，即模型画出的框与人工标注框的重叠率在50%以上，则认为通过测试）和COCO-style $AP$和$AP_{75}$，在train2007数据集上进行微调，在val2017上进行评估，schedule是默认的1×或2×。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.10.png" alt="COCO上的实验结果"></p>
<p>可以看到，MOCO的表现基本上好于有监督模型，此外，在执行特定任务的，Moco的表现也不错：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8AMoco%20%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.11.png" alt="Moco表现情况" style="zoom:80%;" /></p>
<h2 id="6-基于SimCLR的改进版Moco-v2"><a href="#6-基于SimCLR的改进版Moco-v2" class="headerlink" title="6 基于SimCLR的改进版Moco v2"></a>6 基于SimCLR的改进版Moco v2</h2><p>SimCLR是一个end-to-end的对比学习框架，取得了比较好的效果，受SimCLR的启发，作者对Moco进行了改进，主要是对数据增强的方法进行了改进，并添加了SimCLR中的MLP投影头，提升了网络的性能(在Image Net上线性分类的表现提升了6.9%)。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/19/Supervised%20Contrastive%20Learning%EF%BC%9A%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/19/Supervised%20Contrastive%20Learning%EF%BC%9A%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" itemprop="url">Supervised Contrastive Learning：有监督对比学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-19T14:56:15+08:00">
                2021-01-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-概要"><a href="#1-概要" class="headerlink" title="1 概要"></a>1 概要</h2><p>交叉熵损失是监督学习中应用最广泛的损失函数，度量两个分布（标签分布和经验回归分布）之间的KL散度，但是也存在对于有噪声的标签缺乏鲁棒性、可能存在差裕度（允许有余地的余度）导致泛化性能下降的问题。而大多数替代方案还不能很好地用于像ImageNet这样的大规模数据集。</p>
<p>许多对正则交叉熵的改进实际上是通过对loss定义的放宽进行的，特别是参考分布是轴对称的。这写改进通常具有不同的动机：比如标签平滑（Label smoothing）通过偏离轴来模糊区分正确和不正确的标签，从而在许多应用中提供了很小但是很重要的提升；在自蒸馏中，利用前几轮的“软”标签作为参考类分布进行多轮交叉熵训练；混合和相关数据增强策略通常通过线性插值创建明确的、新的训练示例，然后将相同的线性插值应用于目标标签分布，类似于软化原始交叉熵loss。用这些修改方法训练的模型显示了改进的泛化、鲁棒性和校准。</p>
<p>本文提出了一个新的loss，受对比loss与度量学习启发，完全去除参考分布，而只是将来自相同类的规范化嵌入强行加在一起，使得其比来自不同类的嵌入更加紧密。</p>
<p>具体来说，在对比学习中，核心思想是拉近某一个锚点与其正样本之间的距离，拉远锚点与该锚点其他负样本之间的距离，通常来说，一个锚点只有一个正样本，其他全视为负样本。而本文的方法认为每个锚点有许多的正样本，而不是许多负样本，并且通过标签显示样本之间的正负关联。比如下面的图，右侧是典型的对比学习方法，通常将一张原图通过数据增强得到两个子样本，这一对子样本之间构成一对正对，而与其他数据的子样本构成负对；而本文的有监督对比学习中，每个子样本可能都有很多的正对和负对。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.1.png" alt="正负对示例" style="zoom: 67%;" /></p>
<p>本文构造的loss在ResNet50和ResNet200上都取得了不错的Top-1效果，在自动增强的ResNet50上取得78。8%的Top-1精度，比同样数据增强下的交叉熵loss提升了1.6%，不仅如此，还更鲁棒。</p>
<p>具体的Contribution如下：</p>
<ol>
<li>我们提出了一个新的扩展对比损失函数，允许每个锚点有多个正对。因此，我们将对比学习适应于完全监督的设置。</li>
<li>我们表明，与交叉熵相比，这种损失使我们能够了解最先进的表示方式，从而显著提高了Top-1的准确性和鲁棒性。</li>
<li>我们的损失对超参数范围的敏感性不如交叉熵。这是一个重要的实际考虑。我们相信，这是由于我们的损失使用更自然的公式，使从同一类样本的代表被拉得更近，而不是像交叉熵一样强迫他们被拉向一个特定的目标。</li>
<li>我们分析地表明，我们的损失函数的梯度鼓励从hard positive和hard negative中学习。我们还表明，三联体损失是我们损失只有一个正极和负极被使用的一个特例。</li>
</ol>
<p>具体来说，有监督对比学习的框架是交叉熵loss和传统对比学习的结合:</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.2.png" alt="三种框架" style="zoom:80%;" /></p>
<h2 id="2-具体结构"><a href="#2-具体结构" class="headerlink" title="2 具体结构"></a>2 具体结构</h2><h3 id="2-1-表征学习框架"><a href="#2-1-表征学习框架" class="headerlink" title="2.1 表征学习框架"></a>2.1 表征学习框架</h3><p>总的来说，有监督对比学习框架的结构类似于表征学习框架，由如下几个部分组成：</p>
<ol>
<li><p><strong>数据增强模块</strong></p>
<p>数据增强模块$A(·)$的作用是将输入图像转换为随机增强的图像$\widetilde{x}$，对每张图像都生成两张增强的子图像，代表原始数据的不同视图。数据增强分为两个阶段：第一阶段是对数据进行<strong>随机裁剪</strong>，然后将其调整为原分辨率大小；第二阶段使用了三种不同的增强方法，具体包括：（1）<strong>自动增强</strong>，（2）<strong>随机增强</strong>，（3）<strong>Sim增强</strong>（按照顺序进行随机颜色失真和高斯模糊，并可能在序列最后进行额外的稀疏图像扭曲操作）。</p>
</li>
<li><p><strong>编码器网络</strong></p>
<p>编码器网络$E(·)$的作用是将增强后的图像$\widetilde{x}$映射到表征空间，每对子图像输入到同一个编码器中得到一对表征向量，本文用的是ResNet50和ResNet200，最后使用池化层得到一个2048维的表征向量。表征层使用单位超球面进行正则化。</p>
</li>
<li><p><strong>投影网络</strong></p>
<p>投影网络$P(·)$的作用是将表征向量映射成一个最终向量$z$进行loss的计算，本文用的是只有一个隐藏层的多层感知器，输出维度为128。同样使用单位超球面进行正则化。在训练完成后，这个网络会被一个单一线性层取代。</p>
</li>
</ol>
<h3 id="2-2-对比损失"><a href="#2-2-对比损失" class="headerlink" title="2.2 对比损失"></a>2.2 对比损失</h3><p>本文的数据是带有标签的，采用mini batch的方法获取数据，首先从数据中随机采样$N$个样本对，记为$\left\{ {x}_k,{y}_k\right\}_{k=1,2,…,N}$，${y}_k$是${x}_k$的标签，之后进行数据增强获得$2N$个数据样本$\left\{\widetilde{x}_k,\widetilde{y}_k\right\}_{k=1,2,…,2N}$，其中，$\widetilde{x}_{2k}$和$\widetilde{x}_{2k-1}$是分别用两种随机增强方法得到的数据对，在数据增强过程中，标签信息始终不会改变。</p>
<h4 id="2-2-1-自监督对比损失"><a href="#2-2-1-自监督对比损失" class="headerlink" title="2.2.1 自监督对比损失"></a>2.2.1 自监督对比损失</h4><p>本文的自监督对比损失与SimCLR的loss相类似，不过使用的是点积刻画样本之间的相似性，具体表达式如下：</p>
<script type="math/tex; mode=display">
\mathcal{L}^{self}=\sum_{i=1}^{2N}{\mathcal{L}_{i}^{self} }\\
\mathcal{L}_{i}^{self}=-\log\frac{\exp(z_i·z_{j(i) }/\tau) } {\sum_{k=1}^{2N}  {\mathbb{l}_{ [k{\neq}i] }·\exp(z_i·z_{j(i) }/\tau) } }</script><p>上式中，$\mathbb{l}_{ [k{\neq}i] }$是一个指示函数，当且仅当$k=i$时取0，否则为1。$\tau$是进行优化的温度参数。该loss的意义在于拉近$\widetilde{x}_i$于其正对$\widetilde{x}_{j(i)}$之间的距离而拉远$\widetilde{x}_i$与其他负对之间的距离。</p>
<h4 id="2-2-2-有监督的对比损失"><a href="#2-2-2-有监督的对比损失" class="headerlink" title="2.2.2 有监督的对比损失"></a>2.2.2 有监督的对比损失</h4><p>有监督对比损失是对自监督对比损失的推广，从公式中很容易可以看出，有监督对比损失拓展了$\widetilde{x}_i$正对的数量，将所有标签信息相同的子数据都视为正对，计算了$\widetilde{x}_i$与其所有正对之间的相似性，之后进行加权平均。</p>
<script type="math/tex; mode=display">
\mathcal{L}^{sup}=\sum_{i=1}^{2N} {\mathcal{L}_{i}^{sup} }\\
\mathcal{L}_{i}^{sup}=\frac{-1} {2N_{\widetilde{y}_i}-1}\sum_{j=1}^{2N} {\mathbb{l}_{ [i{\neq}j] }·\mathbb{l}_{ [ {\widetilde{y}_i}={\widetilde{y}_j} ] } }·\log\frac{\exp(z_i·z_{j(i)}/\tau)}{\sum_{k=1}^{2N}{\mathbb{l}_{ [k{\neq}i] }·\exp(z_i·z_{j(i)}/\tau)} }</script><p>作者指出对比损失的核心是足够多的负对，以便与正对形成鲜明的对比，他们的改进监督对比损失保留了这一特性。此外，由于增加了正对的数量，这一架构还可以更好地刻画类内相似性。</p>
<h4 id="2-2-3-有监督对比损失的梯度特性"><a href="#2-2-3-有监督对比损失的梯度特性" class="headerlink" title="2.2.3 有监督对比损失的梯度特性"></a>2.2.3 有监督对比损失的梯度特性</h4><p>这一部分论证了hard positive和hard negative更有助于提升网络的性能，主要是通过对有监督对比损失的梯度进行分析，在此略去。</p>
<p>此外，作者在论文中还论述了三联loss是他们的有监督对比损失的特例，此处省略不讲。</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h2><p>作者在评估其框架性能时，使用了<strong>Top-1精度</strong>和<strong>对损坏图像的鲁棒性</strong>两个方面进行衡量，还评价了其模型<strong>对超参数的稳定性</strong>以及<strong>正对数量</strong>对模型表现的影响。在实现上，使用的是训练好的网络，之后将网络的非线性投影头替换成一个简单的线性全连接层，使用标准交叉熵损失训练这个线性层。网络的训练在ImageNet上进行。</p>
<h3 id="3-1-ImageNet分类精度"><a href="#3-1-ImageNet分类精度" class="headerlink" title="3.1 ImageNet分类精度"></a>3.1 ImageNet分类精度</h3><p>这部分实验比较了他们的方法与其他使用交叉熵的有监督方法的Top-1与Top-5精度，同时对比了他们的架构使用交叉熵损失的表现，可以看到，综合来说他们的方法实现了最好的效果，同时，他们的架构在使用交叉熵损失时的表现就不是非常好，相对来说，他们的架构在改进loss的情况下，Top-1精度提升了3.8/2.8个点，Top-5精度提升了1/2.3个点。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.3.png" alt="线性分类评估" style="zoom:80%;" /></p>
<h3 id="3-2-对图像损坏和校准的鲁棒性"><a href="#3-2-对图像损坏和校准的鲁棒性" class="headerlink" title="3.2 对图像损坏和校准的鲁棒性"></a>3.2 对图像损坏和校准的鲁棒性</h3><p>这部分实验评价了他们的方法对图像扰动的稳定性，具体来说，他们选择使用对ImageNet数据库中的图像应用常见的自然扰动，比如加噪声、模糊和对比度变化，构造得到的ImageNet-c数据集进行测试。使用平均损坏误差与平均相对损坏误差作为评价指标，可以看到，他们的方法的误差最小，且使用改进的对比损失替换交叉熵损失也有助于提升网络的性能。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.4.png" alt="平均损坏误差" style="zoom:80%;" /></p>
<p>此外，和交叉熵损失相比，本文的对比损失在不同程度的图像损坏下都能保持一个相对稳定的平均损失误差，相比于交叉熵损失也有更高的Top-1精度：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.8.png" alt="与交叉熵损失的对比" style="zoom: 67%;" /></p>
<h3 id="3-3-对超参数的鲁棒性"><a href="#3-3-对超参数的鲁棒性" class="headerlink" title="3.3 对超参数的鲁棒性"></a>3.3 对超参数的鲁棒性</h3><p>通常深度网络对超参数都很敏感，本文还比较了他们的改进对比损失对不同优化器、不同数据增强和学习率的分类精度稳定性。三种增强方式是本文提出的三种；优化器则选用了LARS、带动量的SGD和RMSProp；选择了最佳学习率以及增大或减小十倍的三个学习率进行评估，可以发现，本文提出的loss确实和交叉熵损失相比，对这三种超参数的变化更鲁棒。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.5.png" alt="对超参数的鲁棒性" style="zoom:67%;" /></p>
<h3 id="3-4-不同正对数量对模型表现的影响"><a href="#3-4-不同正对数量对模型表现的影响" class="headerlink" title="3.4 不同正对数量对模型表现的影响"></a>3.4 不同正对数量对模型表现的影响</h3><p>作者对比了每个子数据有1、2、3、5个正对时的Top-1精度，发现正对越多越有助于提升模型表现，当然同时计算成本也更大。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Contrastive%20Learning%E7%9B%B8%E5%85%B3/%E3%80%8ASupervised%20Contrastive%20Learning%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.7.png" alt="不同正对数量的影响" style="zoom:80%;" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/26/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/26/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url">《Contrastive Learning with Hard Negative Samples》阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-12-26T15:30:33+08:00">
                2020-12-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-动机和思路"><a href="#1-动机和思路" class="headerlink" title="1 动机和思路"></a>1 动机和思路</h2><p>对比学习在无监督表征学习领域的潜力无需多言，已经有非常多的例子证明其效果，目前比较多的针对对比学习的改进包括损失函数、抽样策略、数据增强方法等多方面，但是针对负对的研究相对而言更少一些，一般在构造正负对时，大部分模型都简单的把单张图像及其增强副本作为正对，其余样本均视为负对。这一策略可能会导致的问题是模型把相距很远的样本分得很开，而距离较近的负样本对之间可能比较难被区分。</p>
<p>基于此，本文构造了一个难负对的思想，主要目的在于，把离样本点距离很近但是又确实不属于同一类的样本作为负样本，加大了负样本的难度，从而使得类与类之间分的更开，来提升对比学习模型的表现。</p>
<h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h2><h3 id="2-1-难负样本选取原则"><a href="#2-1-难负样本选取原则" class="headerlink" title="2.1 难负样本选取原则"></a>2.1 难负样本选取原则</h3><p>好的难负样本有两点原则：1）与原始样本的标签不同；2）与原始样本尽量相似。</p>
<p>这一点就与之前的对比学习有比较明显的差异了，因为对比学习一般来说并不使用监督信息，因此除了锚点之外的其他样本，不管标签如何，都被认为是负对，所以问题的一个关键在于“<strong>用无监督的方法筛出不属于同一个标签的样本</strong>”。不仅如此，这里还有一个冲突的地方，既要与锚点尽可能相似，又得不属于同一类，这对于一个无监督模型来说是有难度的，因此本文在实际实现过程中进行了一个权衡，<strong>假如对样本的难度要求不是那么高的时候，就只满足原则1，而忽略原则2</strong>。同时，这种方法应该尽量<strong>不增加额外的训练成本</strong>。</p>
<h3 id="2-2-具体方法"><a href="#2-2-具体方法" class="headerlink" title="2.2 具体方法"></a>2.2 具体方法</h3><p>本文的重点在于如何进行难负样本采样，首先作者给出难负样本的采样分布函数：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.1-1608981534224.jpg" alt=""></p>
<p>即难负样本分布以与正类类别不同为条件的概率分布，$q_β(x^-)$是正负样本点积乘以系数$β$之后的指数项再乘以单纯的负样本采样分布，$β$控制采样的难易程度，值越大，代表样本越难。点积越大，$q_β(x^-)$就越大，表示这个样本更容易被采样，结合原则2，即尽量采样困难负样本。</p>
<p>但是这里没有解决原则1的问题，对于无监督方法，我们仍然不知道该怎么确定采样到的负样本与锚点的标签是不是一致的，这里作者用PU-learning的思想，把负样本分布$q_β(x^-)$拆成来自同标签分布$q_β^+(x^-)$与来自不同标签分布$q_β^-(x^-)$的两个部分：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.2-1608982447207.jpg" alt=""></p>
<p>这里对于来自$q_β^+(x^-)$的样本同样施以原则2，即为：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.3-1608982496092.jpg" alt=""></p>
<p>所以满足原则1和原则2的难负样本分布就可以写为：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.4-1608982538530.jpg" alt=""></p>
<p>上式的第一项就是常规的负样本分布，第二项作者提到可以使用语义保留转换去估计这一分布，传统无监督对比学习做的就是这件事儿。</p>
<p>所以原始对比损失：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.5-1608983198336.jpg" alt=""></p>
<p>就可以改写为只使用难负样本的情况：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.6-1608983226868.jpg" alt=""></p>
<p>分母第二项大括号里头的第一项就是之前的总负样本，第二项就是和锚点相似度很高的来自同一类的负样本。所以只需要求出分母中的两个均值，这个损失就算是给出来了。</p>
<p>计算这两项均值的方法，作者用的是蒙特卡洛重要性采样：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.7-1608983580203.jpg" alt=""></p>
<p>式子右边分母的$Z_β$和$Z_β^+$是两个配分函数，可以用均值近似:</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.8-1608983656405.jpg" alt=""></p>
<p>作者提出，在pytorch框架下，这个操作只需要额外两行代码九就能搞定，不需要做另外的操作。</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h2><p>作者以SimCLR为Baseline在几个数据集上进行了实验，给出了对应的实验效果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.9-1608984262715.jpg" alt=""></p>
<p>发现使用这种难负样本机制有所改善。</p>
<p>第二点是作者把训练好的，模型用到下游分类任务中，给出分类精度：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.10-1608984310866.jpg" alt=""></p>
<p>在部分数据集上比baseline有所提升。</p>
<p>但是这个β值并不是越大越好的，过大可能也存在问题：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/%E3%80%8AContrastive%20Learning%20with%20Hard%20Negative%20Samples%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.11-1608984359417.jpg" alt=""></p>
<p>可以发现，在β设置得较大的时候，模型的质量甚至会下降。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/24/Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/24/Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology/" itemprop="url">《Evaluating  the Disentanglement of Deep Generative Models through Manifold Topology》阅读笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-12-24T18:06:57+08:00">
                2020-12-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-动机"><a href="#1-动机" class="headerlink" title="1 动机"></a>1 动机</h2><p>解耦工作对于模型的泛化、鲁棒性和可解释性来说至关重要，然而由于现有的解耦评测方法通常依赖于训练额外的、新的生成模型（分类器、编码器、回归器等）或在特定数据集以及使用特定方法预处理过的数据集上进行验证，因此评价结果的可靠性较差，且往往与任务较为相关，适用范围有限，也导致了用不同指标评测解耦模型时会出现多种排名结果。</p>
<h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h2><p>本文所提出的评价方法基于拓扑学原理，理论上较为抽象，在实现上，测量每个潜在维度（$z_i$）上的条件子流形的持续同调（TDA, <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31734839），然后，本文的评价核心思想即在于，**以解耦因子为条件的潜在子流形之间具有比以纠缠因子为条件的潜在子流形之间具有更高的拓扑相似度**。因此，采用W.RLT距离去度量这个拓扑相似性。">https://zhuanlan.zhihu.com/p/31734839），然后，本文的评价核心思想即在于，**以解耦因子为条件的潜在子流形之间具有比以纠缠因子为条件的潜在子流形之间具有更高的拓扑相似度**。因此，采用W.RLT距离去度量这个拓扑相似性。</a></p>
<p>本文所采用的从数据样本空间估计拓扑空间同调的方法是相对存在时间（Relative Living Times，RLTs），为了获取RLTs，如下图所示，首先假定训练好的生成模型分布$p_{model}(x)$由一个带孔的流形$M_{model}$支撑，从$p_{model}(x)$中采样出样本点集合$X$，从而得到一个基础的单纯形复合物，改变每个样本点的阈值（即图(d)中每个点的半径）用欧氏距离度量点与点之间的临近测度，在逐渐增大阈值时，会产生不同数量的k维孔（拓扑学中度量同调的重要依据），最终实现对持久性同调的逼近，得到持久性条形码（连续同调评价的产物）。</p>
<p>RLTs则是在改变阈值以生成的多个持久性条形码的矢量化，反映了每个k维孔出现和小时的持续时间内的离散分布，对RLTs取均值，则可以得到这些k维孔的平均相对生成时间，为一个离散概率分布，<strong>测量两个数据样本集之间的平均相对生成时间分布的相似性，就可以作为两个样本集之间拓扑相似性的评价依据</strong>。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.1.jpg" alt=""></p>
<p>为了说明其理论的有效性，作者可视化了解耦和非解耦的生成模型在dsprites数据集上的拓扑结构：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.2.jpg" alt=""></p>
<p>可以发现，解耦模型在尺寸和转角两个因子下的子流形明显不同胚，转角子流形有一维孔，旋转则没有，而每个因子内部的各个子流形是同胚的，而不解耦模型则没有这个性质。</p>
<p>传统的基于RLTs的拓扑相似性度量使用欧氏距离，但是经验上发现使用Wasserstein距离能显著改善度量精度。因此，作者基于W重心提出<strong>W.RLTs度量方法</strong>：</p>
<h3 id="度量思想："><a href="#度量思想：" class="headerlink" title="度量思想："></a>度量思想：</h3><p>前面已经提到，采样后得到的RLTs代表k维空穴存在与否的离散分布，首先求出这个离散分布的重心<img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.3.jpg" style="zoom:80%;" />：分布重心定义为“使用W-2距离计算到达分布中所有点的最小总距离所在的位置”，计算公式如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.4.jpg" alt=""></p>
<p>上面的$λ$是一个权重参数，其和为1。</p>
<p>在求出RLTs的重心之后，从两个角度评价其解耦性能的好坏：</p>
<p>1、 同一个因子内部的条件子流形应该有较高的拓扑相似性；</p>
<p>2、 不同因子的条件子流形具有较低的拓扑相似性。</p>
<p>因此，在某一因子$s_i$为条件下生成的条件子流形下，控制因子$s_i$的值不变，而其他因子的值可以变化，得到一个同一因子下的集群，测量这一集群内的拓扑相似性作为这一因子下的条件子流形的拓扑相似性结果；同时，测量不同因子作为条件的子流形之间的拓扑相似性，作为因子间拓扑相似性结果。作者在CelabA数据集上可视化了W.RLTs结果来说明这一思想，下图中的第一行是同一个因子内部的连续同调W.RLTs，下一行是不同因子之间的W.RLTs：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.5.jpg" alt=""></p>
<h4 id="无监督评价方法"><a href="#无监督评价方法" class="headerlink" title="无监督评价方法"></a>无监督评价方法</h4><p>在评价时，由于实际的因子$s_i$与实验设置的因子$z_j$之间可能并不存咋一一对应的连结，因此，本文使用W距离计算每个因子$z_j$为条件的子流形的拓扑相似性，也就是使用W距离计算不同条件子流形之间的分布重心的距离，得到一个$j<em>j$维的拓扑相似性矩阵$M$，在得到拓扑相似性矩阵之后，使用奇异值分解进行频谱共聚类，其目的在于合并同态的以设置因子$z$为条件的子流形，使得$z$可以与实际因子$s$相对应，即合并解耦作用相似的因子，降维。由此得到最终的$c</em>c$维的共聚类相似性矩阵$M_c$。作者可视化了不同模型在不同数据集下的共聚类相似性矩阵（颜色越深，代表相似性越强，对角线即上即为同一条件子流形的同态相似性可视化结果）：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.6.jpg" alt=""></p>
<p>之后，最小化共聚类相似性矩阵$M_c$的聚类内方差和聚类间方差，去求得最终的聚类数量c，使用共聚类相似性矩阵$M_c$，计算解耦得分$μ$。解耦得分定义为聚类内相似性和聚类间相似性之差：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.7.jpg" alt=""></p>
<p>因此，一个解耦效果较好的模型，想要获得较高的解耦得分，则希望有尽量大的聚类内相似性，同时聚类间的相似性尽量小，类间相似性非常直观，就是$M_c$对角元上的值，因此：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.8.jpg" alt=""></p>
<p>同样的，类间相似性就是$M_c$剔除对角元元素之外的值：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.9.jpg" alt=""></p>
<p>这就是无监督方法的聚类解耦评估方法。</p>
<h4 id="有监督评价方法"><a href="#有监督评价方法" class="headerlink" title="有监督评价方法"></a>有监督评价方法</h4><p>与上面的无监督方法相比，有监督方法的改动在于不再比较不同给定因子$z_j$，之间的拓扑相似性，而是对于有标签的数据集，同时计算实际因子$s_i$的W.RLTs，因此，在评价时，只计算$z_j$与$s_i$之间的拓扑相似性，得到$j*i$维的矩阵$M$，同样进行奇异值分解（具体怎么做没有再说明），因此类间相似性和类内相似性的计算如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.10.jpg" alt=""></p>
<p>与无监督指标不同的地方在于，有监督指标对计算结果进行了归一化处理：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.11.jpg" alt=""></p>
<h4 id="方法的限制"><a href="#方法的限制" class="headerlink" title="方法的限制"></a>方法的限制</h4><p>本评价方法假定数据流形是不完全对称的，因此没有考虑对称流形的情况，同时，RLTs不能计算数据流形的完整拓扑，而是进行逼近。</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h2><p>作者将本文提出的方法与MIG，一个使用一个分类器的方法Disentanglement和一个专门对人脸数据库解耦表现做评估的PPL方法进行比较，对不同解耦模型做排名，得到与其他指标相似的排名结果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.12.jpg" alt=""></p>
<p>大部分指标排名都比较相似，然而对于β-VAE，本文方法的指标与MIG相比差异较大，但是两种指标评价β-VAE时都有较大的方差，因此说明β-VAE的表现可能不是很稳定。同时还发现了两种训练目标比较相似的模型可能在解耦表现上有较大差异（Factor VAE和β-VAE）。</p>
<p>同时报告了定量结果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/《Evaluating%20%20the%20Disentanglement%20of%20Deep%20Generative%20Models%20through%20Manifold%20Topology》阅读笔记/5.13.jpg" alt=""></p>
<p>作者指出，大部分情况下，无监督评估指标更为适用，尤其对于CelebA，由于脸部信息过多，不易于用有监督方法进行评价，而如果想评估特定的解耦（发色，眼镜），用有监督的方法会更合适。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/22/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="缓缓行舟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/22/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/" itemprop="url">四篇图像解耦工作简要介绍</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-12-22T19:20:07+08:00">
                2020-12-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-Variational-Interaction-Information-Maximization-for-Cross-domain-Disentanglement"><a href="#1-Variational-Interaction-Information-Maximization-for-Cross-domain-Disentanglement" class="headerlink" title="1 Variational Interaction Information Maximization for Cross-domain Disentanglement"></a>1 Variational Interaction Information Maximization for Cross-domain Disentanglement</h2><p>这篇文章的思想是使用信息论知识，实现跨域图像解耦表示，是基于VAE的一个改进工作。</p>
<p>对于图像对x，y，二者之间既有共享的表征信息，也有不共享的表征信息，这篇文章提了如下图所示的架构，训练一个VAE，同时学到X的特定表征，Y的特定表征以及X与Y之间的共享表征，这一目的通过最大化X与Y的联合分布之间的边际似然函数实现：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/QQ图片20201222160929.png" alt=""></p>
<p>在实际应用中直接优化上述公式有些困难，作者还做了其他的简化表达，经过改进后，其损失函数的思想在于，希望每个数据之前特异的特征被编码到各自单独的编码器$Z_x,Z_y$之中，而二者之间相互共享的表征则被同一个编码器所$Z_s$编码，为了实现这一目的（三类表征之间尽量分开），作者引入了互信息思想，希望尽可能最小化$I(Z_s, Z_x)$与$I(Z_s, Z_y)$来实现共享表征与特定表征之间的分离，具体实施时使用了如下图所示的公式：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/QQ截图20201222162104.jpg" alt=""></p>
<p>上面这个公式的第一项的作用在于鼓励$Z_s, Z_x$联合向共享域X中提供信息（与数据集X保持紧密联系），后两项的目的在于减少二者之间的信息总量。</p>
<p>同时，为了鼓励共享表征和特异表征之间的分离，还构造了如下的互信息正则化项：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.jpg" alt=""></p>
<p>上面两个等式中，最大化第一项意味着共享表征中含有来自数据集中提取到的表征，最小化第二项意味着从一个数据集中提取到的表征可以很容易地从另一个数据集中推断出来，意味着这个表征是共享的。</p>
<p>下图中的编码器r是设计来为下游任务（图像翻译和检索）使用的。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/1.0.jpg" alt="img;" style="zoom:;" /></p>
<p>解耦效果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/1.4.jpg" style="zoom:67%;" /></p>
<p>定量评估没有说明其具体的解耦指标得分，主要是对跨域图像检索进行的评估。</p>
<h2 id="2-ICAM-Interpretable-Classification-via-Disentangled-Representations-and-Feature-Attribution-Mapping"><a href="#2-ICAM-Interpretable-Classification-via-Disentangled-Representations-and-Feature-Attribution-Mapping" class="headerlink" title="2 ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping"></a>2 ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping</h2><p>一篇在医学影像领域的解耦应用，主要使用GAN-VAE架构，用来鉴别正常人与病患身体结构的差异性，进行影像诊断。网络框架如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/2.1.jpg" alt=""></p>
<p>简要介绍网络各个组成部分的作用：</p>
<p>内容编码器$E^c$：编码输入图像对中共享的与类别无关的信息，用鉴别器对编码特征进行判断，希望编码器对图像对的两张图像的共享信息输入趋同的特征；</p>
<p>属性编码器$E^a$：编码类别相关特征，用来分类；</p>
<p>生成器G：以上述两个编码器输出的特征作为联合输入，目的在于输出受内容特征与属性特征共同控制的图像；</p>
<p>特征映射$Attr Map$：定位类间差异区域。</p>
<p>整个框架基于GAN网络，同时引入VAE思想，使用编码器编码的特征作为重构输入而不是随机噪声，在生成虚假图像后再次进行一次图像生成，用二次生成的图像与真实图像之间的差异性作为最根本的损失。通过这一结构，其目的在于挖掘出决定相似图像类别差异的特征，实现类别与无关特征之间的解耦。</p>
<h2 id="3-Elastic-InfoGAN-Unsupervised-Disentangled-Representation-Learning-in-Class-Imbalanced-Data"><a href="#3-Elastic-InfoGAN-Unsupervised-Disentangled-Representation-Learning-in-Class-Imbalanced-Data" class="headerlink" title="3 Elastic-InfoGAN:  Unsupervised Disentangled Representation Learning in Class-Imbalanced Data"></a>3 Elastic-InfoGAN:  Unsupervised Disentangled Representation Learning in Class-Imbalanced Data</h2><p>本文的贡献是用InfoGAN实现对类不平衡数据的解耦。</p>
<p>InfoGAN假设数据服从均匀分布，因此在类别不平衡的数据中解耦表现较差：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.1.jpg" alt=""></p>
<p>这篇文章针对这一问题对InfoGAN做了两个改进，其一是不对数据分布进行假设，而将其视为优化过程中的可学习参数，为了实现这一点，采用Gumbel-Softmax分布作为噪声的潜在分布，该分布有可微参数，因此可以进行更新；其二是通过实验发现InfoGAN在类不平衡信息中很容易学到图像的低级特征（与之前分享的解释对比学习工作的发现有异曲同工之妙），因此这篇文章引入对比学习的思想，对数据进行增强，强迫模型学习身份表示，以抑制类不平衡的影响。</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.2.jpg" alt=""></p>
<p>整篇文章的工作重点就是上述的两个方面，其一，用可学习分布代替InfoGAN假设的均匀分布，优化InfoGAN的同时更新分布（左图）。k维类别潜码的采样方法如下：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.3.jpg" alt=""></p>
<p>$g_i$代表从Gumbel（0，1）分布采样的样本点，温度参数代表不同类之间的相似程度，假如温度参数很小，将会趋近于onn-hot编码（均匀分布）。</p>
<p>其二，使用简单数据增强方法给数据构造一个正对，同时引入负对，添加一个对比损失项，强迫模型学习身份表示。使用的是常规的对比损失：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.4.jpg" alt=""></p>
<p>最终的损失为InfoLoss和对比损失之和：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.5.jpg" alt=""></p>
<p>定性实验结果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.6.jpg" alt=""></p>
<p>定量实验结果：</p>
<p>使用NMI和ENT（平均熵，评价同一个潜码生成的图像是否属于同一类；每一个潜码是否只与一个真实类别标签关联；越小越好）作为评价指标：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/3.7.jpg" alt=""></p>
<h2 id="4-WAE模型"><a href="#4-WAE模型" class="headerlink" title="4 WAE模型"></a>4 WAE模型</h2><p>ICLR2021中有两篇论文在WAE（WASSERSTEIN AUTOENCODER）的框架下进行解耦图像生成，WAE是2018年由Google在WGAN的基础上提出来的一种自编码器模型，由于目前没有了解其原理，因此只对这两篇论文在WAE基础上的改进进行简单的介绍。</p>
<h3 id="4-1-Learning-disentangled-representations-with-the-Wasserstein-Autoencoder"><a href="#4-1-Learning-disentangled-representations-with-the-Wasserstein-Autoencoder" class="headerlink" title="4.1 Learning  disentangled representations with the Wasserstein Autoencoder"></a>4.1 Learning  disentangled representations with the Wasserstein Autoencoder</h3><p>想法是把β-TCVAE的构造移植到WAE模型中，重点在于利用TCVAE的loss对WAE进行改进：</p>
<p>TCWAE loss：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.1.jpg" alt=""></p>
<p>β-TCVAE loss：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.2.jpg" alt=""></p>
<p>对比两项损失，可以发现TCWAE具有与β-TCVAE几乎相同的loss函数，区别在于没有最后一个互信息项，以及在第一项的度量上有所差异。</p>
<p>效果：</p>
<p><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.3.jpg" alt=""></p>
<h3 id="4-2-DISENTANGLED-RECURRENT-WASSERSTEIN-AUTOENCODER"><a href="#4-2-DISENTANGLED-RECURRENT-WASSERSTEIN-AUTOENCODER" class="headerlink" title="4.2 DISENTANGLED RECURRENT WASSERSTEIN AUTOENCODER"></a>4.2 DISENTANGLED RECURRENT WASSERSTEIN AUTOENCODER</h3><p>第二篇WAE相关的文章是将WAE应用于时序图像解耦的工作，用来捕捉时序图像上的相关信息，实现静态因子和动态因子的解耦。</p>
<p><img src="" alt=""><img src="http://little_z_c.gitee.io/imagebed/Disentanglement%E7%9B%B8%E5%85%B3/%E5%9B%9B%E7%AF%87%E5%9B%BE%E5%83%8F%E8%A7%A3%E8%80%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/4.4.jpg" alt="4.4" style="zoom:67%;" /></p>
<p>上图是这篇文章提出了来的模型的解耦效果，每一行都代表一个时序（对应不同表情）。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">微澜</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
